{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBpHNI3wthDr"
      },
      "source": [
        "<center><h1 style=\"text-align: center;\"><u>TFM - Detección temprana del cáncer a partir de los resultados de análisis de sangre</u></h1>\n",
        "<center><img src=\"https://nachocarnes.es/wp-content/uploads/2018/04/ejWGXui6_400x400.png\" alt=\"Drawing\" style=\"align=left\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# INTRODUCCIÓN\n",
        "\n",
        "En este notebook se lleva a cabo el contraste del modelo CancerA1DE para la detección prematura de cáncer frente a otros modelos de predicción.\n",
        "\n",
        "El artículo sobre el que se basa esta investigación es \"Early Cancer Detection from Multianalyte Blood Test Results\" presente en : https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6548890/\n",
        "\n",
        "El contenido abarca un repaso de los datos, su análisis a todos los niveles y los diferentes enfoques para entrenar, validar y testear los distintos modelos.\n",
        "\n",
        "Los datos se encontrarán en la siguiente ruta en local : C:\\Users\\danie\\OneDrive\\Documentos\\Master\\Lusku\\TFM\\Proposiciones\\Deteccion Cancer\\Datos\n",
        "\n",
        "IMPORTANTE : Cada vez que tenga que rellenarse a mano el valor de una variable, se mostrará con : \n",
        "\n",
        "                    (I) Introducir valor de 'nombreDeLaVariable'\n",
        "\n",
        "Para descargar las librerías usadas en este proyecto se puede hacer usar el comando \"pip install -r requirements.txt\" en el terminal\n",
        "\n",
        "Los bloques de código que empiezan por # P , forman parte del proceso principal y son necesarios ejecutarse para seguir el flujo del proceso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# P\n",
        "# Carga de librerias\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from scipy.stats import skew, kurtosis\n",
        "from scipy.stats import shapiro\n",
        "from scipy.stats import normaltest\n",
        "\n",
        "# Entrenar el modelo\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Selección de las variables por tipo\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import make_column_selector\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "# Modelos\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, roc_curve, auc, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, adjusted_rand_score, r2_score, silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from joblib import dump, load\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Functions\n",
        "\n",
        "# Conversion del dataFrame a tipo numeric\n",
        "def convert_to_numeric(column):\n",
        "    if column.dtype in ['object','category']:\n",
        "        # Verificar si hay letras en todos los registros\n",
        "        contains_letters = any(isinstance(val, str) and any(c.isalpha() for c in val) for val in column)\n",
        "        if not contains_letters :\n",
        "            return pd.to_numeric(column, errors='coerce')\n",
        "    return column\n",
        "\n",
        "def discretizar_df_arboles_1(df, max_depth=40, n_bins=18, rango_discretizacion=(-np.inf, np.inf)):\n",
        "    df_discretizado = pd.DataFrame()\n",
        "    \n",
        "    # Iterar sobre todas las columnas del dataframe original\n",
        "    for columna in df.columns:\n",
        "        if df[columna].dtype.kind in 'biufc' or columna.name == 'Tumor type': # (CHULO) Comprueba si el tipo de datos de la columna es numérico ('b' para booleano, 'i' para entero, 'u' para sin signo, 'f' para flotante o 'c' para complejo)\n",
        "            # Si la columna es numérica, realizar la discretización\n",
        "            dt = DecisionTreeRegressor(max_depth=max_depth)\n",
        "            dt.fit(df[columna].values.reshape(-1, 1), df[columna])\n",
        "            puntos_corte = dt.tree_.threshold[dt.tree_.threshold != -2] # Extrae los puntos de corte del árbol de decisión para la columna numérica específica, ignorando aquellos puntos de corte asociados con nodos hoja (-2)\n",
        "            puntos_corte = np.sort(puntos_corte)\n",
        "            puntos_corte = np.concatenate(([rango_discretizacion[0]], puntos_corte, [rango_discretizacion[1]]))\n",
        "            df_discretizado[f'{columna}'] = pd.cut(df[columna], bins=puntos_corte, labels=range(len(puntos_corte)-1))\n",
        "        else:\n",
        "            # Si la columna no es numérica, simplemente copiarla al dataframe resultante\n",
        "            df_discretizado[columna] = df[columna]\n",
        "            \n",
        "    return df_discretizado.apply(convert_to_numeric)\n",
        "\n",
        "# Discretizar dataFrame y mostrar correlación respecto a la variable objetivo\n",
        "def discretizar_df_arboles(df, imprimir=\"SI\", max_depth=15, rango_discretizacion=(-np.inf, np.inf)):\n",
        "    df_discretizado = pd.DataFrame()\n",
        "    \n",
        "    # Iterar sobre todas las columnas del dataframe original\n",
        "    for columna in df.select_dtypes(include=['number']).columns:\n",
        "        if df[columna].dtype.kind in 'biufc' or columna.name != 'Tumor type': # (CHULO) Comprueba si el tipo de datos de la columna es numérico ('b' para booleano, 'i' para entero, 'u' para sin signo, 'f' para flotante o 'c' para complejo)\n",
        "            # Si la columna es numérica, realizar la discretización\n",
        "            dt = DecisionTreeRegressor(max_depth=max_depth)\n",
        "            dt.fit(df[columna].values.reshape(-1, 1), df[columna])\n",
        "            puntos_corte = dt.tree_.threshold[dt.tree_.threshold != -2] # Extrae los puntos de corte del árbol de decisión para la columna numérica específica, ignorando aquellos puntos de corte asociados con nodos hoja (-2)\n",
        "            puntos_corte = np.sort(puntos_corte)\n",
        "            puntos_corte = np.concatenate(([rango_discretizacion[0]], puntos_corte, [rango_discretizacion[1]]))\n",
        "            # print(f\"\\t Columna : {columna} \\n Puntos de Corte : \\n {puntos_corte}\")\n",
        "            df_discretizado[f'{columna}'] = pd.cut(df[columna], bins=puntos_corte, labels=range(len(puntos_corte)-1))\n",
        "        else:\n",
        "            # Si la columna no es numérica, simplemente copiarla al dataframe resultante\n",
        "            df_discretizado[columna] = df[columna]\n",
        "            \n",
        "   # Calcular el coeficiente de correlación entre las variables numéricas discretas y la variable objetivo binaria\n",
        "    correlaciones_discretas = df_discretizado.corrwith(df_discretizado['Tumor type'])\n",
        "\n",
        "    # Ordenar las correlaciones de mayor a menor\n",
        "    correlaciones_discretas_ordenadas = correlaciones_discretas.abs().sort_values(ascending=False)\n",
        "\n",
        "    # Obtener las top 20 variables numéricas discretas con las correlaciones más altas\n",
        "    top_20_correlaciones_discretas = correlaciones_discretas_ordenadas.nlargest(20)\n",
        "\n",
        "    if imprimir == \"SI\":\n",
        "        # Imprimir las top 20 correlaciones\n",
        "        print(top_20_correlaciones_discretas)\n",
        "\n",
        "    return df_discretizado\n",
        "\n",
        "\n",
        "def escalado_dataFrame(df) :\n",
        "    if df.empty:\n",
        "        raise ValueError(\"El DataFrame está vacío, no se puede realizar el escalado.\")\n",
        "\n",
        "    # Crear un objeto StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Normalizar df_imputacion_iterativa\n",
        "    df_normalized = df.select_dtypes(include=['number']).copy()  # Crear una copia del DataFrame original\n",
        "    \n",
        "    if not df_normalized.empty:\n",
        "        df_normalized[df_normalized.columns] = scaler.fit_transform(df_normalized)\n",
        "    else :\n",
        "        print(\"Esto está vacío\")\n",
        "    return df_normalized\n",
        "\n",
        "def calcular_ganancia_informacion(df_features, target, imprimir = 'SI'):\n",
        "    \n",
        "    # Extraer las características de interés del DataFrame\n",
        "    X_interest = df_features.values\n",
        "    \n",
        "    # Extraer la variable objetivo del DataFrame principal\n",
        "    y = target.values\n",
        "    \n",
        "    # Calcular la Ganancia de Información utilizando Mutual Information\n",
        "    information_gain = mutual_info_classif(X_interest, y, discrete_features=False, random_state=42, n_neighbors=7)\n",
        "    \n",
        "    # Crear un DataFrame para visualizar los resultados\n",
        "    ig_results = pd.DataFrame({'Feature': df_features.columns, 'Information Gain': information_gain})\n",
        "    \n",
        "    # Ordenar los resultados por Ganancia de Información en orden descendente\n",
        "    ig_results_sorted = ig_results.sort_values(by='Information Gain', ascending=False)\n",
        "\n",
        "    if imprimir == \"SI\":\n",
        "        print(ig_results_sorted)\n",
        "\n",
        "# División del conjunto de datos en entrenamiento, validacion y test \n",
        "\n",
        "\n",
        "def split_data(X, y, train_size=0.6, val_size=0.2, test_size=0.2, random_state=None):\n",
        "    \"\"\"\n",
        "    Divide un conjunto de datos en entrenamiento, validación y test.\n",
        "\n",
        "    Args:\n",
        "        X: Matriz de características.\n",
        "        y: Vector de etiquetas.\n",
        "        train_size: Porcentaje de datos para entrenamiento (por defecto: 0.6).\n",
        "        val_size: Porcentaje de datos para validación (por defecto: 0.2).\n",
        "        test_size: Porcentaje de datos para test (por defecto: 0.2).\n",
        "        random_state: Semilla para la aleatorización (por defecto: None).\n",
        "\n",
        "    Returns:\n",
        "        Tuple: (X_train, X_val, X_test, y_train, y_val, y_test).\n",
        "    \"\"\"\n",
        "    assert train_size + val_size + test_size == 1.0, \"La suma de train_size, val_size y test_size debe ser igual a 1.0\"\n",
        "\n",
        "    # Dividir los datos en entrenamiento y test\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state, shuffle=True)\n",
        "\n",
        "    # Calcular porcentaje respecto al tamaño original\n",
        "    val_size_relative = val_size / (1.0 - test_size)\n",
        "\n",
        "    # Dividir los datos de entrenamiento en entrenamiento y validación\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size_relative, random_state=random_state, shuffle=True)\n",
        "\n",
        "    return X_train, X_val, X_test, y_train, y_val, y_test\n",
        "\n",
        "def mostrar_cross_validation(model, X_train, y_train):\n",
        "    cv_scores = cross_val_score(\n",
        "                estimator = model,\n",
        "                X         = X_train,\n",
        "                y         = y_train,\n",
        "                scoring   = 'neg_root_mean_squared_error',\n",
        "                cv        = 5\n",
        "            )\n",
        "    print(\"Cross validation : \")\n",
        "    print(f\"Métricas validación cruzada: {cv_scores}\")\n",
        "    print(f\"Média métricas de validación cruzada: {cv_scores.mean()}\")\n",
        "    print(\"\")\n",
        "\n",
        "def mostrar_score(X_train, y_train, model):\n",
        "    score = round(model.score(X_train, y_train), 3)*100\n",
        "    print(f\"Tanto por ciento de acierto : {score} %\")\n",
        "    print(\"\")\n",
        "\n",
        "def mostrar_estadisticas(y_val, y_pred) :\n",
        "    mse = round(mean_squared_error(y_val, y_pred),3)\n",
        "    accuracy = round(accuracy_score(y_val, y_pred),3)\n",
        "    precision = round(precision_score(y_val, y_pred),3)\n",
        "    recall = round(recall_score(y_val, y_pred),3)\n",
        "    f1 = round(f1_score(y_val, y_pred),3)\n",
        "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(\"Error cuadrático medio en el conjunto de validación:\", mse)\n",
        "    print(\"Matriz de Confusión :\\n\", conf_matrix)\n",
        "    print(\"\")\n",
        "\n",
        "def mostrar_curva_ROC(y_val, y_pred) :\n",
        "    fpr, tpr, _ = roc_curve(y_val, y_pred)\n",
        "\n",
        "    # Calcular el área bajo la curva ROC (AUC)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    # Plotear la curva ROC\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Tasa de Falsos Positivos')\n",
        "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "    plt.title('Curva ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "    \n",
        "def evaluate_model(model, X, y, set_name):\n",
        "    y_pred = model.predict(X)\n",
        "    accuracy = accuracy_score(y, y_pred)\n",
        "    precision = precision_score(y, y_pred)\n",
        "    recall = recall_score(y, y_pred)\n",
        "    f1 = f1_score(y, y_pred)\n",
        "    print(f\"Metrics for {set_name} set:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Función para mostrar métricas y añadirlas a un DataFrame\n",
        "def mostrar_estadisticas_guardar_tabla(y_val, y_pred, set_name, model_name, enfoque, print_roc = 'NO'):\n",
        "    '''\n",
        "    Ejemplo de uso :\n",
        "\n",
        "    # Fase de entrenamiento\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(y_train, kmeans.predict(X_train_prep), \"Training\", results_df, model_name)\n",
        "\n",
        "    # Fase de validación\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(y_val, kmeans.predict(X_val_prep), \"Validation\", results_df, model_name)\n",
        "\n",
        "    # Fase de prueba\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(y_test, kmeans.predict(X_test_prep), \"Test\", results_df, model_name)\n",
        "\n",
        "    # Importante: Al final de todos los modelos (fuera del método): Guardar los resultados en un archivo Excel\n",
        "    results_df.to_excel('model_results.xlsx', index=False)\n",
        "\n",
        "    '''\n",
        "    global tabla_results_df\n",
        "    accuracy = accuracy_score(y_val, y_pred)\n",
        "    precision = precision_score(y_val, y_pred, average='weighted')\n",
        "    recall = recall_score(y_val, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_val, y_pred, average='weighted')\n",
        "    rand_index = adjusted_rand_score(y_val, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_val, y_pred)\n",
        "    r2 = r2_score(y_val, y_pred)\n",
        "    mse = mean_squared_error(y_val, y_pred)\n",
        "    fpr, tpr, _ = roc_curve(y_val, y_pred)\n",
        "    # Calcular el área bajo la curva ROC (AUC)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    if conf_matrix.shape == (2, 2):  # Asegúrate de que es una matriz de confusión 2x2\n",
        "        tn, fp, fn, tp = conf_matrix.ravel()\n",
        "    else:\n",
        "        tn = fp = fn = tp = None  # Si no es una matriz 2x2, asigna valores None\n",
        "    \n",
        "    global_score = calcular_puntuacion_global(accuracy, precision, recall, f1, rand_index, r2, mse, tn, fp, fn, tp, roc_auc)\n",
        "    \n",
        "    # Imprimir todas las métricas\n",
        "    print(f\"Metrics for {set_name} set :\")\n",
        "    print(f\" - Accuracy: {accuracy:.4f}\")\n",
        "    print(f\" - Precision: {precision:.4f}\")\n",
        "    print(f\" - Recall: {recall:.4f}\")\n",
        "    print(f\" - F1-Score: {f1:.4f}\")\n",
        "    print(f\" - Adjusted Rand Index: {rand_index:.4f}\")\n",
        "    print(f\" - Mean Squared Error: {mse:.4f}\")\n",
        "    print(f\" - R-squared: {r2:.4f}\")\n",
        "    print(f\" - Área bajo la curva : {roc_auc:.3f}\")\n",
        "    print(f\" - Confusion Matrix: \\n{conf_matrix}\")\n",
        "    #if tn is not None and fp is not None and fn is not None and tp is not None:\n",
        "    #   print(f\"\\tTN: {tn}\\n\\tFP: {fp}\\n\\tFN: {fn}\\n\\tTP: {tp}\")\n",
        "    print(f\" - Global Score : {global_score}\")\n",
        "    print(f\"\")\n",
        "    \n",
        "    \n",
        "    new_row = pd.DataFrame({\n",
        "        'Model': [model_name],\n",
        "        'Approach': [enfoque],\n",
        "        'Set': [set_name],\n",
        "        'Accuracy': [accuracy],\n",
        "        'Precision': [precision],\n",
        "        'Recall': [recall],\n",
        "        'F1-Score': [f1],\n",
        "        'Adjusted Rand Index': [rand_index],\n",
        "        'Mean Squared Error': [mse],\n",
        "        'R-squared': [r2],\n",
        "        'AUC-ROC': [roc_auc],\n",
        "        'TN': [tn],\n",
        "        'FP': [fp],\n",
        "        'FN': [fn],\n",
        "        'TP': [tp],\n",
        "        'Global Score' : [global_score]\n",
        "    })\n",
        "\n",
        "    if (print_roc == 'SI') :\n",
        "        plot_ROC(fpr, tpr, roc_auc)\n",
        "    \n",
        "    tabla_results_df = pd.concat([tabla_results_df, new_row], ignore_index=True)\n",
        "    \n",
        "    return tabla_results_df\n",
        "\n",
        "def plot_ROC(fpr, tpr, roc_auc):\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='Curva ROC (AUC = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('Tasa de Falsos Positivos')\n",
        "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "    plt.title('Curva ROC')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "def calcular_puntuacion_global(accuracy, precision, recall, f1, rand_index, r2, mse, tn, fp, fn, tp, auc_roc):\n",
        "    # Definir ponderaciones para cada métrica\n",
        "    weights = {\n",
        "        'accuracy': 0.12,\n",
        "        'precision': 0.12,\n",
        "        'recall': 0.12,\n",
        "        'f1': 0.12,\n",
        "        'rand_index': 0.1,\n",
        "        'r2': 0.05,\n",
        "        'mse': 0.05,\n",
        "        'tpr': 0.1,\n",
        "        'fpr': 0.1,\n",
        "        'auc_roc': 0.12,\n",
        "    }\n",
        "    \n",
        "     # Normalizar las métricas\n",
        "    mse_norm = (1 - mse)  # Invertir MSE ya que menor es mejor\n",
        "    r2_norm = (r2 + 1) / 2  # Normalizar R2 para que esté entre 0 y 1\n",
        "    \n",
        "    # TPR y FPR\n",
        "    tpr = recall  # TPR es lo mismo que recall\n",
        "    fpr = fp / (fp + tn) if (fp + tn) != 0 else 0  # Asegurar no división por cero\n",
        "\n",
        "    # Invertir FPR ya que menor es mejor\n",
        "    fpr_inverted = 1 - fpr\n",
        "\n",
        "    # Calcular la puntuación global\n",
        "    global_score = (accuracy * weights['accuracy'] +\n",
        "                    precision * weights['precision'] +\n",
        "                    recall * weights['recall'] +\n",
        "                    f1 * weights['f1'] +\n",
        "                    rand_index * weights['rand_index'] +\n",
        "                    r2_norm * weights['r2'] +\n",
        "                    mse_norm * weights['mse'] +\n",
        "                    tpr * weights['tpr'] +\n",
        "                    fpr_inverted * weights['fpr'] +\n",
        "                    auc_roc * weights['auc_roc'])\n",
        "    \n",
        "    return round(global_score * 100, 2)\n",
        "\n",
        "# Funciones para aprendizaje no supervisado\n",
        "def optimal_cluster_number(X_train, X_val, model, max_clusters=10, method='elbow', plot_grafica = 'NO'):\n",
        "    \"\"\"\n",
        "    Encuentra el número óptimo de clusters para un modelo de clustering utilizando el método del codo (Elbow Method)\n",
        "    u otros métodos.\n",
        "    \"\"\"\n",
        "    if method == 'elbow':\n",
        "        distortions = []\n",
        "        for i in range(1, max_clusters + 1):\n",
        "            model.n_clusters = i\n",
        "            model.fit(X_train)\n",
        "            distortions.append(model.inertia_)\n",
        "        if plot_grafica == 'SI':\n",
        "            # Plotting the elbow curve\n",
        "            plt.plot(range(1, len(distortions) + 1), distortions, marker='o')\n",
        "            plt.xlabel('Número de clusters')\n",
        "            plt.ylabel('Distorsión')\n",
        "            plt.title('Método del codo para encontrar el número óptimo de clusters')\n",
        "            plt.show()\n",
        "\n",
        "        # Finding the optimal number of clusters based on the elbow point\n",
        "        optimal_k = np.argmin(np.gradient(distortions)) + 1\n",
        "        if optimal_k == 1:  # Ensure that the optimal number of clusters is greater than 1\n",
        "            optimal_k = 2\n",
        "        return optimal_k\n",
        "\n",
        "    elif method == 'silhouette':\n",
        "        silhouette_scores = []\n",
        "        for i in range(2, max_clusters + 1):\n",
        "            model.n_clusters = i\n",
        "            model.fit(X_train)\n",
        "            labels = model.predict(X_val)\n",
        "            silhouette_scores.append(silhouette_score(X_val, labels))\n",
        "        \n",
        "        if plot_grafica == 'SI':\n",
        "            # Plotting the silhouette scores\n",
        "            plt.plot(range(2, len(silhouette_scores) + 2), silhouette_scores, marker='o')\n",
        "            plt.xlabel('Número de clusters')\n",
        "            plt.ylabel('Silhouette Score')\n",
        "            plt.title('Silhouette Score para encontrar el número óptimo de clusters')\n",
        "            plt.show()\n",
        "\n",
        "        # Finding the optimal number of clusters based on silhouette score\n",
        "        optimal_k = np.argmax(silhouette_scores) + 2\n",
        "        return optimal_k\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"Método no válido. Métodos disponibles: 'elbow', 'silhouette', etc.\")\n",
        "\n",
        "\n",
        "# Función para motrar estadísticas para modelos de aprendizaje no supervisado   \n",
        "def mostrar_estadisticas_guardar_tabla_NS(X, labels, set_name, model_name):\n",
        "    '''\n",
        "    Ejemplo de uso :\n",
        "\n",
        "    # Fase de entrenamiento\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(X_train, kmeans.predict(X_train), \"Training\", model_name, results_df)\n",
        "\n",
        "    # Fase de validación\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(X_val, kmeans.predict(X_val), \"Validation\", model_name, results_df)\n",
        "\n",
        "    # Fase de prueba\n",
        "    results_df = mostrar_estadisticas_guardar_tabla(X_test, kmeans.predict(X_test), \"Test\", model_name, results_df)\n",
        "\n",
        "    # Importante: Al final de todos los modelos (fuera del método): Guardar los resultados en un archivo Excel\n",
        "    results_df.to_excel('model_results.xlsx', index=False)\n",
        "    '''\n",
        "    global tabla_results_NS_df\n",
        "    silhouette_avg = silhouette_score(X, labels)\n",
        "    db_score = davies_bouldin_score(X, labels)\n",
        "    ch_score = calinski_harabasz_score(X, labels)\n",
        "\n",
        "    global_score = calcular_puntuacion_global_NS(silhouette_avg, db_score, ch_score)\n",
        "    \n",
        "    # Imprimir todas las métricas\n",
        "    print(f\"Metrics for {set_name} set ({model_name}):\")\n",
        "    print(f\" - Silhouette Score: {silhouette_avg:.4f}\")\n",
        "    print(f\" - Davies-Bouldin Index: {db_score:.4f}\")\n",
        "    print(f\" - Calinski-Harabasz Index: {ch_score:.4f}\")\n",
        "    print(f\" - Global Score: {global_score:.4f}\")\n",
        "    print(f\"\")\n",
        "    \n",
        "    new_row = pd.DataFrame({\n",
        "        'Model': [model_name],\n",
        "        'Set': [set_name],\n",
        "        'Silhouette Score': [silhouette_avg],\n",
        "        'Davies-Bouldin Index': [db_score],\n",
        "        'Calinski-Harabasz Index': [ch_score],\n",
        "        'Global Score': [global_score]\n",
        "    })\n",
        "    \n",
        "    tabla_results_NS_df = pd.concat([tabla_results_NS_df, new_row], ignore_index=True)\n",
        "    \n",
        "    return tabla_results_NS_df\n",
        "\n",
        "def calcular_puntuacion_global_NS(silhouette_avg, db_score, ch_score):\n",
        "    # Normalizando los valores para que estén en el rango de 0 a 100\n",
        "    normalized_silhouette = (silhouette_avg + 1) * 50  # Ajustando el rango del Silhouette Score de -1 a 1 a 0 a 100\n",
        "    normalized_davies_bouldin = (1 - db_score) * 50  # Ajustando el rango del Davies-Bouldin Index de 0 a 1 a 0 a 100\n",
        "\n",
        "    # Calculando el puntaje global promediando los puntajes normalizados\n",
        "    global_score = (normalized_silhouette + normalized_davies_bouldin + ch_score) / 3\n",
        "\n",
        "    return global_score\n",
        "\n",
        "def encontrar_numero_optimo_clusters(X_train, X_val, model, max_clusters=10, plot_grafica = 'NO'):\n",
        "    wcss = []\n",
        "    silhouette_scores = []\n",
        "    davies_bouldin_scores = []\n",
        "    calinski_harabasz_scores = []\n",
        "    global_scores = []\n",
        "\n",
        "    for i in range(2, max_clusters+1):\n",
        "        model.n_clusters = i\n",
        "        model.fit(X_train)\n",
        "\n",
        "        # Predecir las etiquetas para los datos de validación\n",
        "        labels = model.predict(X_val)\n",
        "\n",
        "        # Calcular las métricas\n",
        "        silhouette = silhouette_score(X_val, labels)\n",
        "        davies_bouldin = davies_bouldin_score(X_val, labels)\n",
        "        calinski_harabasz = calinski_harabasz_score(X_val, labels)\n",
        "        \n",
        "        global_score = calcular_puntuacion_global_NS(silhouette, davies_bouldin, calinski_harabasz)\n",
        "        \n",
        "        wcss.append(model.inertia_)\n",
        "        silhouette_scores.append(silhouette)\n",
        "        davies_bouldin_scores.append(davies_bouldin)\n",
        "        calinski_harabasz_scores.append(calinski_harabasz)\n",
        "        global_scores.append(global_score)\n",
        "\n",
        "    if plot_grafica == 'SI':\n",
        "        # Plot para el método del codo\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(range(2, max_clusters+1), wcss, marker='o')\n",
        "        plt.title('Método del Codo')\n",
        "        plt.xlabel('Número de Clústeres')\n",
        "        plt.ylabel('WCSS')\n",
        "        plt.show()\n",
        "\n",
        "        # Plot para las métricas de validación\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.plot(range(2, max_clusters+1), silhouette_scores, marker='o', label='Silhouette Score')\n",
        "        plt.plot(range(2, max_clusters+1), davies_bouldin_scores, marker='o', label='Davies-Bouldin Index')\n",
        "        plt.plot(range(2, max_clusters+1), calinski_harabasz_scores, marker='o', label='Calinski-Harabasz Index')\n",
        "        plt.plot(range(2, max_clusters+1), global_scores, marker='o', label='Global Score')\n",
        "        plt.title('Métricas de Validación para Diferentes Números de Clústeres')\n",
        "        plt.xlabel('Número de Clústeres')\n",
        "        plt.ylabel('Valor de la Métrica')\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "\n",
        "    # Encontrar el número óptimo de clústeres basado en el puntaje global\n",
        "    numero_optimo_clusters = range(2, max_clusters+1)[np.argmax(global_scores)]\n",
        "    return numero_optimo_clusters\n",
        "\n",
        "def invert_preprocessor(preprocessor, data_prep):\n",
        "    # Obtén las columnas numéricas y categóricas del preprocesador\n",
        "    numeric_cols = preprocessor.transformers_[0][2]\n",
        "    cat_cols = preprocessor.transformers_[1][2]\n",
        "    \n",
        "    # Invertir la transformación para los datos numéricos\n",
        "    scaler = preprocessor.named_transformers_['scale']\n",
        "    numeric_data_transformed = data_prep[numeric_cols]\n",
        "    original_numeric_data = scaler.inverse_transform(numeric_data_transformed)\n",
        "    \n",
        "    # Invertir la transformación para los datos categóricos\n",
        "    onehot = preprocessor.named_transformers_['onehot']\n",
        "    encoded_columns = onehot.get_feature_names_out(cat_cols)\n",
        "    categorical_data_transformed = data_prep[encoded_columns]\n",
        "    original_categorical_data = onehot.inverse_transform(categorical_data_transformed)\n",
        "    \n",
        "    # Combinar las columnas invertidas con las columnas restantes\n",
        "    original_data = pd.DataFrame(original_numeric_data, columns=numeric_cols)\n",
        "    for i, col in enumerate(cat_cols):\n",
        "        original_data[col] = original_categorical_data[:, i]\n",
        "    \n",
        "    # Combinar las columnas originales si hay columnas que se pasaron sin transformar (remainder='passthrough')\n",
        "    if preprocessor.remainder == 'passthrough':\n",
        "        passthrough_cols = data_prep.columns.difference(numeric_cols + list(encoded_columns))\n",
        "        passthrough_data = data_prep[passthrough_cols]\n",
        "        original_data = pd.concat([original_data, passthrough_data.reset_index(drop=True)], axis=1)\n",
        "    \n",
        "    return original_data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Procesamiento de datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Carga de la URL de donde se encuentran los datos \n",
        "# (I) Introducir valor de nombreArchivo y variar la ruta en local donde se guardan los datos\n",
        "nombreArchivo = 'Tables_S1_to_S11' #nombre del archivo del dataset\n",
        "url_datos = f'C:/Users/danie/OneDrive/Documentos/Master/Lusku/TFM/Proposiciones/Deteccion Cancer/Datos/{nombreArchivo}.xlsx'\n",
        "\n",
        "# Se procede a hacer una carga de los datos. \n",
        "df_original= pd.read_excel(url_datos, sheet_name='Table S6')\n",
        "\n",
        "# Crear una copia del DataFrame original para realizar los cambios\n",
        "df6 = df_original.copy()\n",
        "\n",
        "# Recorrer las columnas del DataFrame para eliminar las cadenas de texto : ['*', '**']\n",
        "for columna in df6.columns:\n",
        "    if df6[columna].dtype == 'object':\n",
        "        # Aplicar la sustitución para cada secuencia en secuencias_a_buscar\n",
        "        for secuencia in ['*', '**']:\n",
        "            df6[columna] = df6[columna].apply(lambda x: x.replace(secuencia, '') if isinstance(x, str) and secuencia in x else x)\n",
        "\n",
        "df_prep = df6.apply(convert_to_numeric)\n",
        "\n",
        "# Relleno de nulos de la variable \"AJCC Stage\"\n",
        "df_prep[\"AJCC Stage\"] = df_prep[\"AJCC Stage\"].fillna(\"0\")\n",
        "\n",
        "# Calcular la media solo para las columnas numéricas\n",
        "numeric_columns = df_prep.select_dtypes(include=['number'])\n",
        "mean_values = numeric_columns.mean()\n",
        "\n",
        "# Rellenar los valores nulos con la media correspondiente\n",
        "df = df_prep.copy()  # Copiar el DataFrame preprocesado para evitar modificarlo\n",
        "for col in mean_values.index:\n",
        "    df[col].fillna(mean_values[col], inplace=True)\n",
        "\n",
        "\n",
        "# Binarización \"Tumor Type\" 0 -> NO CANCER; 1 -> SI CANCER + 'CancerSEEK Test Result'\n",
        "df['Tumor type'] = df['Tumor type'].apply(lambda x: 0 if str(x).strip().lower() == \"normal\" else 1).astype(int)\n",
        "#df['CancerSEEK Test Result'] = df['CancerSEEK Test Result'].apply(lambda x: 0 if str(x).strip().lower() == \"negative\" else 1).astype(int)\n",
        "\n",
        "# Conservar solo las columnas 'CA19-9 (U/ml)', 'CA-125 (U/ml)','HGF (pg/ml)','OPN (pg/ml)', 'Omega score', 'Prolactin (pg/ml)', 'CEA (pg/ml)', 'Myeloperoxidase (ng/ml)', 'TIMP-1 (pg/ml)'\n",
        "columnas_a_conservar = ['Tumor type','CA19-9 (U/ml)', 'CA-125 (U/ml)','HGF (pg/ml)','OPN (pg/ml)', 'Omega score', 'Prolactin (pg/ml)', 'CEA (pg/ml)', 'Myeloperoxidase (ng/ml)', 'TIMP-1 (pg/ml)']\n",
        "\n",
        "## --- CHECKPOINT ----- Datos limpios\n",
        "\n",
        "# Columna objetivo\n",
        "Y_column = df['Tumor type'].copy()\n",
        "# Reducción del dataFrame\n",
        "df_reduced = df[columnas_a_conservar].copy()\n",
        "#Copia del dataFrame entero\n",
        "df_full = df.copy()\n",
        "\n",
        "''' INICIO - Verificacion del information gain'''\n",
        "# Information Gain inicial\n",
        "df_discretizado = discretizar_df_arboles_1(df_reduced.drop(columns=['Tumor type']))\n",
        "df_reduced_discretizado_escalated = escalado_dataFrame(df_discretizado)\n",
        "calcular_ganancia_informacion(df_reduced_discretizado_escalated, Y_column, imprimir = \"NO\")\n",
        "\n",
        "# Information Gain usando arboles de decision (acorde a : \"..the cancer antigen markers are no longer the top predictive features. Instead, we observe the opposite trend for the purity and accuracy measurements..\")\n",
        "df_discretizado_full = discretizar_df_arboles(df_full, imprimir =\"NO\") # Columnas de este segundo enfoque guardadas en columnas_segundo_enfoque\n",
        "df_discretizado_reduced = discretizar_df_arboles(df_reduced,imprimir =\"NO\")\n",
        "\n",
        "''' FIN - Verificacion del information gain'''\n",
        "\n",
        "# Crear un DataFrame para almacenar los resultados en una tabla para luego poder compararlos - APRENDIZAJE SUPERVISADO\n",
        "tabla_results_df = pd.DataFrame(columns=['Model', 'Approach', 'Set', 'Accuracy', 'Precision', 'Recall', 'F1-Score', 'Adjusted Rand Index', 'Mean Squared Error', 'R-squared','AUC-ROC', 'TN', 'FP', 'FN', 'TP', 'Global Score'])\n",
        "\n",
        "# Crear un DataFrame para almacenar los resultados en una tabla para luego poder compararlos - APRENDIZAJE NO SUPERVISADO\n",
        "tabla_results_NS_df = pd.DataFrame(columns=['Model', 'Set', 'Silhouette Score', 'Davies-Bouldin Index', 'Calinski-Harabasz Index', 'Global Score'])\n",
        "\n",
        "## --- CHECKPOINT ----- Datos discretizados + Information gain\n",
        "# TODO Una vez hechos los modelos, habrá que estudiar cómo influye el usar esta serie de variables en la predicción\n",
        "columnas_segundo_enfoque = ['Tumor type','OPN (pg/ml)','IL-6 (pg/ml)','IL-8 (pg/ml)','HGF (pg/ml)','Prolactin (pg/ml)','Omega score','GDF15 (ng/ml)','CYFRA 21-1 (pg/ml)','Myeloperoxidase (ng/ml)','sEGFR (pg/ml)']\n",
        "df_reduced_segundo_enfoque = df[columnas_segundo_enfoque].copy()\n",
        "\n",
        "\n",
        "''' Valores para X\n",
        "1. df : entero, limpio, sin normalizar ni discretizar\n",
        "2. df_reduced : reducido, limpio, sin normalizar ni discretizar\n",
        "3. df_reduced_discretizado_escalated --> Acorde a la Tabla 1\n",
        "4. df_discretizado_full.drop(columns=['Tumor type']) --> Acorde a la Figura S3 (usando todas las variables) discretizado con arbol de decisión\n",
        "5. df_discretizado_reduced.drop(columns=['Tumor type']) --> Acorde a la Figura S3; discretizado con arbol de decisión\n",
        "'''\n",
        "''' Enfoque 1 '''\n",
        "y = df_reduced['Tumor type']\n",
        "X = df_reduced.drop(columns='Tumor type')\n",
        "\n",
        "X_train, X_val, X_test, y_train, y_val, y_test = split_data(X, y, train_size=0.6, val_size=0.2, test_size=0.2, random_state=42)\n",
        "\n",
        "numeric_cols = X_train.select_dtypes(include=['float64', 'int']).columns.to_list()\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns.to_list()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "                   [('scale', StandardScaler(), numeric_cols),\n",
        "                    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)],\n",
        "                remainder = 'passthrough',\n",
        "                verbose_feature_names_out = False\n",
        "               ).set_output(transform=\"pandas\")\n",
        "\n",
        "# TODO DUDA - Añadir un proceso de discretización? Puede mejorar el resultado. Ver cómo influye usar la función 'discretizar_df_arboles' ó la función sklearn.preprocessing.KBinsDiscretizer\n",
        "X_train_prep = preprocessor.fit_transform(X_train)\n",
        "X_val_prep = preprocessor.transform(X_val)\n",
        "X_test_prep  = preprocessor.transform(X_test)\n",
        "df_red_prep = preprocessor.transform(df_reduced)  \n",
        "\n",
        "X_prep = np.vstack((X_train_prep, X_val_prep, X_test_prep))\n",
        "\n",
        "''' Enfoque 2 '''\n",
        "y = df_reduced_segundo_enfoque['Tumor type']\n",
        "X = df_reduced_segundo_enfoque.drop(columns='Tumor type')\n",
        "\n",
        "X_train_2, X_val_2, X_test_2, y_train_2, y_val_2, y_test_2 = split_data(X, y, train_size=0.6, val_size=0.2, test_size=0.2, random_state=42)\n",
        "\n",
        "numeric_cols_2 = X_train_2.select_dtypes(include=['float64', 'int']).columns.to_list()\n",
        "cat_cols_2 = X_train_2.select_dtypes(include=['object', 'category']).columns.to_list()\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "                   [('scale', StandardScaler(), numeric_cols_2),\n",
        "                    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols)],\n",
        "                remainder = 'passthrough',\n",
        "                verbose_feature_names_out = False\n",
        "               ).set_output(transform=\"pandas\")\n",
        "\n",
        "# TODO DUDA - Añadir un proceso de discretización? Puede mejorar el resultado. Ver cómo influye usar la función 'discretizar_df_arboles' ó la función sklearn.preprocessing.KBinsDiscretizer\n",
        "X_train_prep_2 = preprocessor.fit_transform(X_train_2)\n",
        "X_val_prep_2 = preprocessor.transform(X_val_2)\n",
        "X_test_prep_2  = preprocessor.transform(X_test_2)\n",
        "\n",
        "X_prep_2 = np.vstack((X_train_prep_2, X_val_prep_2, X_test_prep_2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aprendizaje supervisado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regresión lineal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.42597539 -0.40579938 -0.39606393 -0.42963374 -0.4116208 ]\n",
            "Média métricas de validación cruzada: -0.4138186485421042\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8026\n",
            " - Precision: 0.8303\n",
            " - Recall: 0.8026\n",
            " - F1-Score: 0.8018\n",
            " - Adjusted Rand Index: 0.3655\n",
            " - Mean Squared Error: 0.1974\n",
            " - R-squared: 0.2014\n",
            " - Área bajo la curva : 0.815\n",
            " - Confusion Matrix: \n",
            "[[452  35]\n",
            " [180 422]]\n",
            " - Global Score : 76.6\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.7747\n",
            " - Precision: 0.8163\n",
            " - Recall: 0.7747\n",
            " - F1-Score: 0.7718\n",
            " - Adjusted Rand Index: 0.2997\n",
            " - Mean Squared Error: 0.2253\n",
            " - R-squared: 0.0910\n",
            " - Área bajo la curva : 0.789\n",
            " - Confusion Matrix: \n",
            "[[155  10]\n",
            " [ 72 127]]\n",
            " - Global Score : 73.86\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7610\n",
            " - Precision: 0.7970\n",
            " - Recall: 0.7610\n",
            " - F1-Score: 0.7597\n",
            " - Adjusted Rand Index: 0.2702\n",
            " - Mean Squared Error: 0.2390\n",
            " - R-squared: 0.0298\n",
            " - Área bajo la curva : 0.777\n",
            " - Confusion Matrix: \n",
            "[[145  15]\n",
            " [ 72 132]]\n",
            " - Global Score : 72.02\n",
            "\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.44593911 -0.38913211 -0.51139359 -0.42922268 -0.42891358]\n",
            "Média métricas de validación cruzada: -0.4409202133490634\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8246\n",
            " - Precision: 0.8463\n",
            " - Recall: 0.8246\n",
            " - F1-Score: 0.8244\n",
            " - Adjusted Rand Index: 0.4209\n",
            " - Mean Squared Error: 0.1754\n",
            " - R-squared: 0.2905\n",
            " - Área bajo la curva : 0.835\n",
            " - Confusion Matrix: \n",
            "[[454  33]\n",
            " [158 444]]\n",
            " - Global Score : 78.98\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.7995\n",
            " - Precision: 0.8271\n",
            " - Recall: 0.7995\n",
            " - F1-Score: 0.7983\n",
            " - Adjusted Rand Index: 0.3568\n",
            " - Mean Squared Error: 0.2005\n",
            " - R-squared: 0.1907\n",
            " - Área bajo la curva : 0.810\n",
            " - Confusion Matrix: \n",
            "[[153  12]\n",
            " [ 61 138]]\n",
            " - Global Score : 76.23\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7940\n",
            " - Precision: 0.8130\n",
            " - Recall: 0.7940\n",
            " - F1-Score: 0.7943\n",
            " - Adjusted Rand Index: 0.3438\n",
            " - Mean Squared Error: 0.2060\n",
            " - R-squared: 0.1636\n",
            " - Área bajo la curva : 0.804\n",
            " - Confusion Matrix: \n",
            "[[142  18]\n",
            " [ 57 147]]\n",
            " - Global Score : 75.12\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos supervisados entrenados/regresionLineal_e2_1.joblib']"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Crear una instancia del modelo de regresión lineal\n",
        "model_LR = LinearRegression()\n",
        "model_name = \"Regresion Lineal\"\n",
        "\n",
        "# Definir un umbral\n",
        "umbral = 0.5\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_LR, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_LR.fit(X_train_prep, y_train)\n",
        "\n",
        "#print(\"Fase de ENTRENAMIENTO estadísticas :\")\n",
        "#mostrar_score(X_train_prep, y_train, model_LR)\n",
        "mostrar_estadisticas_guardar_tabla(y_train, np.where(model_LR.predict(X_train_prep) > umbral, 1, 0), \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "# Predecir en el conjunto de validación\n",
        "y_pred = model_LR.predict(X_val_prep)\n",
        "\n",
        "# Binarizar las predicciones\n",
        "y_pred_bin = np.where(y_pred > umbral, 1, 0)\n",
        "\n",
        "y_test_pred = model_LR.predict(X_test_prep)\n",
        "y_test_pred_bin = np.where(y_test_pred > umbral, 1, 0)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_pred_bin, \"Validation\",  model_name, enfoque = \"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred_bin, \"Test\", model_name, enfoque = \"1\", print_roc = \"NO\")\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "model_LR_2 = LinearRegression()\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_LR_2, X_train_prep_2, y_train_2)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_LR_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "#print(\"Fase de ENTRENAMIENTO estadísticas :\")\n",
        "#mostrar_score(X_train_prep, y_train, model_LR)\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, np.where(model_LR_2.predict(X_train_prep_2) > umbral, 1, 0), \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "# Predecir en el conjunto de validación\n",
        "y_val_pred_2 = model_LR_2.predict(X_val_prep_2)\n",
        "\n",
        "# Binarizar las predicciones\n",
        "y_val_pred_bin_2 = np.where(y_val_pred_2 > umbral, 1, 0)\n",
        "\n",
        "y_test_pred_2 = model_LR_2.predict(X_test_prep_2)\n",
        "y_test_pred_bin_2 = np.where(y_test_pred_2 > umbral, 1, 0)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_bin_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_bin_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "\n",
        "dump(model_LR, 'Modelos supervisados entrenados/regresionLineal_e1_1.joblib')\n",
        "dump(model_LR_2, 'Modelos supervisados entrenados/regresionLineal_e2_1.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regresión logística"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regresión Logística\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.4336743  -0.4175068  -0.33864273 -0.4175068  -0.38996632]\n",
            "Média métricas de validación cruzada: -0.39945938904769146\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8430\n",
            " - Precision: 0.8488\n",
            " - Recall: 0.8430\n",
            " - F1-Score: 0.8434\n",
            " - Adjusted Rand Index: 0.4700\n",
            " - Mean Squared Error: 0.1570\n",
            " - R-squared: 0.3648\n",
            " - Área bajo la curva : 0.847\n",
            " - Confusion Matrix: \n",
            "[[431  56]\n",
            " [115 487]]\n",
            " - Global Score : 80.31\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8324\n",
            " - Precision: 0.8448\n",
            " - Recall: 0.8324\n",
            " - F1-Score: 0.8325\n",
            " - Adjusted Rand Index: 0.4405\n",
            " - Mean Squared Error: 0.1676\n",
            " - R-squared: 0.3238\n",
            " - Área bajo la curva : 0.839\n",
            " - Confusion Matrix: \n",
            "[[150  15]\n",
            " [ 46 153]]\n",
            " - Global Score : 79.46\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7995\n",
            " - Precision: 0.8137\n",
            " - Recall: 0.7995\n",
            " - F1-Score: 0.8000\n",
            " - Adjusted Rand Index: 0.3569\n",
            " - Mean Squared Error: 0.2005\n",
            " - R-squared: 0.1859\n",
            " - Área bajo la curva : 0.808\n",
            " - Confusion Matrix: \n",
            "[[140  20]\n",
            " [ 53 151]]\n",
            " - Global Score : 75.52\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8402\n",
            " - Precision: 0.8459\n",
            " - Recall: 0.8402\n",
            " - F1-Score: 0.8407\n",
            " - Adjusted Rand Index: 0.4625\n",
            " - Mean Squared Error: 0.1598\n",
            " - R-squared: 0.3537\n",
            " - Área bajo la curva : 0.844\n",
            " - Confusion Matrix: \n",
            "[[429  58]\n",
            " [116 486]]\n",
            " - Global Score : 79.95\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8489\n",
            " - Precision: 0.8630\n",
            " - Recall: 0.8489\n",
            " - F1-Score: 0.8489\n",
            " - Adjusted Rand Index: 0.4855\n",
            " - Mean Squared Error: 0.1511\n",
            " - R-squared: 0.3903\n",
            " - Área bajo la curva : 0.856\n",
            " - Confusion Matrix: \n",
            "[[154  11]\n",
            " [ 44 155]]\n",
            " - Global Score : 81.59\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8434\n",
            " - Precision: 0.8513\n",
            " - Recall: 0.8434\n",
            " - F1-Score: 0.8440\n",
            " - Adjusted Rand Index: 0.4703\n",
            " - Mean Squared Error: 0.1566\n",
            " - R-squared: 0.3643\n",
            " - Área bajo la curva : 0.849\n",
            " - Confusion Matrix: \n",
            "[[143  17]\n",
            " [ 40 164]]\n",
            " - Global Score : 80.47\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos supervisados entrenados/regresionLogistica_e2_1.joblib']"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model_name = \"Regresión Logística\"\n",
        "print(model_name)\n",
        "# Crear una instancia del modelo de regresión logística\n",
        "model_LogR = LogisticRegression()\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_LogR, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_LogR.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = model_LogR.predict(X_train_prep)\n",
        "y_val_pred = model_LogR.predict(X_val_prep)\n",
        "y_test_pred = model_LogR.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "model_LogR_2 = LogisticRegression()\n",
        "\n",
        "model_LogR_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "y_train_pred_2 = model_LogR_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = model_LogR_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = model_LogR_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(model_LogR, 'Modelos supervisados entrenados/regresionLogistica_e1_1.joblib')\n",
        "dump(model_LogR_2, 'Modelos supervisados entrenados/regresionLogistica_e2_1.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Arbol de decisión"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Árbol de Decisión\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m# Realizar la búsqueda en rejilla con validación cruzada\u001b[39;00m\n\u001b[0;32m     20\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m grid_search_2 \u001b[38;5;241m=\u001b[39m GridSearchCV(model_DT_leaf, param_grid, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_prep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Mostrar los resultados\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMejores hiperparámetros: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgrid_search\u001b[38;5;241m.\u001b[39mbest_params_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1422\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1420\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1421\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1422\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    846\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    847\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    848\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    849\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    850\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    852\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    853\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    854\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    855\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    856\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    857\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    860\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[38;5;66;03m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout) \u001b[38;5;241m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1708\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "model_name = \"Árbol de Decisión\"\n",
        "print(model_name)\n",
        "model_DT_leaf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# Definir los parámetros a ajustar\n",
        "params = {'max_leaf_nodes': range(2, 50)}  # Probando diferentes valores para max_leaf_nodes\n",
        "\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'splitter': ['best', 'random'],\n",
        "    'max_depth': [None,5, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_leaf_nodes': range(2, 50)\n",
        "}\n",
        "\n",
        "# Realizar la búsqueda en rejilla con validación cruzada\n",
        "grid_search = grid_search_2 = GridSearchCV(model_DT_leaf, param_grid, n_jobs=-1, cv=5)\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(f\"Mejores hiperparámetros: {grid_search.best_params_}\")\n",
        "\n",
        "model_DT = grid_search.best_estimator_\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model_DT.fit(X_train_prep, y_train)\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_DT, X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = model_DT.predict(X_train_prep)\n",
        "y_val_pred = model_DT.predict(X_val_prep)\n",
        "y_test_pred = model_DT.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "dump(model_DT, 'Modelos supervisados entrenados/arbolesDecision_e1_1.joblib')\n",
        "\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "grid_search_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "model_DT_2 = grid_search_2.best_estimator_\n",
        "\n",
        "model_DT_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "y_train_pred_2 = model_DT_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = model_DT_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = model_DT_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(model_DT_2, 'Modelos supervisados entrenados/arbolesDecision_e2_1.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Random Forest\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.35192785 -0.31037119 -0.27925195 -0.28734789 -0.33942212]\n",
            "Média métricas de validación cruzada: -0.3136641980878063\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[487   0]\n",
            " [  0 602]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9121\n",
            " - Precision: 0.9129\n",
            " - Recall: 0.9121\n",
            " - F1-Score: 0.9118\n",
            " - Adjusted Rand Index: 0.6783\n",
            " - Mean Squared Error: 0.0879\n",
            " - R-squared: 0.6453\n",
            " - Área bajo la curva : 0.909\n",
            " - Confusion Matrix: \n",
            "[[144  21]\n",
            " [ 11 188]]\n",
            " - Global Score : 88.0\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9093\n",
            " - Precision: 0.9098\n",
            " - Recall: 0.9093\n",
            " - F1-Score: 0.9090\n",
            " - Adjusted Rand Index: 0.6692\n",
            " - Mean Squared Error: 0.0907\n",
            " - R-squared: 0.6320\n",
            " - Área bajo la curva : 0.905\n",
            " - Confusion Matrix: \n",
            "[[139  21]\n",
            " [ 12 192]]\n",
            " - Global Score : 87.61\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[487   0]\n",
            " [  0 602]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9176\n",
            " - Precision: 0.9178\n",
            " - Recall: 0.9176\n",
            " - F1-Score: 0.9174\n",
            " - Adjusted Rand Index: 0.6966\n",
            " - Mean Squared Error: 0.0824\n",
            " - R-squared: 0.6674\n",
            " - Área bajo la curva : 0.915\n",
            " - Confusion Matrix: \n",
            "[[147  18]\n",
            " [ 12 187]]\n",
            " - Global Score : 88.84\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9478\n",
            " - Precision: 0.9491\n",
            " - Recall: 0.9478\n",
            " - F1-Score: 0.9476\n",
            " - Adjusted Rand Index: 0.8015\n",
            " - Mean Squared Error: 0.0522\n",
            " - R-squared: 0.7881\n",
            " - Área bajo la curva : 0.943\n",
            " - Confusion Matrix: \n",
            "[[145  15]\n",
            " [  4 200]]\n",
            " - Global Score : 92.59\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos supervisados entrenados/randomForest_e2_1.joblib']"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "model_name = \"Random Forest\"\n",
        "print(model_name)\n",
        "RF = RandomForestClassifier()\n",
        "\n",
        "rf_params = {\n",
        "    \"n_estimators\": [100, 200, 300, 400, 500],  \n",
        "    \"max_depth\": [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100],  \n",
        "    \"max_features\": [None, \"sqrt\", \"log2\"], \n",
        "    \"min_samples_split\": [2, 5, 10], \n",
        "    \"min_samples_leaf\": [1, 2, 4], \n",
        "    \"bootstrap\": [True, False]  \n",
        "}\n",
        "'''\n",
        "rf_params = {\n",
        "    'n_estimators': [int(x) for x in np.linspace(start=200, stop=2000, num=10)],\n",
        "    'max_features': ['auto', 'sqrt'],\n",
        "    'max_depth': [int(x) for x in np.linspace(10, 110, num=11)],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "'''\n",
        "random_search = random_search_2 = RandomizedSearchCV(estimator=RF,\n",
        "                                   param_distributions=rf_params,\n",
        "                                   n_iter=100,  \n",
        "                                   cv=5,  \n",
        "                                   verbose=0, \n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1) \n",
        "\n",
        "# random_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# print(\"Mejores parámetros encontrados:\", random_search.best_params_)\n",
        "# {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': False}\n",
        "\n",
        "# model_RF = random_search.best_estimator_\n",
        "params = {'n_estimators': 300, 'min_samples_split': 5, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': None, 'bootstrap': False}\n",
        "model_RF = RandomForestClassifier(**params)\n",
        "model_RF.fit(X_train_prep, y_train)\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_RF, X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = model_RF.predict(X_train_prep)\n",
        "y_val_pred = model_RF.predict(X_val_prep)\n",
        "y_test_pred = model_RF.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "dump(model_RF, 'Modelos supervisados entrenados/randomForest_e1_1.joblib')\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "# random_search_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "# print(\"Mejores parámetros encontrados:\", random_search.best_params_)\n",
        "# {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 50, 'bootstrap': False}\n",
        "\n",
        "# model_RF_2 = random_search_2.best_estimator_\n",
        "params_2 = {'n_estimators': 100, 'min_samples_split': 10, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 50, 'bootstrap': False}\n",
        "\n",
        "model_RF_2 = RandomForestClassifier(**params_2)\n",
        "model_RF_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "y_train_pred_2 = model_RF_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = model_RF_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = model_RF_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(model_RF_2, 'Modelos supervisados entrenados/randomForest_e2_1.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KNN\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.9633\n",
            " - Precision: 0.9634\n",
            " - Recall: 0.9633\n",
            " - F1-Score: 0.9633\n",
            " - Adjusted Rand Index: 0.8583\n",
            " - Mean Squared Error: 0.0367\n",
            " - R-squared: 0.8514\n",
            " - Área bajo la curva : 0.963\n",
            " - Confusion Matrix: \n",
            "[[470  17]\n",
            " [ 23 579]]\n",
            " - Global Score : 95.11\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9368\n",
            " - Precision: 0.9388\n",
            " - Recall: 0.9368\n",
            " - F1-Score: 0.9369\n",
            " - Adjusted Rand Index: 0.7626\n",
            " - Mean Squared Error: 0.0632\n",
            " - R-squared: 0.7450\n",
            " - Área bajo la curva : 0.939\n",
            " - Confusion Matrix: \n",
            "[[159   6]\n",
            " [ 17 182]]\n",
            " - Global Score : 91.94\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9286\n",
            " - Precision: 0.9286\n",
            " - Recall: 0.9286\n",
            " - F1-Score: 0.9286\n",
            " - Adjusted Rand Index: 0.7339\n",
            " - Mean Squared Error: 0.0714\n",
            " - R-squared: 0.7100\n",
            " - Área bajo la curva : 0.928\n",
            " - Confusion Matrix: \n",
            "[[147  13]\n",
            " [ 13 191]]\n",
            " - Global Score : 90.43\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[487   0]\n",
            " [  0 602]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9505\n",
            " - Precision: 0.9522\n",
            " - Recall: 0.9505\n",
            " - F1-Score: 0.9506\n",
            " - Adjusted Rand Index: 0.8115\n",
            " - Mean Squared Error: 0.0495\n",
            " - R-squared: 0.8005\n",
            " - Área bajo la curva : 0.953\n",
            " - Confusion Matrix: \n",
            "[[161   4]\n",
            " [ 14 185]]\n",
            " - Global Score : 93.71\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9533\n",
            " - Precision: 0.9553\n",
            " - Recall: 0.9533\n",
            " - F1-Score: 0.9534\n",
            " - Adjusted Rand Index: 0.8214\n",
            " - Mean Squared Error: 0.0467\n",
            " - R-squared: 0.8104\n",
            " - Área bajo la curva : 0.956\n",
            " - Confusion Matrix: \n",
            "[[157   3]\n",
            " [ 14 190]]\n",
            " - Global Score : 94.11\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\danie\\AppData\\Local\\Temp\\ipykernel_25016\\2151088884.py:271: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  tabla_results_df = pd.concat([tabla_results_df, new_row], ignore_index=True)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos supervisados entrenados/knn_e2_1.joblib']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "model_name = \"KNN\"\n",
        "print(model_name)\n",
        "# Parámetros de búsqueda para KNN\n",
        "'''\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [3, 5, 7, 9, 11],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski']\n",
        "}\n",
        "'''\n",
        "\n",
        "param_grid_knn = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21],\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'minkowski', 'chebyshev'],\n",
        "    'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
        "    'leaf_size': [10, 20, 30, 40, 50]\n",
        "}\n",
        "'''\n",
        "best_params_knn = {\n",
        "    'algorithm': 'brute',\n",
        "    'leaf_size': 10,\n",
        "    'metric': 'hamming',\n",
        "    'n_neighbors': 3,\n",
        "    'weights': 'uniform'\n",
        "}\n",
        "\n",
        "# Best parameters for KNN (enfoque 2)\n",
        "best_params_knn_2 = {\n",
        "    'algorithm': 'brute',\n",
        "    'leaf_size': 10,\n",
        "    'metric': 'hamming',\n",
        "    'n_neighbors': 11,\n",
        "    'weights': 'distance'\n",
        "}\n",
        "'''\n",
        "best_params_knn = {\n",
        "    'algorithm': 'brute',\n",
        "    'leaf_size': 30,\n",
        "    'metric': 'hamming',\n",
        "    'n_neighbors': 7,\n",
        "    'weights': 'uniform'\n",
        "}\n",
        "\n",
        "# Best parameters for KNN (enfoque 2)\n",
        "best_params_knn_2 = {\n",
        "    'algorithm': 'brute',\n",
        "    'leaf_size': 30,\n",
        "    'metric': 'hamming',\n",
        "    'n_neighbors': 11,\n",
        "    'weights': 'distance'\n",
        "}\n",
        "\n",
        "#grid_search_knn = grid_search_knn_2 = GridSearchCV(KNeighborsClassifier(), param_grid_knn, cv=5, scoring='accuracy')\n",
        "grid_search_knn = KNeighborsClassifier(**best_params_knn)\n",
        "grid_search_knn_2 = KNeighborsClassifier(**best_params_knn_2)\n",
        "\n",
        "grid_search_knn.fit(X_train_prep, y_train)\n",
        "\n",
        "#print(\"Best parameters for KNN: \", grid_search_knn.best_params_)\n",
        "\n",
        "y_train_pred = grid_search_knn.predict(X_train_prep)\n",
        "y_val_pred = grid_search_knn.predict(X_val_prep)\n",
        "y_test_pred = grid_search_knn.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search_knn, 'Modelos supervisados entrenados/knn_e1_1.joblib')\n",
        "\n",
        "''' Enfoque 2 '''\n",
        "grid_search_knn_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "#print(\"Best parameters for KNN: \", grid_search_knn_2.best_params_) \n",
        "\n",
        "grid_search_knn_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "y_train_pred_2 = grid_search_knn_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = grid_search_knn_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = grid_search_knn_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "\n",
        "dump(grid_search_knn_2, 'Modelos supervisados entrenados/knn_e2_1.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8549\n",
            " - Precision: 0.8590\n",
            " - Recall: 0.8549\n",
            " - F1-Score: 0.8553\n",
            " - Adjusted Rand Index: 0.5034\n",
            " - Mean Squared Error: 0.1451\n",
            " - R-squared: 0.4131\n",
            " - Área bajo la curva : 0.858\n",
            " - Confusion Matrix: \n",
            "[[432  55]\n",
            " [103 499]]\n",
            " - Global Score : 81.65\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8489\n",
            " - Precision: 0.8524\n",
            " - Recall: 0.8489\n",
            " - F1-Score: 0.8492\n",
            " - Adjusted Rand Index: 0.4855\n",
            " - Mean Squared Error: 0.1511\n",
            " - R-squared: 0.3903\n",
            " - Área bajo la curva : 0.851\n",
            " - Confusion Matrix: \n",
            "[[145  20]\n",
            " [ 35 164]]\n",
            " - Global Score : 80.86\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8049\n",
            " - Precision: 0.8117\n",
            " - Recall: 0.8049\n",
            " - F1-Score: 0.8057\n",
            " - Adjusted Rand Index: 0.3703\n",
            " - Mean Squared Error: 0.1951\n",
            " - R-squared: 0.2082\n",
            " - Área bajo la curva : 0.809\n",
            " - Confusion Matrix: \n",
            "[[135  25]\n",
            " [ 46 158]]\n",
            " - Global Score : 75.67\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8843\n",
            " - Precision: 0.8890\n",
            " - Recall: 0.8843\n",
            " - F1-Score: 0.8846\n",
            " - Adjusted Rand Index: 0.5904\n",
            " - Mean Squared Error: 0.1157\n",
            " - R-squared: 0.5320\n",
            " - Área bajo la curva : 0.888\n",
            " - Confusion Matrix: \n",
            "[[450  37]\n",
            " [ 89 513]]\n",
            " - Global Score : 85.4\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8571\n",
            " - Precision: 0.8640\n",
            " - Recall: 0.8571\n",
            " - F1-Score: 0.8574\n",
            " - Adjusted Rand Index: 0.5089\n",
            " - Mean Squared Error: 0.1429\n",
            " - R-squared: 0.4235\n",
            " - Área bajo la curva : 0.862\n",
            " - Confusion Matrix: \n",
            "[[150  15]\n",
            " [ 37 162]]\n",
            " - Global Score : 82.16\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8407\n",
            " - Precision: 0.8469\n",
            " - Recall: 0.8407\n",
            " - F1-Score: 0.8412\n",
            " - Adjusted Rand Index: 0.4627\n",
            " - Mean Squared Error: 0.1593\n",
            " - R-squared: 0.3532\n",
            " - Área bajo la curva : 0.845\n",
            " - Confusion Matrix: \n",
            "[[141  19]\n",
            " [ 39 165]]\n",
            " - Global Score : 80.01\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos supervisados entrenados/svm_e2_1.joblib']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "model_name = \"SVM\"\n",
        "print(model_name)\n",
        "# Define la cuadrícula de parámetros para buscar\n",
        "'''param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid']\n",
        "}'''\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],\n",
        "    'gamma': [1, 0.1, 0.01, 0.001],\n",
        "    'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "    'degree': [2, 3, 4],              # Grados para el kernel polinómico\n",
        "    'coef0': [0.0, 0.1, 0.5, 1.0],     # Término independiente en los kernels polinómico y sigmoidal\n",
        "    'shrinking': [True, False],       # Si se debe usar la heurística de encogimiento\n",
        "    'decision_function_shape': ['ovo', 'ovr']  # Forma de la función de decisión para clasificación multiclase\n",
        "}\n",
        "\n",
        "# Crea un objeto GridSearchCV\n",
        "# grid_search_svm = grid_search_svm_2 = GridSearchCV(SVC(), param_grid, refit=True)\n",
        "\n",
        "best_param = {'C': 100, 'coef0': 0.0, 'decision_function_shape': 'ovo', 'degree': 2, 'gamma': 0.1, 'kernel': 'rbf', 'shrinking': True}\n",
        "# Ajusta el objeto GridSearchCV a los datos\n",
        "\n",
        "grid_search_svm = SVC(**best_param)\n",
        "\n",
        "grid_search_svm.fit(X_train_prep, y_train)\n",
        "\n",
        "# Imprime los mejores parámetros\n",
        "# print(\"Mejores parámetros: \", grid_search_svm.best_params_)\n",
        "\n",
        "y_train_pred = grid_search_svm.predict(X_train_prep)\n",
        "y_val_pred = grid_search_svm.predict(X_val_prep)\n",
        "y_test_pred = grid_search_svm.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search_svm, 'Modelos supervisados entrenados/svm_e1_1.joblib')\n",
        "\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "best_param_2 =  {'C': 100, 'coef0': 1.0, 'decision_function_shape': 'ovo', 'degree': 2, 'gamma': 0.1, 'kernel': 'poly', 'shrinking': True}\n",
        "grid_search_svm_2 = SVC(**best_param_2)\n",
        "grid_search_svm_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "# print(\"Best parameters for KNN: \", grid_search_svm_2.best_params_) \n",
        "\n",
        "grid_search_svm_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "y_train_pred_2 = grid_search_svm_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = grid_search_svm_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = grid_search_svm_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search_svm_2, 'Modelos supervisados entrenados/svm_e2_1.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Naive Bayes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Gaussiano"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes - Gaussian\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.51580524 -0.48367856 -0.44412578 -0.49770114 -0.46539216]\n",
            "Média métricas de validación cruzada: -0.4813405758378038\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.7668\n",
            " - Precision: 0.8266\n",
            " - Recall: 0.7668\n",
            " - F1-Score: 0.7620\n",
            " - Adjusted Rand Index: 0.2833\n",
            " - Mean Squared Error: 0.2332\n",
            " - R-squared: 0.0565\n",
            " - Área bajo la curva : 0.786\n",
            " - Confusion Matrix: \n",
            "[[471  16]\n",
            " [238 364]]\n",
            " - Global Score : 73.54\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.7830\n",
            " - Precision: 0.8457\n",
            " - Recall: 0.7830\n",
            " - F1-Score: 0.7779\n",
            " - Adjusted Rand Index: 0.3180\n",
            " - Mean Squared Error: 0.2170\n",
            " - R-squared: 0.1242\n",
            " - Área bajo la curva : 0.800\n",
            " - Confusion Matrix: \n",
            "[[163   2]\n",
            " [ 77 122]]\n",
            " - Global Score : 75.49\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7088\n",
            " - Precision: 0.7923\n",
            " - Recall: 0.7088\n",
            " - F1-Score: 0.6989\n",
            " - Adjusted Rand Index: 0.1707\n",
            " - Mean Squared Error: 0.2912\n",
            " - R-squared: -0.1821\n",
            " - Área bajo la curva : 0.735\n",
            " - Confusion Matrix: \n",
            "[[153   7]\n",
            " [ 99 105]]\n",
            " - Global Score : 67.68\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.7815\n",
            " - Precision: 0.8283\n",
            " - Recall: 0.7815\n",
            " - F1-Score: 0.7786\n",
            " - Adjusted Rand Index: 0.3158\n",
            " - Mean Squared Error: 0.2185\n",
            " - R-squared: 0.1159\n",
            " - Área bajo la curva : 0.798\n",
            " - Confusion Matrix: \n",
            "[[465  22]\n",
            " [216 386]]\n",
            " - Global Score : 74.83\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.7555\n",
            " - Precision: 0.8176\n",
            " - Recall: 0.7555\n",
            " - F1-Score: 0.7493\n",
            " - Adjusted Rand Index: 0.2586\n",
            " - Mean Squared Error: 0.2445\n",
            " - R-squared: 0.0134\n",
            " - Área bajo la curva : 0.773\n",
            " - Confusion Matrix: \n",
            "[[159   6]\n",
            " [ 83 116]]\n",
            " - Global Score : 72.3\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7445\n",
            " - Precision: 0.8070\n",
            " - Recall: 0.7445\n",
            " - F1-Score: 0.7396\n",
            " - Adjusted Rand Index: 0.2362\n",
            " - Mean Squared Error: 0.2555\n",
            " - R-squared: -0.0371\n",
            " - Área bajo la curva : 0.767\n",
            " - Confusion Matrix: \n",
            "[[152   8]\n",
            " [ 85 119]]\n",
            " - Global Score : 71.06\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos supervisados entrenados/nb_gauss_e2_1.joblib']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model_NB = GaussianNB()\n",
        "model_name = \"Naive Bayes - Gaussian\"\n",
        "print(model_name)\n",
        "\n",
        "param_grid = {\n",
        "    'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n",
        "}\n",
        "\n",
        "grid_search_nb = grid_search_nb_2 = GridSearchCV(model_NB, param_grid, cv=5, refit=True)\n",
        "\n",
        "grid_search_nb.fit(X_train_prep, y_train)\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_NB, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "grid_search_nb.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = grid_search_nb.predict(X_train_prep)\n",
        "y_val_pred = grid_search_nb.predict(X_val_prep)\n",
        "y_test_pred = grid_search_nb.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search_nb, 'Modelos supervisados entrenados/nb_gauss_e1_1.joblib')\n",
        "\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "\n",
        "grid_search_nb_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "y_train_pred_2 = grid_search_nb_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = grid_search_nb_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = grid_search_nb_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search_nb_2, 'Modelos supervisados entrenados/nb_gauss_e2_1.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Bernouilli"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes - Bernoulli\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.46923713 -0.47409982 -0.4175068  -0.51580524 -0.46539216]\n",
            "Média métricas de validación cruzada: -0.4684082305729117\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Training set :\n",
            " - Accuracy: 0.8246\n",
            " - Precision: 0.8304\n",
            " - Recall: 0.8246\n",
            " - F1-Score: 0.8251\n",
            " - Adjusted Rand Index: 0.4210\n",
            " - Mean Squared Error: 0.1754\n",
            " - R-squared: 0.2905\n",
            " - Área bajo la curva : 0.828\n",
            " - Confusion Matrix: \n",
            "[[421  66]\n",
            " [125 477]]\n",
            " - Global Score : 78.05\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9368\n",
            " - Precision: 0.9388\n",
            " - Recall: 0.9368\n",
            " - F1-Score: 0.9369\n",
            " - Adjusted Rand Index: 0.7626\n",
            " - Mean Squared Error: 0.0632\n",
            " - R-squared: 0.7450\n",
            " - Área bajo la curva : 0.939\n",
            " - Confusion Matrix: \n",
            "[[159   6]\n",
            " [ 17 182]]\n",
            " - Global Score : 91.94\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8049\n",
            " - Precision: 0.8178\n",
            " - Recall: 0.8049\n",
            " - F1-Score: 0.8055\n",
            " - Adjusted Rand Index: 0.3702\n",
            " - Mean Squared Error: 0.1951\n",
            " - R-squared: 0.2082\n",
            " - Área bajo la curva : 0.812\n",
            " - Confusion Matrix: \n",
            "[[140  20]\n",
            " [ 51 153]]\n",
            " - Global Score : 76.1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:1212: RuntimeWarning: invalid value encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Training set :\n",
            " - Accuracy: 0.8145\n",
            " - Precision: 0.8222\n",
            " - Recall: 0.8145\n",
            " - F1-Score: 0.8150\n",
            " - Adjusted Rand Index: 0.3951\n",
            " - Mean Squared Error: 0.1855\n",
            " - R-squared: 0.2497\n",
            " - Área bajo la curva : 0.819\n",
            " - Confusion Matrix: \n",
            "[[421  66]\n",
            " [136 466]]\n",
            " - Global Score : 76.96\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8159\n",
            " - Precision: 0.8243\n",
            " - Recall: 0.8159\n",
            " - F1-Score: 0.8162\n",
            " - Adjusted Rand Index: 0.3976\n",
            " - Mean Squared Error: 0.1841\n",
            " - R-squared: 0.2573\n",
            " - Área bajo la curva : 0.821\n",
            " - Confusion Matrix: \n",
            "[[144  21]\n",
            " [ 46 153]]\n",
            " - Global Score : 77.2\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7940\n",
            " - Precision: 0.8018\n",
            " - Recall: 0.7940\n",
            " - F1-Score: 0.7947\n",
            " - Adjusted Rand Index: 0.3439\n",
            " - Mean Squared Error: 0.2060\n",
            " - R-squared: 0.1636\n",
            " - Área bajo la curva : 0.799\n",
            " - Confusion Matrix: \n",
            "[[134  26]\n",
            " [ 49 155]]\n",
            " - Global Score : 74.43\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:624: FutureWarning: The default value for `force_alpha` will change to `True` in 1.4. To suppress this warning, manually set the value of `force_alpha`.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\naive_bayes.py:633: UserWarning: alpha too small will result in numeric errors, setting alpha = 1.0e-10. Use `force_alpha=True` to keep alpha unchanged.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos supervisados entrenados/nb_bernouilli_e2_1.joblib']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "model_NBB = BernoulliNB()\n",
        "model_name = \"Naive Bayes - Bernoulli\"\n",
        "print(model_name)\n",
        "\n",
        "param_grid = {\n",
        "    'alpha': [0.0, 0.1, 0.5, 1.0, 2.0],\n",
        "    'binarize': [None, 0.0, 0.1, 0.5, 1.0]\n",
        "}\n",
        "# Crea un objeto GridSearchCV\n",
        "grid_search_nbb = grid_search_nbb_2 = GridSearchCV(model_NBB, param_grid, cv=5, refit=True)\n",
        "\n",
        "# Ajusta el objeto GridSearchCV a los datos de entrenamiento preprocesados\n",
        "grid_search_nbb.fit(X_train_prep, y_train)\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model_NBB, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "grid_search_nbb.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = grid_search_nbb.predict(X_train_prep)\n",
        "y_pred = grid_search_nbb.predict(X_val_prep)\n",
        "y_test_pred = grid_search_nbb.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search_nbb, 'Modelos supervisados entrenados/nb_bernouilli_e1_1.joblib')\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "\n",
        "grid_search_nbb_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "y_train_pred_2 = grid_search_nbb_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = grid_search_nbb_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = grid_search_nbb_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search_nbb_2, 'Modelos supervisados entrenados/nb_bernouilli_e2_1.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danie\\.virtualenvs\\Repositorio_compartido-Xxik1eBC\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\.virtualenvs\\Repositorio_compartido-Xxik1eBC\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\.virtualenvs\\Repositorio_compartido-Xxik1eBC\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\.virtualenvs\\Repositorio_compartido-Xxik1eBC\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\.virtualenvs\\Repositorio_compartido-Xxik1eBC\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\.virtualenvs\\Repositorio_compartido-Xxik1eBC\\Lib\\site-packages\\sklearn\\ensemble\\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.31037119 -0.24419875 -0.26231153 -0.27091418 -0.31108551]\n",
            "Média métricas de validación cruzada: -0.27977623193693607\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[487   0]\n",
            " [  0 602]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9231\n",
            " - Precision: 0.9231\n",
            " - Recall: 0.9231\n",
            " - F1-Score: 0.9231\n",
            " - Adjusted Rand Index: 0.7152\n",
            " - Mean Squared Error: 0.0769\n",
            " - R-squared: 0.6896\n",
            " - Área bajo la curva : 0.922\n",
            " - Confusion Matrix: \n",
            "[[151  14]\n",
            " [ 14 185]]\n",
            " - Global Score : 89.75\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9203\n",
            " - Precision: 0.9206\n",
            " - Recall: 0.9203\n",
            " - F1-Score: 0.9204\n",
            " - Adjusted Rand Index: 0.7059\n",
            " - Mean Squared Error: 0.0797\n",
            " - R-squared: 0.6766\n",
            " - Área bajo la curva : 0.920\n",
            " - Confusion Matrix: \n",
            "[[147  13]\n",
            " [ 16 188]]\n",
            " - Global Score : 89.46\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.802571</td>\n",
              "      <td>0.830296</td>\n",
              "      <td>0.802571</td>\n",
              "      <td>0.801846</td>\n",
              "      <td>0.365453</td>\n",
              "      <td>0.197429</td>\n",
              "      <td>0.201379</td>\n",
              "      <td>0.814564</td>\n",
              "      <td>452</td>\n",
              "      <td>35</td>\n",
              "      <td>180</td>\n",
              "      <td>422</td>\n",
              "      <td>76.60</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.774725</td>\n",
              "      <td>0.816318</td>\n",
              "      <td>0.774725</td>\n",
              "      <td>0.771756</td>\n",
              "      <td>0.299727</td>\n",
              "      <td>0.225275</td>\n",
              "      <td>0.090970</td>\n",
              "      <td>0.788792</td>\n",
              "      <td>155</td>\n",
              "      <td>10</td>\n",
              "      <td>72</td>\n",
              "      <td>127</td>\n",
              "      <td>73.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresion Lineal</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.760989</td>\n",
              "      <td>0.796967</td>\n",
              "      <td>0.760989</td>\n",
              "      <td>0.759651</td>\n",
              "      <td>0.270159</td>\n",
              "      <td>0.239011</td>\n",
              "      <td>0.029779</td>\n",
              "      <td>0.776654</td>\n",
              "      <td>145</td>\n",
              "      <td>15</td>\n",
              "      <td>72</td>\n",
              "      <td>132</td>\n",
              "      <td>72.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.842975</td>\n",
              "      <td>0.848799</td>\n",
              "      <td>0.842975</td>\n",
              "      <td>0.843414</td>\n",
              "      <td>0.470046</td>\n",
              "      <td>0.157025</td>\n",
              "      <td>0.364817</td>\n",
              "      <td>0.846990</td>\n",
              "      <td>431</td>\n",
              "      <td>56</td>\n",
              "      <td>115</td>\n",
              "      <td>487</td>\n",
              "      <td>80.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.832418</td>\n",
              "      <td>0.844801</td>\n",
              "      <td>0.832418</td>\n",
              "      <td>0.832535</td>\n",
              "      <td>0.440458</td>\n",
              "      <td>0.167582</td>\n",
              "      <td>0.323770</td>\n",
              "      <td>0.838968</td>\n",
              "      <td>150</td>\n",
              "      <td>15</td>\n",
              "      <td>46</td>\n",
              "      <td>153</td>\n",
              "      <td>79.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.802198</td>\n",
              "      <td>0.817277</td>\n",
              "      <td>0.802198</td>\n",
              "      <td>0.802706</td>\n",
              "      <td>0.363533</td>\n",
              "      <td>0.197802</td>\n",
              "      <td>0.197059</td>\n",
              "      <td>0.810723</td>\n",
              "      <td>141</td>\n",
              "      <td>19</td>\n",
              "      <td>53</td>\n",
              "      <td>151</td>\n",
              "      <td>75.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.910927</td>\n",
              "      <td>0.910946</td>\n",
              "      <td>0.910927</td>\n",
              "      <td>0.910936</td>\n",
              "      <td>0.675114</td>\n",
              "      <td>0.089073</td>\n",
              "      <td>0.639692</td>\n",
              "      <td>0.910021</td>\n",
              "      <td>439</td>\n",
              "      <td>48</td>\n",
              "      <td>49</td>\n",
              "      <td>553</td>\n",
              "      <td>88.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.859890</td>\n",
              "      <td>0.859828</td>\n",
              "      <td>0.859890</td>\n",
              "      <td>0.859853</td>\n",
              "      <td>0.516736</td>\n",
              "      <td>0.140110</td>\n",
              "      <td>0.434628</td>\n",
              "      <td>0.858398</td>\n",
              "      <td>139</td>\n",
              "      <td>26</td>\n",
              "      <td>25</td>\n",
              "      <td>174</td>\n",
              "      <td>81.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Árbol de Decisión</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.851648</td>\n",
              "      <td>0.851536</td>\n",
              "      <td>0.851648</td>\n",
              "      <td>0.851173</td>\n",
              "      <td>0.493086</td>\n",
              "      <td>0.148352</td>\n",
              "      <td>0.397794</td>\n",
              "      <td>0.846752</td>\n",
              "      <td>129</td>\n",
              "      <td>31</td>\n",
              "      <td>23</td>\n",
              "      <td>181</td>\n",
              "      <td>80.30</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>487</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>602</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.914835</td>\n",
              "      <td>0.915895</td>\n",
              "      <td>0.914835</td>\n",
              "      <td>0.914512</td>\n",
              "      <td>0.687455</td>\n",
              "      <td>0.085165</td>\n",
              "      <td>0.656342</td>\n",
              "      <td>0.911238</td>\n",
              "      <td>144</td>\n",
              "      <td>21</td>\n",
              "      <td>10</td>\n",
              "      <td>189</td>\n",
              "      <td>88.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.909341</td>\n",
              "      <td>0.910202</td>\n",
              "      <td>0.909341</td>\n",
              "      <td>0.908917</td>\n",
              "      <td>0.669212</td>\n",
              "      <td>0.090659</td>\n",
              "      <td>0.631985</td>\n",
              "      <td>0.904289</td>\n",
              "      <td>138</td>\n",
              "      <td>22</td>\n",
              "      <td>11</td>\n",
              "      <td>193</td>\n",
              "      <td>87.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.854651</td>\n",
              "      <td>0.848485</td>\n",
              "      <td>0.848907</td>\n",
              "      <td>0.485299</td>\n",
              "      <td>0.151515</td>\n",
              "      <td>0.387105</td>\n",
              "      <td>0.852758</td>\n",
              "      <td>435</td>\n",
              "      <td>52</td>\n",
              "      <td>113</td>\n",
              "      <td>489</td>\n",
              "      <td>81.02</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.804945</td>\n",
              "      <td>0.811110</td>\n",
              "      <td>0.804945</td>\n",
              "      <td>0.805347</td>\n",
              "      <td>0.370244</td>\n",
              "      <td>0.195055</td>\n",
              "      <td>0.212913</td>\n",
              "      <td>0.808665</td>\n",
              "      <td>140</td>\n",
              "      <td>25</td>\n",
              "      <td>46</td>\n",
              "      <td>153</td>\n",
              "      <td>75.71</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>KNN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.774725</td>\n",
              "      <td>0.785429</td>\n",
              "      <td>0.774725</td>\n",
              "      <td>0.775488</td>\n",
              "      <td>0.299987</td>\n",
              "      <td>0.225275</td>\n",
              "      <td>0.085539</td>\n",
              "      <td>0.780821</td>\n",
              "      <td>133</td>\n",
              "      <td>27</td>\n",
              "      <td>55</td>\n",
              "      <td>149</td>\n",
              "      <td>72.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.854913</td>\n",
              "      <td>0.859023</td>\n",
              "      <td>0.854913</td>\n",
              "      <td>0.855308</td>\n",
              "      <td>0.503399</td>\n",
              "      <td>0.145087</td>\n",
              "      <td>0.413106</td>\n",
              "      <td>0.857984</td>\n",
              "      <td>432</td>\n",
              "      <td>55</td>\n",
              "      <td>103</td>\n",
              "      <td>499</td>\n",
              "      <td>81.65</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.848901</td>\n",
              "      <td>0.852435</td>\n",
              "      <td>0.848901</td>\n",
              "      <td>0.849227</td>\n",
              "      <td>0.485523</td>\n",
              "      <td>0.151099</td>\n",
              "      <td>0.390285</td>\n",
              "      <td>0.851454</td>\n",
              "      <td>145</td>\n",
              "      <td>20</td>\n",
              "      <td>35</td>\n",
              "      <td>164</td>\n",
              "      <td>80.86</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>SVN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.804945</td>\n",
              "      <td>0.811726</td>\n",
              "      <td>0.804945</td>\n",
              "      <td>0.805659</td>\n",
              "      <td>0.370256</td>\n",
              "      <td>0.195055</td>\n",
              "      <td>0.208211</td>\n",
              "      <td>0.809130</td>\n",
              "      <td>135</td>\n",
              "      <td>25</td>\n",
              "      <td>46</td>\n",
              "      <td>158</td>\n",
              "      <td>75.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.766758</td>\n",
              "      <td>0.826607</td>\n",
              "      <td>0.766758</td>\n",
              "      <td>0.762041</td>\n",
              "      <td>0.283319</td>\n",
              "      <td>0.233242</td>\n",
              "      <td>0.056513</td>\n",
              "      <td>0.785898</td>\n",
              "      <td>471</td>\n",
              "      <td>16</td>\n",
              "      <td>238</td>\n",
              "      <td>364</td>\n",
              "      <td>73.54</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.782967</td>\n",
              "      <td>0.845750</td>\n",
              "      <td>0.782967</td>\n",
              "      <td>0.777865</td>\n",
              "      <td>0.318004</td>\n",
              "      <td>0.217033</td>\n",
              "      <td>0.124227</td>\n",
              "      <td>0.800472</td>\n",
              "      <td>163</td>\n",
              "      <td>2</td>\n",
              "      <td>77</td>\n",
              "      <td>122</td>\n",
              "      <td>75.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Naive Bayes - Gaussian</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.708791</td>\n",
              "      <td>0.792288</td>\n",
              "      <td>0.708791</td>\n",
              "      <td>0.698914</td>\n",
              "      <td>0.170664</td>\n",
              "      <td>0.291209</td>\n",
              "      <td>-0.182108</td>\n",
              "      <td>0.735478</td>\n",
              "      <td>153</td>\n",
              "      <td>7</td>\n",
              "      <td>99</td>\n",
              "      <td>105</td>\n",
              "      <td>67.68</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.793388</td>\n",
              "      <td>0.809136</td>\n",
              "      <td>0.793388</td>\n",
              "      <td>0.793538</td>\n",
              "      <td>0.343655</td>\n",
              "      <td>0.206612</td>\n",
              "      <td>0.164234</td>\n",
              "      <td>0.801551</td>\n",
              "      <td>428</td>\n",
              "      <td>59</td>\n",
              "      <td>166</td>\n",
              "      <td>436</td>\n",
              "      <td>74.93</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.802573</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>0.785468</td>\n",
              "      <td>0.324632</td>\n",
              "      <td>0.214286</td>\n",
              "      <td>0.135313</td>\n",
              "      <td>0.793665</td>\n",
              "      <td>145</td>\n",
              "      <td>20</td>\n",
              "      <td>58</td>\n",
              "      <td>141</td>\n",
              "      <td>74.10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Naive Bayes - Bernoulli</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.755495</td>\n",
              "      <td>0.782567</td>\n",
              "      <td>0.755495</td>\n",
              "      <td>0.755042</td>\n",
              "      <td>0.258904</td>\n",
              "      <td>0.244505</td>\n",
              "      <td>0.007475</td>\n",
              "      <td>0.768382</td>\n",
              "      <td>140</td>\n",
              "      <td>20</td>\n",
              "      <td>69</td>\n",
              "      <td>135</td>\n",
              "      <td>70.99</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>487</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>602</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.923077</td>\n",
              "      <td>0.715184</td>\n",
              "      <td>0.076923</td>\n",
              "      <td>0.689600</td>\n",
              "      <td>0.922400</td>\n",
              "      <td>151</td>\n",
              "      <td>14</td>\n",
              "      <td>14</td>\n",
              "      <td>185</td>\n",
              "      <td>89.75</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>AdaBoost 1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.920330</td>\n",
              "      <td>0.920606</td>\n",
              "      <td>0.920330</td>\n",
              "      <td>0.920405</td>\n",
              "      <td>0.705872</td>\n",
              "      <td>0.079670</td>\n",
              "      <td>0.676593</td>\n",
              "      <td>0.920159</td>\n",
              "      <td>147</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "      <td>188</td>\n",
              "      <td>89.46</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      Model         Set  Accuracy  Precision    Recall  \\\n",
              "0          Regresion Lineal    Training  0.802571   0.830296  0.802571   \n",
              "1          Regresion Lineal  Validation  0.774725   0.816318  0.774725   \n",
              "2          Regresion Lineal        Test  0.760989   0.796967  0.760989   \n",
              "3       Regresión Logística    Training  0.842975   0.848799  0.842975   \n",
              "4       Regresión Logística  Validation  0.832418   0.844801  0.832418   \n",
              "5       Regresión Logística        Test  0.802198   0.817277  0.802198   \n",
              "6         Árbol de Decisión    Training  0.910927   0.910946  0.910927   \n",
              "7         Árbol de Decisión  Validation  0.859890   0.859828  0.859890   \n",
              "8         Árbol de Decisión        Test  0.851648   0.851536  0.851648   \n",
              "9             Random Forest    Training  1.000000   1.000000  1.000000   \n",
              "10            Random Forest  Validation  0.914835   0.915895  0.914835   \n",
              "11            Random Forest        Test  0.909341   0.910202  0.909341   \n",
              "12                      KNN    Training  0.848485   0.854651  0.848485   \n",
              "13                      KNN  Validation  0.804945   0.811110  0.804945   \n",
              "14                      KNN        Test  0.774725   0.785429  0.774725   \n",
              "15                      SVN    Training  0.854913   0.859023  0.854913   \n",
              "16                      SVN  Validation  0.848901   0.852435  0.848901   \n",
              "17                      SVN        Test  0.804945   0.811726  0.804945   \n",
              "18   Naive Bayes - Gaussian    Training  0.766758   0.826607  0.766758   \n",
              "19   Naive Bayes - Gaussian  Validation  0.782967   0.845750  0.782967   \n",
              "20   Naive Bayes - Gaussian        Test  0.708791   0.792288  0.708791   \n",
              "21  Naive Bayes - Bernoulli    Training  0.793388   0.809136  0.793388   \n",
              "22  Naive Bayes - Bernoulli  Validation  0.785714   0.802573  0.785714   \n",
              "23  Naive Bayes - Bernoulli        Test  0.755495   0.782567  0.755495   \n",
              "24               AdaBoost 1    Training  1.000000   1.000000  1.000000   \n",
              "25               AdaBoost 1  Validation  0.923077   0.923077  0.923077   \n",
              "26               AdaBoost 1        Test  0.920330   0.920606  0.920330   \n",
              "\n",
              "    F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared   AUC-ROC  \\\n",
              "0   0.801846             0.365453            0.197429   0.201379  0.814564   \n",
              "1   0.771756             0.299727            0.225275   0.090970  0.788792   \n",
              "2   0.759651             0.270159            0.239011   0.029779  0.776654   \n",
              "3   0.843414             0.470046            0.157025   0.364817  0.846990   \n",
              "4   0.832535             0.440458            0.167582   0.323770  0.838968   \n",
              "5   0.802706             0.363533            0.197802   0.197059  0.810723   \n",
              "6   0.910936             0.675114            0.089073   0.639692  0.910021   \n",
              "7   0.859853             0.516736            0.140110   0.434628  0.858398   \n",
              "8   0.851173             0.493086            0.148352   0.397794  0.846752   \n",
              "9   1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "10  0.914512             0.687455            0.085165   0.656342  0.911238   \n",
              "11  0.908917             0.669212            0.090659   0.631985  0.904289   \n",
              "12  0.848907             0.485299            0.151515   0.387105  0.852758   \n",
              "13  0.805347             0.370244            0.195055   0.212913  0.808665   \n",
              "14  0.775488             0.299987            0.225275   0.085539  0.780821   \n",
              "15  0.855308             0.503399            0.145087   0.413106  0.857984   \n",
              "16  0.849227             0.485523            0.151099   0.390285  0.851454   \n",
              "17  0.805659             0.370256            0.195055   0.208211  0.809130   \n",
              "18  0.762041             0.283319            0.233242   0.056513  0.785898   \n",
              "19  0.777865             0.318004            0.217033   0.124227  0.800472   \n",
              "20  0.698914             0.170664            0.291209  -0.182108  0.735478   \n",
              "21  0.793538             0.343655            0.206612   0.164234  0.801551   \n",
              "22  0.785468             0.324632            0.214286   0.135313  0.793665   \n",
              "23  0.755042             0.258904            0.244505   0.007475  0.768382   \n",
              "24  1.000000             1.000000            0.000000   1.000000  1.000000   \n",
              "25  0.923077             0.715184            0.076923   0.689600  0.922400   \n",
              "26  0.920405             0.705872            0.079670   0.676593  0.920159   \n",
              "\n",
              "     TN  FP   FN   TP  Global Score  \n",
              "0   452  35  180  422         76.60  \n",
              "1   155  10   72  127         73.86  \n",
              "2   145  15   72  132         72.02  \n",
              "3   431  56  115  487         80.31  \n",
              "4   150  15   46  153         79.46  \n",
              "5   141  19   53  151         75.89  \n",
              "6   439  48   49  553         88.17  \n",
              "7   139  26   25  174         81.65  \n",
              "8   129  31   23  181         80.30  \n",
              "9   487   0    0  602        100.00  \n",
              "10  144  21   10  189         88.32  \n",
              "11  138  22   11  193         87.54  \n",
              "12  435  52  113  489         81.02  \n",
              "13  140  25   46  153         75.71  \n",
              "14  133  27   55  149         72.34  \n",
              "15  432  55  103  499         81.65  \n",
              "16  145  20   35  164         80.86  \n",
              "17  135  25   46  158         75.67  \n",
              "18  471  16  238  364         73.54  \n",
              "19  163   2   77  122         75.49  \n",
              "20  153   7   99  105         67.68  \n",
              "21  428  59  166  436         74.93  \n",
              "22  145  20   58  141         74.10  \n",
              "23  140  20   69  135         70.99  \n",
              "24  487   0    0  602        100.00  \n",
              "25  151  14   14  185         89.75  \n",
              "26  147  13   16  188         89.46  "
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "model_name = \"AdaBoost 1\"\n",
        "print(model_name)\n",
        "\n",
        "# Inicializar el clasificador débil (stump)\n",
        "base_estimator = DecisionTreeClassifier(max_depth=2) # max_depth=1 menos sobreajuste, resultados ligeramente peores \n",
        "\n",
        "# Inicializar el modelo AdaBoost\n",
        "model = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(model, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "model.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = model.predict(X_train_prep)\n",
        "y_pred = model.predict(X_val_prep)\n",
        "y_test_pred = model.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_pred, \"Validation\",model_name, print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, print_roc = \"NO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "AdaBoost\n",
            "Mejores hiperparámetros: {'learning_rate': 0.07, 'n_estimators': 100}\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.30289127 -0.28734789 -0.22463018 -0.27925195 -0.29590134]\n",
            "Média métricas de validación cruzada: -0.27800452358336736\n",
            "\n",
            "Index(['CA19-9 (U/ml)', 'CA-125 (U/ml)', 'HGF (pg/ml)', 'OPN (pg/ml)',\n",
            "       'Omega score', 'Prolactin (pg/ml)', 'CEA (pg/ml)',\n",
            "       'Myeloperoxidase (ng/ml)', 'TIMP-1 (pg/ml)'],\n",
            "      dtype='object')\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.9614\n",
            " - Precision: 0.9615\n",
            " - Recall: 0.9614\n",
            " - F1-Score: 0.9615\n",
            " - Adjusted Rand Index: 0.8515\n",
            " - Mean Squared Error: 0.0386\n",
            " - R-squared: 0.8440\n",
            " - Área bajo la curva : 0.962\n",
            " - Confusion Matrix: \n",
            "[[469  18]\n",
            " [ 24 578]]\n",
            " - Global Score : 94.87\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9478\n",
            " - Precision: 0.9480\n",
            " - Recall: 0.9478\n",
            " - F1-Score: 0.9477\n",
            " - Adjusted Rand Index: 0.8015\n",
            " - Mean Squared Error: 0.0522\n",
            " - R-squared: 0.7894\n",
            " - Área bajo la curva : 0.946\n",
            " - Confusion Matrix: \n",
            "[[153  12]\n",
            " [  7 192]]\n",
            " - Global Score : 92.83\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9121\n",
            " - Precision: 0.9122\n",
            " - Recall: 0.9121\n",
            " - F1-Score: 0.9119\n",
            " - Adjusted Rand Index: 0.6783\n",
            " - Mean Squared Error: 0.0879\n",
            " - R-squared: 0.6431\n",
            " - Área bajo la curva : 0.909\n",
            " - Confusion Matrix: \n",
            "[[141  19]\n",
            " [ 13 191]]\n",
            " - Global Score : 88.07\n",
            "\n",
            "Index(['OPN (pg/ml)', 'IL-6 (pg/ml)', 'IL-8 (pg/ml)', 'HGF (pg/ml)',\n",
            "       'Prolactin (pg/ml)', 'Omega score', 'GDF15 (ng/ml)',\n",
            "       'CYFRA 21-1 (pg/ml)', 'Myeloperoxidase (ng/ml)', 'sEGFR (pg/ml)'],\n",
            "      dtype='object')\n",
            "Mejores hiperparámetros: {'learning_rate': 0.07, 'n_estimators': 100}\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.9734\n",
            " - Precision: 0.9734\n",
            " - Recall: 0.9734\n",
            " - F1-Score: 0.9734\n",
            " - Adjusted Rand Index: 0.8962\n",
            " - Mean Squared Error: 0.0266\n",
            " - R-squared: 0.8923\n",
            " - Área bajo la curva : 0.972\n",
            " - Confusion Matrix: \n",
            "[[469  18]\n",
            " [ 11 591]]\n",
            " - Global Score : 96.31\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9560\n",
            " - Precision: 0.9561\n",
            " - Recall: 0.9560\n",
            " - F1-Score: 0.9561\n",
            " - Adjusted Rand Index: 0.8314\n",
            " - Mean Squared Error: 0.0440\n",
            " - R-squared: 0.8226\n",
            " - Área bajo la curva : 0.956\n",
            " - Confusion Matrix: \n",
            "[[158   7]\n",
            " [  9 190]]\n",
            " - Global Score : 94.15\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9588\n",
            " - Precision: 0.9588\n",
            " - Recall: 0.9588\n",
            " - F1-Score: 0.9587\n",
            " - Adjusted Rand Index: 0.8415\n",
            " - Mean Squared Error: 0.0412\n",
            " - R-squared: 0.8327\n",
            " - Área bajo la curva : 0.957\n",
            " - Confusion Matrix: \n",
            "[[151   9]\n",
            " [  6 198]]\n",
            " - Global Score : 94.32\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos supervisados entrenados/adaBoost_e2_1.joblib']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import  GridSearchCV\n",
        "\n",
        "model_name = \"AdaBoost\"\n",
        "print(model_name)\n",
        "\n",
        "# Inicializar el clasificador débil (stump)\n",
        "base_estimator = DecisionTreeClassifier(max_depth=2)\n",
        "\n",
        "# Configurar la búsqueda de hiperparámetros\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 250, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.07, 0.10, 0.5, 1.0]\n",
        "}\n",
        "\n",
        "# Inicializar el modelo AdaBoost\n",
        "ada = AdaBoostClassifier(estimator=base_estimator, n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "grid_search = grid_search_2 = GridSearchCV(estimator=ada, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "\n",
        "# Entrenar el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "# Obtener los mejores hiperparámetros\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Mejores hiperparámetros: {best_params}\")\n",
        "\n",
        "# Mejor modelo\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(best_model, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "best_model.fit(X_train_prep, y_train)\n",
        "print(X_train_prep.columns)\n",
        "\n",
        "y_train_pred = best_model.predict(X_train_prep)\n",
        "y_val_pred = best_model.predict(X_val_prep)\n",
        "y_test_pred = best_model.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search, 'Modelos supervisados entrenados/adaBoost_e1_1.joblib')\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "\n",
        "grid_search_2.fit(X_train_prep_2, y_train_2)\n",
        "print(X_train_prep_2.columns)\n",
        "\n",
        "best_params = grid_search_2.best_params_\n",
        "print(f\"Mejores hiperparámetros: {best_params}\")\n",
        "\n",
        "y_train_pred_2 = grid_search_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = grid_search_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = grid_search_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search_2, 'Modelos supervisados entrenados/adaBoost_e2_1.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Gradient Boosting\n",
            "Cross validation : \n",
            "Métricas validación cruzada: [-0.30289127 -0.28734789 -0.22463018 -0.27925195 -0.29590134]\n",
            "Média métricas de validación cruzada: -0.27800452358336736\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.9706\n",
            " - Precision: 0.9706\n",
            " - Recall: 0.9706\n",
            " - F1-Score: 0.9706\n",
            " - Adjusted Rand Index: 0.8858\n",
            " - Mean Squared Error: 0.0294\n",
            " - R-squared: 0.8811\n",
            " - Área bajo la curva : 0.970\n",
            " - Confusion Matrix: \n",
            "[[471  16]\n",
            " [ 16 586]]\n",
            " - Global Score : 96.02\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9286\n",
            " - Precision: 0.9285\n",
            " - Recall: 0.9286\n",
            " - F1-Score: 0.9285\n",
            " - Adjusted Rand Index: 0.7339\n",
            " - Mean Squared Error: 0.0714\n",
            " - R-squared: 0.7118\n",
            " - Área bajo la curva : 0.927\n",
            " - Confusion Matrix: \n",
            "[[151  14]\n",
            " [ 12 187]]\n",
            " - Global Score : 90.4\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9203\n",
            " - Precision: 0.9203\n",
            " - Recall: 0.9203\n",
            " - F1-Score: 0.9202\n",
            " - Adjusted Rand Index: 0.7058\n",
            " - Mean Squared Error: 0.0797\n",
            " - R-squared: 0.6766\n",
            " - Área bajo la curva : 0.918\n",
            " - Confusion Matrix: \n",
            "[[144  16]\n",
            " [ 13 191]]\n",
            " - Global Score : 89.25\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.9991\n",
            " - Precision: 0.9991\n",
            " - Recall: 0.9991\n",
            " - F1-Score: 0.9991\n",
            " - Adjusted Rand Index: 0.9963\n",
            " - Mean Squared Error: 0.0009\n",
            " - R-squared: 0.9963\n",
            " - Área bajo la curva : 0.999\n",
            " - Confusion Matrix: \n",
            "[[486   1]\n",
            " [  0 602]]\n",
            " - Global Score : 99.86\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9505\n",
            " - Precision: 0.9505\n",
            " - Recall: 0.9505\n",
            " - F1-Score: 0.9505\n",
            " - Adjusted Rand Index: 0.8115\n",
            " - Mean Squared Error: 0.0495\n",
            " - R-squared: 0.8005\n",
            " - Área bajo la curva : 0.950\n",
            " - Confusion Matrix: \n",
            "[[156   9]\n",
            " [  9 190]]\n",
            " - Global Score : 93.36\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9725\n",
            " - Precision: 0.9725\n",
            " - Recall: 0.9725\n",
            " - F1-Score: 0.9725\n",
            " - Adjusted Rand Index: 0.8928\n",
            " - Mean Squared Error: 0.0275\n",
            " - R-squared: 0.8885\n",
            " - Área bajo la curva : 0.971\n",
            " - Confusion Matrix: \n",
            "[[154   6]\n",
            " [  4 200]]\n",
            " - Global Score : 96.2\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos supervisados entrenados/gradientBoosting_e2_1.joblib']"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model_name = \"Gradient Boosting\"\n",
        "print(model_name)\n",
        "\n",
        "# Configurar la búsqueda de hiperparámetros\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200, 250, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.07, 0.10, 0.5, 1.0],\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "best_params_gb_1 = {\n",
        "    'learning_rate': 0.05,  # Reducir la tasa de aprendizaje\n",
        "    'max_depth': 4,  # Reducir la profundidad máxima\n",
        "    'min_samples_leaf': 1,\n",
        "    'min_samples_split': 5,\n",
        "    'n_estimators': 100,\n",
        "}\n",
        "\n",
        "# Mejores parámetros para Gradient Boosting (enfoque 2)\n",
        "best_params_gb_2 = {\n",
        "    'learning_rate': 0.05,  # Reducir la tasa de aprendizaje\n",
        "    'max_depth': 4,  # Reducir la profundidad máxima\n",
        "    'min_samples_leaf': 1,\n",
        "    'min_samples_split': 5,\n",
        "    'n_estimators': 200,\n",
        "}\n",
        "# Inicializar el modelo AdaBoost\n",
        "# GradBoost = GradientBoostingClassifier(n_estimators=50, learning_rate=1.0, random_state=42)\n",
        "\n",
        "# grid_search = grid_search_2 = GridSearchCV(estimator=GradBoost, param_grid=param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
        "grid_search = GradientBoostingClassifier(**best_params_gb_1)\n",
        "grid_search_2 = GradientBoostingClassifier(**best_params_gb_2)\n",
        "\n",
        "# Entrenar el modelo con búsqueda de hiperparámetros\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Obtener los mejores hiperparámetros\n",
        "#best_params = grid_search.best_params_\n",
        "#print(f\"Mejores hiperparámetros 1: {best_params}\")\n",
        "\n",
        "# Cross validation\n",
        "mostrar_cross_validation(best_model, X_train_prep, y_train)\n",
        "\n",
        "# Entrenar el modelo usando los datos de entrenamiento preprocesados\n",
        "best_model.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = grid_search.predict(X_train_prep)\n",
        "y_val_pred = grid_search.predict(X_val_prep)\n",
        "y_test_pred = grid_search.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search, 'Modelos supervisados entrenados/gradientBoosting_e1_1.joblib')\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "\n",
        "grid_search_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "#best_params = grid_search_2.best_params_\n",
        "#print(f\"Mejores hiperparámetros 2: {best_params}\")\n",
        "\n",
        "y_train_pred_2 = grid_search_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = grid_search_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = grid_search_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search_2, 'Modelos supervisados entrenados/gradientBoosting_e2_1.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Redes Neuronales Artificiales"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Artificial Network Layers (ANN)\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Mejores parámetros encontrados: {'model__optimizer': 'SGD', 'model__neurons': 10, 'model__init_mode': 'normal', 'model__dropout_rate': 0.2, 'model__activation': 'relu', 'epochs': 150, 'batch_size': 20}\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8356\n",
            " - Precision: 0.8408\n",
            " - Recall: 0.8356\n",
            " - F1-Score: 0.8361\n",
            " - Adjusted Rand Index: 0.4501\n",
            " - Mean Squared Error: 0.1644\n",
            " - R-squared: 0.3351\n",
            " - Área bajo la curva : 0.839\n",
            " - Confusion Matrix: \n",
            "[[425  62]\n",
            " [117 485]]\n",
            " - Global Score : 79.35\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8159\n",
            " - Precision: 0.8243\n",
            " - Recall: 0.8159\n",
            " - F1-Score: 0.8162\n",
            " - Adjusted Rand Index: 0.3976\n",
            " - Mean Squared Error: 0.1841\n",
            " - R-squared: 0.2573\n",
            " - Área bajo la curva : 0.821\n",
            " - Confusion Matrix: \n",
            "[[144  21]\n",
            " [ 46 153]]\n",
            " - Global Score : 77.2\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7802\n",
            " - Precision: 0.7922\n",
            " - Recall: 0.7802\n",
            " - F1-Score: 0.7809\n",
            " - Adjusted Rand Index: 0.3122\n",
            " - Mean Squared Error: 0.2198\n",
            " - R-squared: 0.1078\n",
            " - Área bajo la curva : 0.787\n",
            " - Confusion Matrix: \n",
            "[[135  25]\n",
            " [ 55 149]]\n",
            " - Global Score : 73.08\n",
            "\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8705\n",
            " - Precision: 0.8723\n",
            " - Recall: 0.8705\n",
            " - F1-Score: 0.8708\n",
            " - Adjusted Rand Index: 0.5487\n",
            " - Mean Squared Error: 0.1295\n",
            " - R-squared: 0.4763\n",
            " - Área bajo la curva : 0.872\n",
            " - Confusion Matrix: \n",
            "[[431  56]\n",
            " [ 85 517]]\n",
            " - Global Score : 83.36\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8571\n",
            " - Precision: 0.8629\n",
            " - Recall: 0.8571\n",
            " - F1-Score: 0.8574\n",
            " - Adjusted Rand Index: 0.5089\n",
            " - Mean Squared Error: 0.1429\n",
            " - R-squared: 0.4235\n",
            " - Área bajo la curva : 0.861\n",
            " - Confusion Matrix: \n",
            "[[149  16]\n",
            " [ 36 163]]\n",
            " - Global Score : 82.08\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8434\n",
            " - Precision: 0.8447\n",
            " - Recall: 0.8434\n",
            " - F1-Score: 0.8437\n",
            " - Adjusted Rand Index: 0.4702\n",
            " - Mean Squared Error: 0.1566\n",
            " - R-squared: 0.3643\n",
            " - Área bajo la curva : 0.843\n",
            " - Confusion Matrix: \n",
            "[[135  25]\n",
            " [ 32 172]]\n",
            " - Global Score : 79.83\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos Entrenados/ann_e2_1.joblib']"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Input\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "\n",
        "# Crear una instancia del modelo de regresión lineal\n",
        "model_ANN = tf.keras.models.Sequential()\n",
        "model_name = \"Artificial Network Layers (ANN)\"\n",
        "print(model_name)\n",
        "\n",
        "def create_model(optimizer='adam', init_mode='uniform', activation='relu', dropout_rate=0.0, neurons=1):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(X_train_prep.shape[1],)))\n",
        "    model.add(Dense(neurons, kernel_initializer=init_mode, activation=activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Crear el modelo KerasClassifier con parámetros por defecto\n",
        "model = KerasClassifier(model=create_model, verbose=0)\n",
        "\n",
        "\n",
        "# Definir el grid de hiperparámetros a buscar\n",
        "param_grid = {\n",
        "    'model__optimizer': ['SGD', 'Adam'],\n",
        "    'model__init_mode': ['uniform', 'lecun_uniform', 'normal'],\n",
        "    'model__activation': ['relu', 'tanh', 'sigmoid'],\n",
        "    'model__dropout_rate': [0.0, 0.1, 0.2],\n",
        "    'model__neurons': [5, 10, 15],\n",
        "    'batch_size': [10, 20, 40],\n",
        "    'epochs': [50, 100, 150],\n",
        "}\n",
        "\n",
        "# Crear el objeto RandomizedSearchCV\n",
        "random_search = RandomizedSearchCV(estimator=model,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=10,  # Número de combinaciones a probar\n",
        "                                   cv=3,  # Número de folds para la validación cruzada\n",
        "                                   verbose=2,\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "# Entrenar el modelo usando RandomizedSearchCV\n",
        "random_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Obtener los mejores parámetros encontrados\n",
        "best_params = random_search.best_params_\n",
        "print(\"Mejores parámetros encontrados:\", best_params)\n",
        "\n",
        "# Evaluar el mejor modelo en el conjunto de prueba\n",
        "model_ANN = random_search.best_estimator_\n",
        "\n",
        "model_ANN.fit(X_train_prep, y_train, batch_size=32, epochs=50)\n",
        "\n",
        "# Predecir en los conjuntos de datos\n",
        "y_train_pred = model_ANN.predict(X_train_prep)\n",
        "y_val_pred = model_ANN.predict(X_val_prep)\n",
        "y_test_pred = model_ANN.predict(X_test_prep)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "def create_model2(optimizer='adam', init_mode='uniform', activation='relu', dropout_rate=0.0, neurons=1):\n",
        "    model = Sequential()\n",
        "    model.add(Input(shape=(X_train_prep_2.shape[1],)))\n",
        "    model.add(Dense(neurons, kernel_initializer=init_mode, activation=activation))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "    model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "model2 = KerasClassifier(model=create_model2, verbose=0)\n",
        "\n",
        "random_search_2 = RandomizedSearchCV(estimator=model2,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=10,  # Número de combinaciones a probar\n",
        "                                   cv=3,  # Número de folds para la validación cruzada\n",
        "                                   verbose=2,\n",
        "                                   random_state=42,\n",
        "                                   n_jobs=-1)\n",
        "\n",
        "random_search_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "y_train_pred_2 = random_search_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = random_search_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = random_search_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(random_search, 'Modelos supervisados entrenados/ann_e1_1.joblib')\n",
        "dump(random_search_2, 'Modelos supervisados entrenados/ann_e2_1.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Máquinas de vectores de soporte de regresión (SVR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVR\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8393\n",
            " - Precision: 0.8477\n",
            " - Recall: 0.8393\n",
            " - F1-Score: 0.8397\n",
            " - Adjusted Rand Index: 0.4600\n",
            " - Mean Squared Error: 0.1607\n",
            " - R-squared: 0.3500\n",
            " - Área bajo la curva : 0.845\n",
            " - Confusion Matrix: \n",
            "[[436  51]\n",
            " [124 478]]\n",
            " - Global Score : 80.05\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8269\n",
            " - Precision: 0.8407\n",
            " - Recall: 0.8269\n",
            " - F1-Score: 0.8270\n",
            " - Adjusted Rand Index: 0.4259\n",
            " - Mean Squared Error: 0.1731\n",
            " - R-squared: 0.3016\n",
            " - Área bajo la curva : 0.834\n",
            " - Confusion Matrix: \n",
            "[[150  15]\n",
            " [ 48 151]]\n",
            " - Global Score : 78.87\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7830\n",
            " - Precision: 0.7986\n",
            " - Recall: 0.7830\n",
            " - F1-Score: 0.7835\n",
            " - Adjusted Rand Index: 0.3184\n",
            " - Mean Squared Error: 0.2170\n",
            " - R-squared: 0.1190\n",
            " - Área bajo la curva : 0.792\n",
            " - Confusion Matrix: \n",
            "[[138  22]\n",
            " [ 57 147]]\n",
            " - Global Score : 73.63\n",
            "\n",
            "Mejores parámetros {'C': 1, 'epsilon': 0.2, 'gamma': 'scale', 'kernel': 'rbf'}\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8687\n",
            " - Precision: 0.8785\n",
            " - Recall: 0.8687\n",
            " - F1-Score: 0.8690\n",
            " - Adjusted Rand Index: 0.5433\n",
            " - Mean Squared Error: 0.1313\n",
            " - R-squared: 0.4688\n",
            " - Área bajo la curva : 0.875\n",
            " - Confusion Matrix: \n",
            "[[455  32]\n",
            " [111 491]]\n",
            " - Global Score : 83.8\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8269\n",
            " - Precision: 0.8439\n",
            " - Recall: 0.8269\n",
            " - F1-Score: 0.8268\n",
            " - Adjusted Rand Index: 0.4259\n",
            " - Mean Squared Error: 0.1731\n",
            " - R-squared: 0.3016\n",
            " - Área bajo la curva : 0.835\n",
            " - Confusion Matrix: \n",
            "[[152  13]\n",
            " [ 50 149]]\n",
            " - Global Score : 79.04\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8324\n",
            " - Precision: 0.8415\n",
            " - Recall: 0.8324\n",
            " - F1-Score: 0.8330\n",
            " - Adjusted Rand Index: 0.4405\n",
            " - Mean Squared Error: 0.1676\n",
            " - R-squared: 0.3197\n",
            " - Área bajo la curva : 0.838\n",
            " - Confusion Matrix: \n",
            "[[142  18]\n",
            " [ 43 161]]\n",
            " - Global Score : 79.2\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos Entrenados/svr_e2_1.joblib']"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "# TODO + 10 minutos de ejecución -- CORREGIR\n",
        "# Definir el modelo SVR\n",
        "model_name = \"SVR\"\n",
        "print(model_name)\n",
        "svr = SVR()\n",
        "\n",
        "# Definir los hiperparámetros a buscar\n",
        "'''param_grid = {\n",
        "    'kernel': ['linear', 'rbf'],\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'epsilon': [0.1, 0.2]\n",
        "}'''\n",
        "\n",
        "param_grid = {\n",
        "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
        "    'C': [0.1, 1, 10, 100, 1000],\n",
        "    'gamma': ['scale', 'auto'],\n",
        "    'epsilon': [0.1, 0.2, 0.5, 0.3]\n",
        "}\n",
        "best_params = {'C': 1, 'epsilon': 0.2, 'gamma': 'auto', 'kernel': 'rbf'}\n",
        "\n",
        "# Definir GridSearchCV\n",
        "# grid_search_2 = GridSearchCV(svr, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
        "grid_search = SVR(**best_params)\n",
        "grid_search_2 = SVR(**best_params)\n",
        "# Entrenar el modelo con los datos de entrenamiento y encontrar los mejores hiperparámetros\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Obtener el mejor modelo y sus hiperparámetros\n",
        "# model_SVR = grid_search.best_estimator_\n",
        "\n",
        "# print(f\"Mejores parámetros {grid_search.best_params_}\")\n",
        "\n",
        "y_train_pred = grid_search.predict(X_train_prep)\n",
        "y_val_pred = grid_search.predict(X_val_prep)\n",
        "y_test_pred = grid_search.predict(X_test_prep)\n",
        "\n",
        "umbral = 0.55\n",
        "\n",
        "y_train_pred_binary = (y_train_pred >= umbral).astype(int)\n",
        "y_val_pred_binary = (y_val_pred >= umbral).astype(int)\n",
        "y_test_pred_binary = (y_test_pred >= umbral).astype(int)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred_binary, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred_binary, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred_binary, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "\n",
        "grid_search_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "print(f\"Mejores parámetros {grid_search_2.best_params_}\")\n",
        "\n",
        "y_train_pred_2 = grid_search_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = grid_search_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = grid_search_2.predict(X_test_prep_2)\n",
        "\n",
        "y_train_pred_binary_2 = (y_train_pred_2 >= umbral).astype(int)\n",
        "y_val_pred_binary_2 = (y_val_pred_2 >= umbral).astype(int)\n",
        "y_test_pred_binary_2 = (y_test_pred_2 >= umbral).astype(int)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_binary_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_binary_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_binary_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search, 'Modelos supervisados entrenados/svr_e1_1.joblib')\n",
        "dump(grid_search_2, 'Modelos supervisados entrenados/svr_e2_1.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regresión polinomial"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Regresión polinomial\n",
            "Mejores parámetros {'logistic__C': 0.1, 'logistic__fit_intercept': True, 'poly__degree': 3}\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8430\n",
            " - Precision: 0.8446\n",
            " - Recall: 0.8430\n",
            " - F1-Score: 0.8433\n",
            " - Adjusted Rand Index: 0.4700\n",
            " - Mean Squared Error: 0.1570\n",
            " - R-squared: 0.3648\n",
            " - Área bajo la curva : 0.844\n",
            " - Confusion Matrix: \n",
            "[[415  72]\n",
            " [ 99 503]]\n",
            " - Global Score : 79.89\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8379\n",
            " - Precision: 0.8401\n",
            " - Recall: 0.8379\n",
            " - F1-Score: 0.8382\n",
            " - Adjusted Rand Index: 0.4552\n",
            " - Mean Squared Error: 0.1621\n",
            " - R-squared: 0.3459\n",
            " - Área bajo la curva : 0.839\n",
            " - Confusion Matrix: \n",
            "[[141  24]\n",
            " [ 35 164]]\n",
            " - Global Score : 79.35\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7857\n",
            " - Precision: 0.7911\n",
            " - Recall: 0.7857\n",
            " - F1-Score: 0.7865\n",
            " - Adjusted Rand Index: 0.3247\n",
            " - Mean Squared Error: 0.2143\n",
            " - R-squared: 0.1301\n",
            " - Área bajo la curva : 0.789\n",
            " - Confusion Matrix: \n",
            "[[130  30]\n",
            " [ 48 156]]\n",
            " - Global Score : 73.23\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8871\n",
            " - Precision: 0.8901\n",
            " - Recall: 0.8871\n",
            " - F1-Score: 0.8873\n",
            " - Adjusted Rand Index: 0.5989\n",
            " - Mean Squared Error: 0.1129\n",
            " - R-squared: 0.5431\n",
            " - Área bajo la curva : 0.890\n",
            " - Confusion Matrix: \n",
            "[[446  41]\n",
            " [ 82 520]]\n",
            " - Global Score : 85.61\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8379\n",
            " - Precision: 0.8442\n",
            " - Recall: 0.8379\n",
            " - F1-Score: 0.8382\n",
            " - Adjusted Rand Index: 0.4552\n",
            " - Mean Squared Error: 0.1621\n",
            " - R-squared: 0.3459\n",
            " - Área bajo la curva : 0.842\n",
            " - Confusion Matrix: \n",
            "[[146  19]\n",
            " [ 40 159]]\n",
            " - Global Score : 79.74\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8352\n",
            " - Precision: 0.8414\n",
            " - Recall: 0.8352\n",
            " - F1-Score: 0.8358\n",
            " - Adjusted Rand Index: 0.4478\n",
            " - Mean Squared Error: 0.1648\n",
            " - R-squared: 0.3309\n",
            " - Área bajo la curva : 0.839\n",
            " - Confusion Matrix: \n",
            "[[140  20]\n",
            " [ 40 164]]\n",
            " - Global Score : 79.33\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos Entrenados/regPoli_e2_1.joblib']"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "model_name = \"Regresión polinomial\"\n",
        "print(model_name)\n",
        "\n",
        "# Definir el pipeline para regresión polinomial\n",
        "pipeline = Pipeline([\n",
        "    ('poly', PolynomialFeatures()),\n",
        "    ('logistic', LogisticRegression(max_iter=10000))\n",
        "])\n",
        "\n",
        "# Definir los hiperparámetros a buscar. Un grado muy alto del polinomio puede llevar a sobreajuste (overfitting)\n",
        "param_grid = {\n",
        "    'poly__degree': [2, 3, 4, 5],  # grados del polinomio\n",
        "    'logistic__C': [0.1, 1, 10, 100],  # regularización\n",
        "    'logistic__fit_intercept': [True, False]\n",
        "}\n",
        "\n",
        "# Definir GridSearchCV\n",
        "grid_search = grid_search_2 = GridSearchCV(pipeline, param_grid, cv=5, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento y encontrar los mejores hiperparámetros\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Obtener el mejor modelo y sus hiperparámetros\n",
        "model_RPoli = grid_search.best_estimator_\n",
        "\n",
        "print(f\"Mejores parámetros {grid_search.best_params_}\")\n",
        "\n",
        "y_train_pred = model_RPoli.predict(X_train_prep)\n",
        "y_val_pred = model_RPoli.predict(X_val_prep)\n",
        "y_test_pred = model_RPoli.predict(X_test_prep)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "\n",
        "grid_search_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "y_train_pred_2 = grid_search_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = grid_search_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = grid_search_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "\n",
        "dump(grid_search, 'Modelos supervisados entrenados/regPoli_e1_1.joblib')\n",
        "dump(grid_search_2, 'Modelos supervisados entrenados/regPoli_e2_1.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Máquinas de aprendizaje extremo (Extreme Learning Machines, ELM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pycuda\n",
            "  Using cached pycuda-2024.1.tar.gz (1.7 MB)\n",
            "  Installing build dependencies: started\n",
            "  Installing build dependencies: finished with status 'done'\n",
            "  Getting requirements to build wheel: started\n",
            "  Getting requirements to build wheel: finished with status 'done'\n",
            "  Preparing metadata (pyproject.toml): started\n",
            "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
            "Collecting pytools>=2011.2 (from pycuda)\n",
            "  Using cached pytools-2024.1.5-py2.py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting appdirs>=1.4.0 (from pycuda)\n",
            "  Using cached appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\n",
            "Collecting mako (from pycuda)\n",
            "  Using cached Mako-1.3.5-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: platformdirs>=2.2.0 in c:\\users\\danie\\.virtualenvs\\repositorio_compartido-xxik1ebc\\lib\\site-packages (from pytools>=2011.2->pycuda) (3.11.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.9.2 in c:\\users\\danie\\.virtualenvs\\repositorio_compartido-xxik1ebc\\lib\\site-packages (from mako->pycuda) (2.1.3)\n",
            "Using cached appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Using cached pytools-2024.1.5-py2.py3-none-any.whl (88 kB)\n",
            "Using cached Mako-1.3.5-py3-none-any.whl (78 kB)\n",
            "Building wheels for collected packages: pycuda\n",
            "  Building wheel for pycuda (pyproject.toml): started\n",
            "  Building wheel for pycuda (pyproject.toml): finished with status 'error'\n",
            "Failed to build pycuda\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × Building wheel for pycuda (pyproject.toml) did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [137 lines of output]\n",
            "      ***************************************************************\n",
            "      *** WARNING: nvcc not in path.\n",
            "      *** May need to set CUDA_INC_DIR for installation to succeed.\n",
            "      ***************************************************************\n",
            "      *************************************************************\n",
            "      *** I have detected that you have not run configure.py.\n",
            "      *************************************************************\n",
            "      *** Additionally, no global config files were found.\n",
            "      *** I will go ahead with the default configuration.\n",
            "      *** In all likelihood, this will not work out.\n",
            "      ***\n",
            "      *** See README_SETUP.txt for more information.\n",
            "      ***\n",
            "      *** If the build does fail, just re-run configure.py with the\n",
            "      *** correct arguments, and then retry. Good luck!\n",
            "      *************************************************************\n",
            "      *** HIT Ctrl-C NOW IF THIS IS NOT WHAT YOU WANT\n",
            "      *************************************************************\n",
            "      Continuing in 10 seconds...\n",
            "      Continuing in 9 seconds...\n",
            "      Continuing in 8 seconds...\n",
            "      Continuing in 7 seconds...\n",
            "      Continuing in 6 seconds...\n",
            "      Continuing in 5 seconds...\n",
            "      Continuing in 4 seconds...\n",
            "      Continuing in 3 seconds...\n",
            "      Continuing in 2 seconds...\n",
            "      Continuing in 1 seconds...\n",
            "      \n",
            "      C:\\Users\\danie\\AppData\\Local\\Temp\\pip-build-env-_u3c2ilt\\overlay\\Lib\\site-packages\\setuptools\\_distutils\\dist.py:266: UserWarning: Unknown distribution option: 'test_requires'\n",
            "        warnings.warn(msg)\n",
            "      running bdist_wheel\n",
            "      running build\n",
            "      running build_py\n",
            "      creating build\n",
            "      creating build\\lib.win-amd64-cpython-312\n",
            "      creating build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\autoinit.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\autoprimaryctx.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\characterize.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\compiler.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\cumath.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\curandom.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\debug.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\driver.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\elementwise.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\gpuarray.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\reduction.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\scan.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\tools.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\_cluda.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\_mymako.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      copying pycuda\\__init__.py -> build\\lib.win-amd64-cpython-312\\pycuda\n",
            "      creating build\\lib.win-amd64-cpython-312\\pycuda\\gl\n",
            "      copying pycuda\\gl\\autoinit.py -> build\\lib.win-amd64-cpython-312\\pycuda\\gl\n",
            "      copying pycuda\\gl\\__init__.py -> build\\lib.win-amd64-cpython-312\\pycuda\\gl\n",
            "      creating build\\lib.win-amd64-cpython-312\\pycuda\\sparse\n",
            "      copying pycuda\\sparse\\cg.py -> build\\lib.win-amd64-cpython-312\\pycuda\\sparse\n",
            "      copying pycuda\\sparse\\coordinate.py -> build\\lib.win-amd64-cpython-312\\pycuda\\sparse\n",
            "      copying pycuda\\sparse\\inner.py -> build\\lib.win-amd64-cpython-312\\pycuda\\sparse\n",
            "      copying pycuda\\sparse\\operator.py -> build\\lib.win-amd64-cpython-312\\pycuda\\sparse\n",
            "      copying pycuda\\sparse\\packeted.py -> build\\lib.win-amd64-cpython-312\\pycuda\\sparse\n",
            "      copying pycuda\\sparse\\pkt_build.py -> build\\lib.win-amd64-cpython-312\\pycuda\\sparse\n",
            "      copying pycuda\\sparse\\__init__.py -> build\\lib.win-amd64-cpython-312\\pycuda\\sparse\n",
            "      creating build\\lib.win-amd64-cpython-312\\pycuda\\compyte\n",
            "      copying pycuda\\compyte\\array.py -> build\\lib.win-amd64-cpython-312\\pycuda\\compyte\n",
            "      copying pycuda\\compyte\\dtypes.py -> build\\lib.win-amd64-cpython-312\\pycuda\\compyte\n",
            "      copying pycuda\\compyte\\__init__.py -> build\\lib.win-amd64-cpython-312\\pycuda\\compyte\n",
            "      running egg_info\n",
            "      writing pycuda.egg-info\\PKG-INFO\n",
            "      writing dependency_links to pycuda.egg-info\\dependency_links.txt\n",
            "      writing requirements to pycuda.egg-info\\requires.txt\n",
            "      writing top-level names to pycuda.egg-info\\top_level.txt\n",
            "      reading manifest file 'pycuda.egg-info\\SOURCES.txt'\n",
            "      reading manifest template 'MANIFEST.in'\n",
            "      warning: no files found matching 'doc\\source\\*.rst'\n",
            "      warning: no files found matching 'doc\\source\\conf.py'\n",
            "      warning: no files found matching 'doc\\source\\_static\\*.css'\n",
            "      warning: no files found matching 'doc\\source\\_templates\\*.html'\n",
            "      warning: no files found matching '*.cpp' under directory 'bpl-subset\\bpl_subset\\boost'\n",
            "      warning: no files found matching '*.html' under directory 'bpl-subset\\bpl_subset\\boost'\n",
            "      warning: no files found matching '*.inl' under directory 'bpl-subset\\bpl_subset\\boost'\n",
            "      warning: no files found matching '*.txt' under directory 'bpl-subset\\bpl_subset\\boost'\n",
            "      warning: no files found matching '*.h' under directory 'bpl-subset\\bpl_subset\\libs'\n",
            "      warning: no files found matching '*.ipp' under directory 'bpl-subset\\bpl_subset\\libs'\n",
            "      warning: no files found matching '*.pl' under directory 'bpl-subset\\bpl_subset\\libs'\n",
            "      adding license file 'LICENSE'\n",
            "      writing manifest file 'pycuda.egg-info\\SOURCES.txt'\n",
            "      C:\\Users\\danie\\AppData\\Local\\Temp\\pip-build-env-_u3c2ilt\\overlay\\Lib\\site-packages\\setuptools\\command\\build_py.py:207: _Warning: Package 'pycuda.cuda' is absent from the `packages` configuration.\n",
            "      !!\n",
            "      \n",
            "              ********************************************************************************\n",
            "              ############################\n",
            "              # Package would be ignored #\n",
            "              ############################\n",
            "              Python recognizes 'pycuda.cuda' as an importable package[^1],\n",
            "              but it is absent from setuptools' `packages` configuration.\n",
            "      \n",
            "              This leads to an ambiguous overall configuration. If you want to distribute this\n",
            "              package, please make sure that 'pycuda.cuda' is explicitly added\n",
            "              to the `packages` configuration field.\n",
            "      \n",
            "              Alternatively, you can also rely on setuptools' discovery methods\n",
            "              (for example by using `find_namespace_packages(...)`/`find_namespace:`\n",
            "              instead of `find_packages(...)`/`find:`).\n",
            "      \n",
            "              You can read more about \"package discovery\" on setuptools documentation page:\n",
            "      \n",
            "              - https://setuptools.pypa.io/en/latest/userguide/package_discovery.html\n",
            "      \n",
            "              If you don't want 'pycuda.cuda' to be distributed and are\n",
            "              already explicitly excluding 'pycuda.cuda' via\n",
            "              `find_namespace_packages(...)/find_namespace` or `find_packages(...)/find`,\n",
            "              you can try to use `exclude_package_data`, or `include-package-data=False` in\n",
            "              combination with a more fine grained `package-data` configuration.\n",
            "      \n",
            "              You can read more about \"package data files\" on setuptools documentation page:\n",
            "      \n",
            "              - https://setuptools.pypa.io/en/latest/userguide/datafiles.html\n",
            "      \n",
            "      \n",
            "              [^1]: For Python, any directory (with suitable naming) can be imported,\n",
            "                    even if it does not contain any `.py` files.\n",
            "                    On the other hand, currently there is no concept of package data\n",
            "                    directory, all directories are treated like packages.\n",
            "              ********************************************************************************\n",
            "      \n",
            "      !!\n",
            "        check.warn(importable)\n",
            "      creating build\\lib.win-amd64-cpython-312\\pycuda\\cuda\n",
            "      copying pycuda\\cuda\\pycuda-complex-impl.hpp -> build\\lib.win-amd64-cpython-312\\pycuda\\cuda\n",
            "      copying pycuda\\cuda\\pycuda-complex.hpp -> build\\lib.win-amd64-cpython-312\\pycuda\\cuda\n",
            "      copying pycuda\\cuda\\pycuda-helpers.hpp -> build\\lib.win-amd64-cpython-312\\pycuda\\cuda\n",
            "      copying pycuda\\sparse\\pkt_build_cython.pyx -> build\\lib.win-amd64-cpython-312\\pycuda\\sparse\n",
            "      running build_ext\n",
            "      building '_driver' extension\n",
            "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  ERROR: Failed building wheel for pycuda\n",
            "ERROR: Could not build wheels for pycuda, which is required to install pyproject.toml-based projects\n"
          ]
        }
      ],
      "source": [
        "# !pip install hpelm\n",
        "# !pip install pycuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extreme Learning Machine (ELM)\n",
            "Using slower basic Python solver\n",
            "Metrics for Training set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[487   0]\n",
            " [  0 602]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[165   0]\n",
            " [  0 199]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[160   0]\n",
            " [  0 204]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Using slower basic Python solver\n",
            "Metrics for Training set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[487   0]\n",
            " [  0 602]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[165   0]\n",
            " [  0 199]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[160   0]\n",
            " [  0 204]]\n",
            " - Global Score : 100.0\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Approach</th>\n",
              "      <th>Set</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1-Score</th>\n",
              "      <th>Adjusted Rand Index</th>\n",
              "      <th>Mean Squared Error</th>\n",
              "      <th>R-squared</th>\n",
              "      <th>AUC-ROC</th>\n",
              "      <th>TN</th>\n",
              "      <th>FP</th>\n",
              "      <th>FN</th>\n",
              "      <th>TP</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>1</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.842975</td>\n",
              "      <td>0.848799</td>\n",
              "      <td>0.842975</td>\n",
              "      <td>0.843414</td>\n",
              "      <td>0.470046</td>\n",
              "      <td>0.157025</td>\n",
              "      <td>0.364817</td>\n",
              "      <td>0.846990</td>\n",
              "      <td>431</td>\n",
              "      <td>56</td>\n",
              "      <td>115</td>\n",
              "      <td>487</td>\n",
              "      <td>80.31</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.832418</td>\n",
              "      <td>0.844801</td>\n",
              "      <td>0.832418</td>\n",
              "      <td>0.832535</td>\n",
              "      <td>0.440458</td>\n",
              "      <td>0.167582</td>\n",
              "      <td>0.323770</td>\n",
              "      <td>0.838968</td>\n",
              "      <td>150</td>\n",
              "      <td>15</td>\n",
              "      <td>46</td>\n",
              "      <td>153</td>\n",
              "      <td>79.46</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>1</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.802198</td>\n",
              "      <td>0.817277</td>\n",
              "      <td>0.802198</td>\n",
              "      <td>0.802706</td>\n",
              "      <td>0.363533</td>\n",
              "      <td>0.197802</td>\n",
              "      <td>0.197059</td>\n",
              "      <td>0.810723</td>\n",
              "      <td>141</td>\n",
              "      <td>19</td>\n",
              "      <td>53</td>\n",
              "      <td>151</td>\n",
              "      <td>75.89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>2</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.840220</td>\n",
              "      <td>0.845878</td>\n",
              "      <td>0.840220</td>\n",
              "      <td>0.840667</td>\n",
              "      <td>0.462511</td>\n",
              "      <td>0.159780</td>\n",
              "      <td>0.353674</td>\n",
              "      <td>0.844106</td>\n",
              "      <td>429</td>\n",
              "      <td>58</td>\n",
              "      <td>116</td>\n",
              "      <td>486</td>\n",
              "      <td>79.95</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Regresión Logística</td>\n",
              "      <td>2</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.848901</td>\n",
              "      <td>0.863040</td>\n",
              "      <td>0.848901</td>\n",
              "      <td>0.848939</td>\n",
              "      <td>0.485499</td>\n",
              "      <td>0.151099</td>\n",
              "      <td>0.390285</td>\n",
              "      <td>0.856114</td>\n",
              "      <td>154</td>\n",
              "      <td>11</td>\n",
              "      <td>44</td>\n",
              "      <td>155</td>\n",
              "      <td>81.59</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>82</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>1</td>\n",
              "      <td>Validation</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>199</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>83</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>1</td>\n",
              "      <td>Test</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>204</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>2</td>\n",
              "      <td>Training</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>487</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>602</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>2</td>\n",
              "      <td>Validation</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>165</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>199</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>Extreme Learning Machine (ELM)</td>\n",
              "      <td>2</td>\n",
              "      <td>Test</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>160</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>204</td>\n",
              "      <td>100.00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>87 rows × 16 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                             Model Approach         Set  Accuracy  Precision  \\\n",
              "0              Regresión Logística        1    Training  0.842975   0.848799   \n",
              "1              Regresión Logística        1  Validation  0.832418   0.844801   \n",
              "2              Regresión Logística        1        Test  0.802198   0.817277   \n",
              "3              Regresión Logística        2    Training  0.840220   0.845878   \n",
              "4              Regresión Logística        2  Validation  0.848901   0.863040   \n",
              "..                             ...      ...         ...       ...        ...   \n",
              "82  Extreme Learning Machine (ELM)        1  Validation  1.000000   1.000000   \n",
              "83  Extreme Learning Machine (ELM)        1        Test  1.000000   1.000000   \n",
              "84  Extreme Learning Machine (ELM)        2    Training  1.000000   1.000000   \n",
              "85  Extreme Learning Machine (ELM)        2  Validation  1.000000   1.000000   \n",
              "86  Extreme Learning Machine (ELM)        2        Test  1.000000   1.000000   \n",
              "\n",
              "      Recall  F1-Score  Adjusted Rand Index  Mean Squared Error  R-squared  \\\n",
              "0   0.842975  0.843414             0.470046            0.157025   0.364817   \n",
              "1   0.832418  0.832535             0.440458            0.167582   0.323770   \n",
              "2   0.802198  0.802706             0.363533            0.197802   0.197059   \n",
              "3   0.840220  0.840667             0.462511            0.159780   0.353674   \n",
              "4   0.848901  0.848939             0.485499            0.151099   0.390285   \n",
              "..       ...       ...                  ...                 ...        ...   \n",
              "82  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "83  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "84  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "85  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "86  1.000000  1.000000             1.000000            0.000000   1.000000   \n",
              "\n",
              "     AUC-ROC   TN  FP   FN   TP  Global Score  \n",
              "0   0.846990  431  56  115  487         80.31  \n",
              "1   0.838968  150  15   46  153         79.46  \n",
              "2   0.810723  141  19   53  151         75.89  \n",
              "3   0.844106  429  58  116  486         79.95  \n",
              "4   0.856114  154  11   44  155         81.59  \n",
              "..       ...  ...  ..  ...  ...           ...  \n",
              "82  1.000000  165   0    0  199        100.00  \n",
              "83  1.000000  160   0    0  204        100.00  \n",
              "84  1.000000  487   0    0  602        100.00  \n",
              "85  1.000000  165   0    0  199        100.00  \n",
              "86  1.000000  160   0    0  204        100.00  \n",
              "\n",
              "[87 rows x 16 columns]"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hpelm import ELM\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "## TODO Verificar funcionamiento\n",
        "model_name = 'Extreme Learning Machine (ELM)'\n",
        "print(model_name)\n",
        "\n",
        "# Definir el número de neuronas y el tipo de activación para ELM\n",
        "num_neuronas = 100  \n",
        "tipo_activacion = 'sigm'  \n",
        "\n",
        "# Convert labels to NumPy arrays\n",
        "X_train_np = np.array(X_train_prep)\n",
        "X_val_np = np.array(X_val_prep)\n",
        "X_test_np = np.array(X_test_prep)\n",
        "\n",
        "# Convert labels to NumPy arrays for one-hot encoding\n",
        "y_train_np = y_train.to_numpy().reshape(-1, 1)  # Convert to NumPy array and reshape\n",
        "y_val_np = y_val.to_numpy().reshape(-1, 1)\n",
        "y_test_np = y_test.to_numpy().reshape(-1, 1)\n",
        "\n",
        "# One-hot encode the labels\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "y_train_enc = encoder.fit_transform(y_train_np)\n",
        "y_val_enc = encoder.transform(y_val_np)\n",
        "y_test_enc = encoder.transform(y_test_np)\n",
        "\n",
        "# Train the ELM model (assuming X_train is a NumPy array now)\n",
        "model_ELM = ELM(X_train_np.shape[1], y_train_enc.shape[1], batch=256, accelerator=\"basic\", classification=\"c\")\n",
        "model_ELM.add_neurons(num_neuronas, tipo_activacion)\n",
        "model_ELM.train(X_train_np, y_train_enc, \"c\")\n",
        "\n",
        "# Evaluar el modelo en los datos de entrenamiento, evaluación y test\n",
        "y_train_pred = model_ELM.predict(X_train_np)\n",
        "y_val_pred = model_ELM.predict(X_val_np)\n",
        "y_test_pred = model_ELM.predict(X_test_np)\n",
        "\n",
        "# Convertir las predicciones a etiquetas de clase\n",
        "y_train_pred_labels = y_train_pred.argmax(axis=1)\n",
        "y_val_pred_labels = y_val_pred.argmax(axis=1)\n",
        "y_test_pred_labels = y_test_pred.argmax(axis=1)\n",
        "\n",
        "# Convertir las etiquetas verdaderas a etiquetas de clase\n",
        "y_train_true_labels = y_train_enc.argmax(axis=1)\n",
        "y_val_true_labels = y_val_enc.argmax(axis=1)\n",
        "y_test_true_labels = y_test_enc.argmax(axis=1)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_true_labels, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_true_labels, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_true_labels, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "''' Enfoque 2 ''' \n",
        "\n",
        "# Convert labels to NumPy arrays for Enfoque 2\n",
        "X_train_np_2 = np.array(X_train_prep_2)\n",
        "X_val_np_2 = np.array(X_val_prep_2)\n",
        "X_test_np_2 = np.array(X_test_prep_2)\n",
        "\n",
        "# Convert labels to NumPy arrays for one-hot encoding for Enfoque 2\n",
        "y_train_np_2 = y_train_2.to_numpy().reshape(-1, 1)\n",
        "y_val_np_2 = y_val_2.to_numpy().reshape(-1, 1)\n",
        "y_test_np_2 = y_test_2.to_numpy().reshape(-1, 1)\n",
        "\n",
        "# One-hot encode the labels for Enfoque 2\n",
        "y_train_enc_2 = encoder.fit_transform(y_train_np_2)\n",
        "y_val_enc_2 = encoder.transform(y_val_np_2)\n",
        "y_test_enc_2 = encoder.transform(y_test_np_2)\n",
        "\n",
        "# Train the ELM model for Enfoque 2\n",
        "model_ELM_2 = ELM(X_train_np_2.shape[1], y_train_enc_2.shape[1], batch=256, accelerator=\"basic\", classification=\"c\")\n",
        "model_ELM_2.add_neurons(num_neuronas, tipo_activacion)\n",
        "model_ELM_2.train(X_train_np_2, y_train_enc_2, \"c\")\n",
        "\n",
        "# Evaluar el modelo en los datos de entrenamiento, evaluación y test para Enfoque 2\n",
        "y_train_pred_2 = model_ELM_2.predict(X_train_np_2)\n",
        "y_val_pred_2 = model_ELM_2.predict(X_val_np_2)\n",
        "y_test_pred_2 = model_ELM_2.predict(X_test_np_2)\n",
        "\n",
        "# Convertir las predicciones a etiquetas de clase para Enfoque 2\n",
        "y_train_pred_labels_2 = y_train_pred_2.argmax(axis=1)\n",
        "y_val_pred_labels_2 = y_val_pred_2.argmax(axis=1)\n",
        "y_test_pred_labels_2 = y_test_pred_2.argmax(axis=1)\n",
        "\n",
        "# Convertir las etiquetas verdaderas a etiquetas de clase para Enfoque 2\n",
        "y_train_true_labels_2 = y_train_enc_2.argmax(axis=1)\n",
        "y_val_true_labels_2 = y_val_enc_2.argmax(axis=1)\n",
        "y_test_true_labels_2 = y_test_enc_2.argmax(axis=1)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_true_labels_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_true_labels_2, \"Validation\",model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_true_labels_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting elm\n",
            "  Downloading elm-0.1.3-py2.py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting numpy==1.15.4 (from elm)\n",
            "  Downloading numpy-1.15.4.zip (4.5 MB)\n",
            "     ---------------------------------------- 0.0/4.5 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/4.5 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/4.5 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/4.5 MB 330.3 kB/s eta 0:00:14\n",
            "     ---------------------------------------- 0.0/4.5 MB 330.3 kB/s eta 0:00:14\n",
            "     ---------------------------------------- 0.0/4.5 MB 330.3 kB/s eta 0:00:14\n",
            "     ---------------------------------------- 0.0/4.5 MB 330.3 kB/s eta 0:00:14\n",
            "     ---------------------------------------- 0.0/4.5 MB 122.9 kB/s eta 0:00:37\n",
            "     ---------------------------------------- 0.0/4.5 MB 122.9 kB/s eta 0:00:37\n",
            "     ---------------------------------------- 0.0/4.5 MB 122.9 kB/s eta 0:00:37\n",
            "     ---------------------------------------- 0.0/4.5 MB 122.9 kB/s eta 0:00:37\n",
            "      --------------------------------------- 0.1/4.5 MB 113.0 kB/s eta 0:00:40\n",
            "      --------------------------------------- 0.1/4.5 MB 113.0 kB/s eta 0:00:40\n",
            "      --------------------------------------- 0.1/4.5 MB 127.5 kB/s eta 0:00:35\n",
            "      --------------------------------------- 0.1/4.5 MB 138.1 kB/s eta 0:00:32\n",
            "      --------------------------------------- 0.1/4.5 MB 138.1 kB/s eta 0:00:32\n",
            "     - -------------------------------------- 0.1/4.5 MB 152.6 kB/s eta 0:00:29\n",
            "     - -------------------------------------- 0.1/4.5 MB 152.6 kB/s eta 0:00:29\n",
            "     - -------------------------------------- 0.1/4.5 MB 152.6 kB/s eta 0:00:29\n",
            "     - -------------------------------------- 0.1/4.5 MB 138.7 kB/s eta 0:00:32\n",
            "     - -------------------------------------- 0.1/4.5 MB 138.7 kB/s eta 0:00:32\n",
            "     - -------------------------------------- 0.1/4.5 MB 146.9 kB/s eta 0:00:30\n",
            "     - -------------------------------------- 0.2/4.5 MB 148.1 kB/s eta 0:00:30\n",
            "     - -------------------------------------- 0.2/4.5 MB 161.4 kB/s eta 0:00:27\n",
            "     - -------------------------------------- 0.2/4.5 MB 171.0 kB/s eta 0:00:26\n",
            "     - -------------------------------------- 0.2/4.5 MB 172.9 kB/s eta 0:00:25\n",
            "     - -------------------------------------- 0.2/4.5 MB 172.9 kB/s eta 0:00:25\n",
            "     -- ------------------------------------- 0.2/4.5 MB 183.6 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.2/4.5 MB 180.2 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.2/4.5 MB 180.2 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.3/4.5 MB 183.0 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.3/4.5 MB 183.0 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.3/4.5 MB 183.0 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.3/4.5 MB 183.2 kB/s eta 0:00:23\n",
            "     -- ------------------------------------- 0.3/4.5 MB 183.2 kB/s eta 0:00:23\n",
            "     -- ------------------------------------- 0.3/4.5 MB 175.2 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.3/4.5 MB 175.2 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.3/4.5 MB 177.7 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.3/4.5 MB 177.7 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.3/4.5 MB 177.2 kB/s eta 0:00:24\n",
            "     -- ------------------------------------- 0.3/4.5 MB 177.2 kB/s eta 0:00:24\n",
            "     --- ------------------------------------ 0.3/4.5 MB 177.8 kB/s eta 0:00:24\n",
            "     --- ------------------------------------ 0.4/4.5 MB 182.7 kB/s eta 0:00:23\n",
            "     --- ------------------------------------ 0.4/4.5 MB 182.1 kB/s eta 0:00:23\n",
            "     --- ------------------------------------ 0.4/4.5 MB 188.0 kB/s eta 0:00:22\n",
            "     --- ------------------------------------ 0.4/4.5 MB 188.7 kB/s eta 0:00:22\n",
            "     --- ------------------------------------ 0.4/4.5 MB 188.7 kB/s eta 0:00:22\n",
            "     --- ------------------------------------ 0.4/4.5 MB 188.7 kB/s eta 0:00:22\n",
            "     --- ------------------------------------ 0.4/4.5 MB 188.7 kB/s eta 0:00:22\n",
            "     --- ------------------------------------ 0.4/4.5 MB 188.7 kB/s eta 0:00:22\n",
            "     --- ------------------------------------ 0.4/4.5 MB 180.8 kB/s eta 0:00:23\n",
            "     --- ------------------------------------ 0.4/4.5 MB 187.3 kB/s eta 0:00:22\n",
            "     ---- ----------------------------------- 0.5/4.5 MB 186.7 kB/s eta 0:00:22\n",
            "     ---- ----------------------------------- 0.5/4.5 MB 191.5 kB/s eta 0:00:21\n",
            "     ---- ----------------------------------- 0.5/4.5 MB 191.5 kB/s eta 0:00:21\n",
            "     ---- ----------------------------------- 0.5/4.5 MB 191.5 kB/s eta 0:00:21\n",
            "     ---- ----------------------------------- 0.5/4.5 MB 186.1 kB/s eta 0:00:22\n",
            "     ---- ----------------------------------- 0.5/4.5 MB 188.4 kB/s eta 0:00:22\n",
            "     ---- ----------------------------------- 0.5/4.5 MB 192.8 kB/s eta 0:00:21\n",
            "     ---- ----------------------------------- 0.5/4.5 MB 192.8 kB/s eta 0:00:21\n",
            "     ---- ----------------------------------- 0.5/4.5 MB 192.8 kB/s eta 0:00:21\n",
            "     ---- ----------------------------------- 0.5/4.5 MB 187.8 kB/s eta 0:00:21\n",
            "     ---- ----------------------------------- 0.6/4.5 MB 191.9 kB/s eta 0:00:21\n",
            "     ---- ----------------------------------- 0.6/4.5 MB 191.9 kB/s eta 0:00:21\n",
            "     ---- ----------------------------------- 0.6/4.5 MB 191.9 kB/s eta 0:00:21\n",
            "     ---- ----------------------------------- 0.6/4.5 MB 191.9 kB/s eta 0:00:21\n",
            "     ----- ---------------------------------- 0.6/4.5 MB 181.5 kB/s eta 0:00:22\n",
            "     ----- ---------------------------------- 0.6/4.5 MB 181.5 kB/s eta 0:00:22\n",
            "     ----- ---------------------------------- 0.6/4.5 MB 183.5 kB/s eta 0:00:22\n",
            "     ----- ---------------------------------- 0.6/4.5 MB 187.3 kB/s eta 0:00:21\n",
            "     ----- ---------------------------------- 0.6/4.5 MB 187.3 kB/s eta 0:00:21\n",
            "     ----- ---------------------------------- 0.6/4.5 MB 185.0 kB/s eta 0:00:21\n",
            "     ----- ---------------------------------- 0.6/4.5 MB 185.0 kB/s eta 0:00:21\n",
            "     ----- ---------------------------------- 0.6/4.5 MB 185.0 kB/s eta 0:00:21\n",
            "     ----- ---------------------------------- 0.6/4.5 MB 186.4 kB/s eta 0:00:21\n",
            "     ----- ---------------------------------- 0.7/4.5 MB 190.7 kB/s eta 0:00:20\n",
            "     ------ --------------------------------- 0.7/4.5 MB 196.0 kB/s eta 0:00:20\n",
            "     ------ --------------------------------- 0.7/4.5 MB 196.0 kB/s eta 0:00:20\n",
            "     ------ --------------------------------- 0.7/4.5 MB 197.5 kB/s eta 0:00:20\n",
            "     ------ --------------------------------- 0.7/4.5 MB 197.7 kB/s eta 0:00:19\n",
            "     ------ --------------------------------- 0.7/4.5 MB 197.7 kB/s eta 0:00:19\n",
            "     ------ --------------------------------- 0.7/4.5 MB 198.3 kB/s eta 0:00:19\n",
            "     ------ --------------------------------- 0.8/4.5 MB 200.4 kB/s eta 0:00:19\n",
            "     ------ --------------------------------- 0.8/4.5 MB 200.4 kB/s eta 0:00:19\n",
            "     ------ --------------------------------- 0.8/4.5 MB 200.4 kB/s eta 0:00:19\n",
            "     ------- -------------------------------- 0.8/4.5 MB 201.9 kB/s eta 0:00:19\n",
            "     ------- -------------------------------- 0.8/4.5 MB 201.9 kB/s eta 0:00:19\n",
            "     ------- -------------------------------- 0.8/4.5 MB 204.8 kB/s eta 0:00:18\n",
            "     ------- -------------------------------- 0.8/4.5 MB 206.7 kB/s eta 0:00:18\n",
            "     ------- -------------------------------- 0.9/4.5 MB 206.8 kB/s eta 0:00:18\n",
            "     ------- -------------------------------- 0.9/4.5 MB 206.8 kB/s eta 0:00:18\n",
            "     ------- -------------------------------- 0.9/4.5 MB 207.1 kB/s eta 0:00:18\n",
            "     ------- -------------------------------- 0.9/4.5 MB 207.1 kB/s eta 0:00:18\n",
            "     -------- ------------------------------- 0.9/4.5 MB 209.8 kB/s eta 0:00:17\n",
            "     -------- ------------------------------- 0.9/4.5 MB 212.9 kB/s eta 0:00:17\n",
            "     -------- ------------------------------- 0.9/4.5 MB 213.0 kB/s eta 0:00:17\n",
            "     -------- ------------------------------- 1.0/4.5 MB 215.4 kB/s eta 0:00:17\n",
            "     -------- ------------------------------- 1.0/4.5 MB 219.2 kB/s eta 0:00:16\n",
            "     --------- ------------------------------ 1.0/4.5 MB 221.5 kB/s eta 0:00:16\n",
            "     --------- ------------------------------ 1.0/4.5 MB 223.0 kB/s eta 0:00:16\n",
            "     --------- ------------------------------ 1.1/4.5 MB 226.6 kB/s eta 0:00:16\n",
            "     --------- ------------------------------ 1.1/4.5 MB 226.6 kB/s eta 0:00:16\n",
            "     --------- ------------------------------ 1.1/4.5 MB 230.1 kB/s eta 0:00:15\n",
            "     ---------- ----------------------------- 1.1/4.5 MB 235.0 kB/s eta 0:00:15\n",
            "     ---------- ----------------------------- 1.2/4.5 MB 238.3 kB/s eta 0:00:14\n",
            "     ---------- ----------------------------- 1.2/4.5 MB 238.3 kB/s eta 0:00:14\n",
            "     ---------- ----------------------------- 1.2/4.5 MB 238.7 kB/s eta 0:00:14\n",
            "     ---------- ----------------------------- 1.2/4.5 MB 242.7 kB/s eta 0:00:14\n",
            "     ---------- ----------------------------- 1.2/4.5 MB 242.4 kB/s eta 0:00:14\n",
            "     ----------- ---------------------------- 1.3/4.5 MB 249.6 kB/s eta 0:00:13\n",
            "     ----------- ---------------------------- 1.3/4.5 MB 253.3 kB/s eta 0:00:13\n",
            "     ----------- ---------------------------- 1.3/4.5 MB 255.8 kB/s eta 0:00:13\n",
            "     ------------ --------------------------- 1.4/4.5 MB 258.6 kB/s eta 0:00:13\n",
            "     ------------ --------------------------- 1.4/4.5 MB 261.0 kB/s eta 0:00:12\n",
            "     ------------ --------------------------- 1.4/4.5 MB 262.2 kB/s eta 0:00:12\n",
            "     ------------ --------------------------- 1.4/4.5 MB 263.0 kB/s eta 0:00:12\n",
            "     ------------ --------------------------- 1.4/4.5 MB 264.5 kB/s eta 0:00:12\n",
            "     ------------- -------------------------- 1.5/4.5 MB 267.9 kB/s eta 0:00:12\n",
            "     ------------- -------------------------- 1.5/4.5 MB 270.4 kB/s eta 0:00:12\n",
            "     ------------- -------------------------- 1.5/4.5 MB 272.9 kB/s eta 0:00:11\n",
            "     ------------- -------------------------- 1.6/4.5 MB 278.8 kB/s eta 0:00:11\n",
            "     -------------- ------------------------- 1.6/4.5 MB 279.0 kB/s eta 0:00:11\n",
            "     -------------- ------------------------- 1.6/4.5 MB 278.8 kB/s eta 0:00:11\n",
            "     -------------- ------------------------- 1.6/4.5 MB 281.7 kB/s eta 0:00:11\n",
            "     -------------- ------------------------- 1.6/4.5 MB 283.9 kB/s eta 0:00:10\n",
            "     -------------- ------------------------- 1.7/4.5 MB 286.2 kB/s eta 0:00:10\n",
            "     --------------- ------------------------ 1.7/4.5 MB 289.9 kB/s eta 0:00:10\n",
            "     --------------- ------------------------ 1.8/4.5 MB 296.3 kB/s eta 0:00:10\n",
            "     --------------- ------------------------ 1.8/4.5 MB 297.5 kB/s eta 0:00:10\n",
            "     ---------------- ----------------------- 1.8/4.5 MB 302.0 kB/s eta 0:00:09\n",
            "     ---------------- ----------------------- 1.9/4.5 MB 306.4 kB/s eta 0:00:09\n",
            "     ---------------- ----------------------- 1.9/4.5 MB 309.1 kB/s eta 0:00:09\n",
            "     ----------------- ---------------------- 1.9/4.5 MB 315.1 kB/s eta 0:00:09\n",
            "     ----------------- ---------------------- 2.0/4.5 MB 317.7 kB/s eta 0:00:08\n",
            "     ----------------- ---------------------- 2.0/4.5 MB 317.8 kB/s eta 0:00:08\n",
            "     ------------------ --------------------- 2.0/4.5 MB 322.8 kB/s eta 0:00:08\n",
            "     ------------------ --------------------- 2.0/4.5 MB 323.7 kB/s eta 0:00:08\n",
            "     ------------------ --------------------- 2.1/4.5 MB 326.1 kB/s eta 0:00:08\n",
            "     ------------------ --------------------- 2.1/4.5 MB 328.5 kB/s eta 0:00:08\n",
            "     ------------------- -------------------- 2.1/4.5 MB 331.7 kB/s eta 0:00:08\n",
            "     ------------------- -------------------- 2.2/4.5 MB 335.6 kB/s eta 0:00:07\n",
            "     ------------------- -------------------- 2.2/4.5 MB 337.9 kB/s eta 0:00:07\n",
            "     -------------------- ------------------- 2.2/4.5 MB 339.4 kB/s eta 0:00:07\n",
            "     -------------------- ------------------- 2.3/4.5 MB 342.4 kB/s eta 0:00:07\n",
            "     -------------------- ------------------- 2.3/4.5 MB 342.3 kB/s eta 0:00:07\n",
            "     -------------------- ------------------- 2.3/4.5 MB 344.5 kB/s eta 0:00:07\n",
            "     --------------------- ------------------ 2.4/4.5 MB 347.4 kB/s eta 0:00:07\n",
            "     --------------------- ------------------ 2.4/4.5 MB 348.1 kB/s eta 0:00:07\n",
            "     --------------------- ------------------ 2.4/4.5 MB 347.9 kB/s eta 0:00:07\n",
            "     --------------------- ------------------ 2.4/4.5 MB 351.5 kB/s eta 0:00:06\n",
            "     --------------------- ------------------ 2.5/4.5 MB 352.8 kB/s eta 0:00:06\n",
            "     ---------------------- ----------------- 2.5/4.5 MB 354.8 kB/s eta 0:00:06\n",
            "     ---------------------- ----------------- 2.5/4.5 MB 356.8 kB/s eta 0:00:06\n",
            "     ---------------------- ----------------- 2.5/4.5 MB 359.6 kB/s eta 0:00:06\n",
            "     ----------------------- ---------------- 2.6/4.5 MB 363.0 kB/s eta 0:00:06\n",
            "     ----------------------- ---------------- 2.6/4.5 MB 364.1 kB/s eta 0:00:06\n",
            "     ----------------------- ---------------- 2.6/4.5 MB 364.7 kB/s eta 0:00:06\n",
            "     ----------------------- ---------------- 2.7/4.5 MB 367.9 kB/s eta 0:00:05\n",
            "     ------------------------ --------------- 2.7/4.5 MB 369.1 kB/s eta 0:00:05\n",
            "     ------------------------ --------------- 2.7/4.5 MB 368.2 kB/s eta 0:00:05\n",
            "     ------------------------ --------------- 2.8/4.5 MB 370.6 kB/s eta 0:00:05\n",
            "     ------------------------ --------------- 2.8/4.5 MB 370.4 kB/s eta 0:00:05\n",
            "     ------------------------- -------------- 2.8/4.5 MB 373.4 kB/s eta 0:00:05\n",
            "     ------------------------- -------------- 2.8/4.5 MB 375.1 kB/s eta 0:00:05\n",
            "     ------------------------- -------------- 2.9/4.5 MB 377.0 kB/s eta 0:00:05\n",
            "     ------------------------- -------------- 2.9/4.5 MB 376.0 kB/s eta 0:00:05\n",
            "     -------------------------- ------------- 2.9/4.5 MB 378.3 kB/s eta 0:00:05\n",
            "     -------------------------- ------------- 2.9/4.5 MB 380.0 kB/s eta 0:00:05\n",
            "     -------------------------- ------------- 3.0/4.5 MB 383.5 kB/s eta 0:00:04\n",
            "     -------------------------- ------------- 3.0/4.5 MB 383.3 kB/s eta 0:00:04\n",
            "     --------------------------- ------------ 3.0/4.5 MB 383.6 kB/s eta 0:00:04\n",
            "     --------------------------- ------------ 3.1/4.5 MB 387.0 kB/s eta 0:00:04\n",
            "     --------------------------- ------------ 3.1/4.5 MB 386.8 kB/s eta 0:00:04\n",
            "     ---------------------------- ----------- 3.1/4.5 MB 390.2 kB/s eta 0:00:04\n",
            "     ---------------------------- ----------- 3.2/4.5 MB 391.7 kB/s eta 0:00:04\n",
            "     ---------------------------- ----------- 3.2/4.5 MB 395.7 kB/s eta 0:00:04\n",
            "     ----------------------------- ---------- 3.2/4.5 MB 396.7 kB/s eta 0:00:04\n",
            "     ----------------------------- ---------- 3.3/4.5 MB 397.5 kB/s eta 0:00:04\n",
            "     ----------------------------- ---------- 3.3/4.5 MB 398.9 kB/s eta 0:00:03\n",
            "     ----------------------------- ---------- 3.3/4.5 MB 399.9 kB/s eta 0:00:03\n",
            "     ------------------------------ --------- 3.4/4.5 MB 402.6 kB/s eta 0:00:03\n",
            "     ------------------------------ --------- 3.4/4.5 MB 404.4 kB/s eta 0:00:03\n",
            "     ------------------------------ --------- 3.4/4.5 MB 404.2 kB/s eta 0:00:03\n",
            "     ------------------------------ --------- 3.5/4.5 MB 404.8 kB/s eta 0:00:03\n",
            "     ------------------------------- -------- 3.5/4.5 MB 405.7 kB/s eta 0:00:03\n",
            "     ------------------------------- -------- 3.5/4.5 MB 407.1 kB/s eta 0:00:03\n",
            "     ------------------------------- -------- 3.5/4.5 MB 408.4 kB/s eta 0:00:03\n",
            "     ------------------------------- -------- 3.6/4.5 MB 411.7 kB/s eta 0:00:03\n",
            "     -------------------------------- ------- 3.6/4.5 MB 411.5 kB/s eta 0:00:03\n",
            "     -------------------------------- ------- 3.7/4.5 MB 415.1 kB/s eta 0:00:02\n",
            "     -------------------------------- ------- 3.7/4.5 MB 417.2 kB/s eta 0:00:02\n",
            "     --------------------------------- ------ 3.7/4.5 MB 418.4 kB/s eta 0:00:02\n",
            "     --------------------------------- ------ 3.7/4.5 MB 419.3 kB/s eta 0:00:02\n",
            "     --------------------------------- ------ 3.7/4.5 MB 419.0 kB/s eta 0:00:02\n",
            "     --------------------------------- ------ 3.8/4.5 MB 418.7 kB/s eta 0:00:02\n",
            "     --------------------------------- ------ 3.8/4.5 MB 419.5 kB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 3.8/4.5 MB 419.3 kB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 3.9/4.5 MB 421.3 kB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 3.9/4.5 MB 423.6 kB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 3.9/4.5 MB 424.7 kB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 4.0/4.5 MB 425.9 kB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 4.0/4.5 MB 429.2 kB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 4.0/4.5 MB 428.2 kB/s eta 0:00:02\n",
            "     ------------------------------------ --- 4.0/4.5 MB 428.3 kB/s eta 0:00:02\n",
            "     ------------------------------------ --- 4.1/4.5 MB 429.0 kB/s eta 0:00:01\n",
            "     ------------------------------------ --- 4.1/4.5 MB 429.4 kB/s eta 0:00:01\n",
            "     ------------------------------------- -- 4.1/4.5 MB 431.9 kB/s eta 0:00:01\n",
            "     ------------------------------------- -- 4.2/4.5 MB 431.6 kB/s eta 0:00:01\n",
            "     ------------------------------------- -- 4.2/4.5 MB 432.0 kB/s eta 0:00:01\n",
            "     ------------------------------------- -- 4.2/4.5 MB 433.1 kB/s eta 0:00:01\n",
            "     -------------------------------------- - 4.3/4.5 MB 435.9 kB/s eta 0:00:01\n",
            "     -------------------------------------- - 4.3/4.5 MB 438.0 kB/s eta 0:00:01\n",
            "     -------------------------------------- - 4.3/4.5 MB 440.1 kB/s eta 0:00:01\n",
            "     ---------------------------------------  4.4/4.5 MB 441.1 kB/s eta 0:00:01\n",
            "     ---------------------------------------  4.4/4.5 MB 440.0 kB/s eta 0:00:01\n",
            "     ---------------------------------------  4.4/4.5 MB 440.0 kB/s eta 0:00:01\n",
            "     ---------------------------------------  4.4/4.5 MB 441.7 kB/s eta 0:00:01\n",
            "     ---------------------------------------  4.5/4.5 MB 442.4 kB/s eta 0:00:01\n",
            "     ---------------------------------------  4.5/4.5 MB 441.3 kB/s eta 0:00:01\n",
            "     ---------------------------------------- 4.5/4.5 MB 440.2 kB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Collecting deap==1.2.2 (from elm)\n",
            "  Downloading deap-1.2.2.tar.gz (936 kB)\n",
            "     ---------------------------------------- 0.0/936.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/936.6 kB ? eta -:--:--\n",
            "     ---------------------------------------- 10.2/936.6 kB ? eta -:--:--\n",
            "     - ----------------------------------- 41.0/936.6 kB 487.6 kB/s eta 0:00:02\n",
            "     - ----------------------------------- 41.0/936.6 kB 487.6 kB/s eta 0:00:02\n",
            "     -- ---------------------------------- 61.4/936.6 kB 326.1 kB/s eta 0:00:03\n",
            "     --- --------------------------------- 81.9/936.6 kB 381.3 kB/s eta 0:00:03\n",
            "     --- --------------------------------- 92.2/936.6 kB 348.6 kB/s eta 0:00:03\n",
            "     ---- ------------------------------- 112.6/936.6 kB 363.1 kB/s eta 0:00:03\n",
            "     ---- ------------------------------- 122.9/936.6 kB 342.4 kB/s eta 0:00:03\n",
            "     ------ ----------------------------- 163.8/936.6 kB 409.6 kB/s eta 0:00:02\n",
            "     ------ ----------------------------- 174.1/936.6 kB 388.2 kB/s eta 0:00:02\n",
            "     ------- ---------------------------- 204.8/936.6 kB 401.2 kB/s eta 0:00:02\n",
            "     --------- -------------------------- 245.8/936.6 kB 442.7 kB/s eta 0:00:02\n",
            "     --------- -------------------------- 256.0/936.6 kB 436.5 kB/s eta 0:00:02\n",
            "     ---------- ------------------------- 276.5/936.6 kB 436.5 kB/s eta 0:00:02\n",
            "     ----------- ------------------------ 307.2/936.6 kB 441.9 kB/s eta 0:00:02\n",
            "     ------------ ----------------------- 337.9/936.6 kB 455.7 kB/s eta 0:00:02\n",
            "     ------------- ---------------------- 358.4/936.6 kB 454.5 kB/s eta 0:00:02\n",
            "     -------------- --------------------- 389.1/936.6 kB 466.0 kB/s eta 0:00:02\n",
            "     --------------- -------------------- 409.6/936.6 kB 473.2 kB/s eta 0:00:02\n",
            "     ---------------- ------------------- 419.8/936.6 kB 468.1 kB/s eta 0:00:02\n",
            "     ---------------- ------------------- 440.3/936.6 kB 458.5 kB/s eta 0:00:02\n",
            "     ------------------ ----------------- 471.0/936.6 kB 460.8 kB/s eta 0:00:02\n",
            "     ------------------- ---------------- 501.8/936.6 kB 469.5 kB/s eta 0:00:01\n",
            "     ------------------- ---------------- 501.8/936.6 kB 469.5 kB/s eta 0:00:01\n",
            "     -------------------- --------------- 522.2/936.6 kB 455.1 kB/s eta 0:00:01\n",
            "     -------------------- --------------- 532.5/936.6 kB 445.6 kB/s eta 0:00:01\n",
            "     --------------------- -------------- 553.0/936.6 kB 445.2 kB/s eta 0:00:01\n",
            "     ---------------------- ------------- 583.7/936.6 kB 447.3 kB/s eta 0:00:01\n",
            "     ----------------------- ------------ 614.4/936.6 kB 454.6 kB/s eta 0:00:01\n",
            "     ------------------------ ----------- 634.9/936.6 kB 459.3 kB/s eta 0:00:01\n",
            "     ------------------------- ---------- 665.6/936.6 kB 460.9 kB/s eta 0:00:01\n",
            "     -------------------------- --------- 686.1/936.6 kB 464.8 kB/s eta 0:00:01\n",
            "     --------------------------- -------- 716.8/936.6 kB 471.0 kB/s eta 0:00:01\n",
            "     --------------------------- -------- 716.8/936.6 kB 471.0 kB/s eta 0:00:01\n",
            "     ---------------------------- ------- 747.5/936.6 kB 457.9 kB/s eta 0:00:01\n",
            "     ---------------------------- ------- 747.5/936.6 kB 457.9 kB/s eta 0:00:01\n",
            "     ---------------------------- ------- 747.5/936.6 kB 457.9 kB/s eta 0:00:01\n",
            "     ----------------------------- ------ 768.0/936.6 kB 440.8 kB/s eta 0:00:01\n",
            "     ------------------------------- ---- 819.2/936.6 kB 450.2 kB/s eta 0:00:01\n",
            "     ------------------------------- ---- 829.4/936.6 kB 447.9 kB/s eta 0:00:01\n",
            "     --------------------------------- -- 880.6/936.6 kB 460.3 kB/s eta 0:00:01\n",
            "     ---------------------------------- - 901.1/936.6 kB 463.5 kB/s eta 0:00:01\n",
            "     -----------------------------------  911.4/936.6 kB 461.2 kB/s eta 0:00:01\n",
            "     ------------------------------------ 936.6/936.6 kB 455.9 kB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'error'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  error: subprocess-exited-with-error\n",
            "  \n",
            "  × python setup.py egg_info did not run successfully.\n",
            "  │ exit code: 1\n",
            "  ╰─> [1 lines of output]\n",
            "      error in deap setup command: use_2to3 is invalid.\n",
            "      [end of output]\n",
            "  \n",
            "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "error: metadata-generation-failed\n",
            "\n",
            "× Encountered error while generating package metadata.\n",
            "╰─> See above for output.\n",
            "\n",
            "note: This is an issue with the package mentioned above, not pip.\n",
            "hint: See above for details.\n",
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install elm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'elm'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01melm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ELMRegressor\n\u001b[0;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExtreme Learning Machine (ELM)\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(model_name)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'elm'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from elm import ELMRegressor\n",
        "\n",
        "model_name = 'Extreme Learning Machine (ELM)'\n",
        "print(model_name)\n",
        "\n",
        "elm = ELMRegressor()\n",
        "\n",
        "param_grid = {\n",
        "    'n_neurons': [10, 20, 50],\n",
        "    'alpha': [0.01, 0.1, 1]\n",
        "}\n",
        "\n",
        "grid_search = grid_search_2 = GridSearchCV(estimator=elm, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "y_train_pred = grid_search.predict(X_train_prep)\n",
        "y_val_pred = grid_search.predict(X_val_prep)\n",
        "y_test_pred = grid_search.predict(X_test_prep)\n",
        "\n",
        "# Evaluar el modelo\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\",model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc = \"NO\")\n",
        "\n",
        "''' Enfoque 2 '''\n",
        "grid_search_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "y_train_pred_2 = grid_search_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = grid_search_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = grid_search_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc = \"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc = \"NO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Perceptrón multicapa (MLP)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptrón Multicapa (MLP)\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8320\n",
            " - Precision: 0.8392\n",
            " - Recall: 0.8320\n",
            " - F1-Score: 0.8324\n",
            " - Adjusted Rand Index: 0.4403\n",
            " - Mean Squared Error: 0.1680\n",
            " - R-squared: 0.3202\n",
            " - Área bajo la curva : 0.837\n",
            " - Confusion Matrix: \n",
            "[[429  58]\n",
            " [125 477]]\n",
            " - Global Score : 79.06\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.7967\n",
            " - Precision: 0.8107\n",
            " - Recall: 0.7967\n",
            " - F1-Score: 0.7967\n",
            " - Adjusted Rand Index: 0.3503\n",
            " - Mean Squared Error: 0.2033\n",
            " - R-squared: 0.1797\n",
            " - Área bajo la curva : 0.804\n",
            " - Confusion Matrix: \n",
            "[[145  20]\n",
            " [ 54 145]]\n",
            " - Global Score : 75.25\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.7775\n",
            " - Precision: 0.7888\n",
            " - Recall: 0.7775\n",
            " - F1-Score: 0.7782\n",
            " - Adjusted Rand Index: 0.3061\n",
            " - Mean Squared Error: 0.2225\n",
            " - R-squared: 0.0967\n",
            " - Área bajo la curva : 0.784\n",
            " - Confusion Matrix: \n",
            "[[134  26]\n",
            " [ 55 149]]\n",
            " - Global Score : 72.71\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 0.8898\n",
            " - Precision: 0.8897\n",
            " - Recall: 0.8898\n",
            " - F1-Score: 0.8897\n",
            " - Adjusted Rand Index: 0.6074\n",
            " - Mean Squared Error: 0.1102\n",
            " - R-squared: 0.5543\n",
            " - Área bajo la curva : 0.887\n",
            " - Confusion Matrix: \n",
            "[[421  66]\n",
            " [ 54 548]]\n",
            " - Global Score : 85.31\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.8791\n",
            " - Precision: 0.8790\n",
            " - Recall: 0.8791\n",
            " - F1-Score: 0.8791\n",
            " - Adjusted Rand Index: 0.5737\n",
            " - Mean Squared Error: 0.1209\n",
            " - R-squared: 0.5122\n",
            " - Área bajo la curva : 0.878\n",
            " - Confusion Matrix: \n",
            "[[142  23]\n",
            " [ 21 178]]\n",
            " - Global Score : 84.04\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.8489\n",
            " - Precision: 0.8487\n",
            " - Recall: 0.8489\n",
            " - F1-Score: 0.8487\n",
            " - Adjusted Rand Index: 0.4854\n",
            " - Mean Squared Error: 0.1511\n",
            " - R-squared: 0.3866\n",
            " - Área bajo la curva : 0.846\n",
            " - Confusion Matrix: \n",
            "[[131  29]\n",
            " [ 26 178]]\n",
            " - Global Score : 80.13\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos Entrenados/mlp_e2_1.joblib']"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Nombre del modelo\n",
        "model_name = 'Perceptrón Multicapa (MLP)'\n",
        "print(model_name)\n",
        "\n",
        "# Definimos el modelo MLP\n",
        "mlp = MLPRegressor(max_iter=1000)\n",
        "\n",
        "# Definimos los parámetros que queremos optimizar\n",
        "param_grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 100)],\n",
        "    'alpha': [0.0001, 0.001, 0.01],\n",
        "    'learning_rate_init': [0.001, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "# Usamos GridSearchCV para encontrar los mejores hiperparámetros para el enfoque 1\n",
        "grid_search_1 = GridSearchCV(estimator=mlp, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "# Entrenamos el modelo con el enfoque 1\n",
        "grid_search_1.fit(X_train_prep, y_train)\n",
        "\n",
        "# Hacemos predicciones con el mejor modelo encontrado para el enfoque 1\n",
        "y_train_pred_1 = grid_search_1.predict(X_train_prep)\n",
        "y_val_pred_1 = grid_search_1.predict(X_val_prep)\n",
        "y_test_pred_1 = grid_search_1.predict(X_test_prep)\n",
        "\n",
        "threshold = 0.5\n",
        "y_train_pred_1_bin = (y_train_pred_1 > threshold).astype(int)\n",
        "y_val_pred_1_bin = (y_val_pred_1 > threshold).astype(int)\n",
        "y_test_pred_1_bin = (y_test_pred_1 > threshold).astype(int)\n",
        "\n",
        "# Evaluar el modelo con el enfoque 1\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred_1_bin, \"Training\", model_name, enfoque=\"1\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred_1_bin, \"Validation\", model_name, enfoque=\"1\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred_1_bin, \"Test\", model_name, enfoque=\"1\", print_roc=\"NO\")\n",
        "\n",
        "# Usamos GridSearchCV para encontrar los mejores hiperparámetros para el enfoque 2\n",
        "grid_search_2 = GridSearchCV(estimator=mlp, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "# Entrenamos el modelo con el enfoque 2\n",
        "grid_search_2.fit(X_train_prep_2, y_train_2)\n",
        "\n",
        "# Hacemos predicciones con el mejor modelo encontrado para el enfoque 2\n",
        "y_train_pred_2 = grid_search_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = grid_search_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = grid_search_2.predict(X_test_prep_2)\n",
        "\n",
        "y_train_pred_2_bin = (y_train_pred_2 > threshold).astype(int)\n",
        "y_val_pred_2_bin = (y_val_pred_2 > threshold).astype(int)\n",
        "y_test_pred_2_bin = (y_test_pred_2 > threshold).astype(int)\n",
        "\n",
        "# Evaluar el modelo con el enfoque 2\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2_bin, \"Training\", model_name, enfoque=\"2\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2_bin, \"Validation\", model_name, enfoque=\"2\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2_bin, \"Test\", model_name, enfoque=\"2\", print_roc=\"NO\")\n",
        "\n",
        "dump(grid_search_1, 'Modelos supervisados entrenados/mlp_e1_1.joblib')\n",
        "dump(grid_search_2, 'Modelos supervisados entrenados/mlp_e2_1.joblib')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Red Neuronal Recurrente (RNN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Red Neuronal Recurrente (RNN)\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'reshape'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21980\\2019065353.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[0mgrid_search_1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrnn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'neg_mean_squared_error'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Ajustamos la forma de los datos para que sean compatibles con RNN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mX_train_prep_rnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_prep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_prep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_prep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mX_val_prep_rnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_val_prep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_prep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val_prep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[0mX_test_prep_rnn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test_prep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test_prep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_prep\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Entrenamos el modelo con el enfoque 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\danie\\.virtualenvs\\Repositorio_compartido-Xxik1eBC\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   6200\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6201\u001b[0m             \u001b[1;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6202\u001b[0m         \u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6203\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'reshape'"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, SimpleRNN\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "# Nombre del modelo\n",
        "model_name = 'Red Neuronal Recurrente (RNN)'\n",
        "print(model_name)\n",
        "\n",
        "# Función para crear el modelo RNN\n",
        "def create_rnn_model(units=50, activation='relu', optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(units=units, activation=activation, input_shape=(X_train_prep.shape[1], 1)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Definimos el modelo KerasRegressor\n",
        "rnn = KerasRegressor(model=create_rnn_model, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "# Definimos los parámetros que queremos optimizar\n",
        "param_grid = {\n",
        "    'model__units': [50, 100],\n",
        "    'model__activation': ['relu', 'tanh'],\n",
        "    'model__optimizer': ['adam', 'rmsprop']\n",
        "}\n",
        "\n",
        "# Usamos GridSearchCV para encontrar los mejores hiperparámetros para el enfoque 1\n",
        "grid_search_1 = GridSearchCV(estimator=rnn, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "# Ajustamos la forma de los datos para que sean compatibles con RNN\n",
        "X_train_prep_rnn = X_train_prep.reshape((X_train_prep.shape[0], X_train_prep.shape[1], 1))\n",
        "X_val_prep_rnn = X_val_prep.reshape((X_val_prep.shape[0], X_val_prep.shape[1], 1))\n",
        "X_test_prep_rnn = X_test_prep.reshape((X_test_prep.shape[0], X_test_prep.shape[1], 1))\n",
        "\n",
        "# Entrenamos el modelo con el enfoque 1\n",
        "grid_search_1.fit(X_train_prep_rnn, y_train)\n",
        "\n",
        "# Hacemos predicciones con el mejor modelo encontrado para el enfoque 1\n",
        "y_train_pred_1 = grid_search_1.predict(X_train_prep_rnn)\n",
        "y_val_pred_1 = grid_search_1.predict(X_val_prep_rnn)\n",
        "y_test_pred_1 = grid_search_1.predict(X_test_prep_rnn)\n",
        "\n",
        "# Evaluar el modelo con el enfoque 1\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred_1, \"Training\", model_name, enfoque=\"1\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred_1, \"Validation\", model_name, enfoque=\"1\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred_1, \"Test\", model_name, enfoque=\"1\", print_roc=\"NO\")\n",
        "\n",
        "# Función para crear el modelo RNN para el enfoque 2\n",
        "def create_rnn_model2(units=50, activation='relu', optimizer='adam'):\n",
        "    model = Sequential()\n",
        "    model.add(SimpleRNN(units=units, activation=activation, input_shape=(X_train_prep_2.shape[1], 1)))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "rnn2 = KerasRegressor(model=create_rnn_model2, epochs=50, batch_size=32, verbose=0)\n",
        "\n",
        "# Usamos GridSearchCV para encontrar los mejores hiperparámetros para el enfoque 2\n",
        "grid_search_2 = GridSearchCV(estimator=rnn2, param_grid=param_grid, scoring='neg_mean_squared_error', cv=3)\n",
        "\n",
        "# Ajustamos la forma de los datos para el enfoque 2\n",
        "X_train_prep_2_rnn = X_train_prep_2.reshape((X_train_prep_2.shape[0], X_train_prep_2.shape[1], 1))\n",
        "X_val_prep_2_rnn = X_val_prep_2.reshape((X_val_prep_2.shape[0], X_val_prep_2.shape[1], 1))\n",
        "X_test_prep_2_rnn = X_test_prep_2.reshape((X_test_prep_2.shape[0], X_test_prep_2.shape[1], 1))\n",
        "\n",
        "# Entrenamos el modelo con el enfoque 2\n",
        "grid_search_2.fit(X_train_prep_2_rnn, y_train_2)\n",
        "\n",
        "# Hacemos predicciones con el mejor modelo encontrado para el enfoque 2\n",
        "y_train_pred_2 = grid_search_2.predict(X_train_prep_2_rnn)\n",
        "y_val_pred_2 = grid_search_2.predict(X_val_prep_2_rnn)\n",
        "y_test_pred_2 = grid_search_2.predict(X_test_prep_2_rnn)\n",
        "\n",
        "# Evaluar el modelo con el enfoque 2\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc=\"NO\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformar tabla_results_df a formato Excel "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pasar a formato excel \n",
        "tabla_results_df.to_excel('model_results.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tabla final modelos para predicción en test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Model                  | Set  | Accuracy   | Precision  | Recall     | F1-Score   | Adjusted Rand Index | Mean Squared Error | R-squared  | AUC-ROC    | TN  | FP | FN | TP  | Global Score |\n",
        "|------------------------|------|------------|------------|------------|------------|----------------------|--------------------|------------|------------|-----|----|----|-----|--------------|\n",
        "| Regresion Lineal       | Test | 0.760989   | 0.796967   | 0.760989   | 0.759651   | 0.270159             | 0.239011           | 0.029779   | 0.776654   | 145 | 15 | 72 | 132 | 72.02         |\n",
        "| Regresión Logística    | Test | 0.802198   | 0.817277   | 0.802198   | 0.802706   | 0.363533             | 0.197802           | 0.197059   | 0.810723   | 141 | 19 | 53 | 151 | 75.89         |\n",
        "| Árbol de Decisión      | Test | 0.851648   | 0.851536   | 0.851648   | 0.851173   | 0.493086             | 0.148352           | 0.397794   | 0.846752   | 129 | 31 | 23 | 181 | 80.30         |\n",
        "| Random Forest          | Test | 0.909341   | 0.910202   | 0.909341   | 0.908917   | 0.669212             | 0.090659           | 0.631985   | 0.904289   | 138 | 22 | 11 | 193 | 87.54         |\n",
        "| KNN                    | Test | 0.774725   | 0.785429   | 0.774725   | 0.775488   | 0.299987             | 0.225275           | 0.085539   | 0.780821   | 133 | 27 | 55 | 149 | 72.34         |\n",
        "| SVN                    | Test | 0.804945   | 0.811726   | 0.804945   | 0.805659   | 0.370256             | 0.195055           | 0.208211   | 0.809130   | 135 | 25 | 46 | 158 | 75.67         |\n",
        "| Naive Bayes - Gaussian | Test | 0.708791   | 0.792288   | 0.708791   | 0.698914   | 0.170664             | 0.291209           | -0.182108  | 0.735478   | 153 | 7  | 99 | 105 | 67.68         |\n",
        "| Naive Bayes - Bernoulli| Test | 0.755495   | 0.782567   | 0.755495   | 0.755042   | 0.258904             | 0.244505           | 0.007475   | 0.768382   | 140 | 20 | 69 | 135 | 70.99         |\n",
        "| AdaBoost 1             | Test | 0.920330   | 0.920606   | 0.920330   | 0.920405   | 0.705872             | 0.079670           | 0.676593   | 0.920159   | 147 | 13 | 16 | 188 | 89.46         |\n",
        "| AdaBoost 2             | Test | 0.923077   | 0.923077   | 0.923077   | 0.922963   | 0.715134             | 0.076923           | 0.687745   | 0.920588   | 144 | 16 | 12 | 192 | 89.57         |\n",
        "| Gradient Boosting      | Test | 0.925824   | 0.925787   | 0.925824   | 0.925799   | 0.724505             | 0.074176           | 0.698897   | 0.924387   | 146 | 14 | 13 | 191 | 90.04         |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusiones\n",
        "\n",
        "Se han obtenido resultados bastante positivos con ciertos modelos supervisados :     \n",
        "| Model             | Set  | Accuracy   | Precision  | Recall     | F1-Score   | Adjusted Rand Index | Mean Squared Error | R-squared  | AUC-ROC    | TN  | FP | FN | TP  | Global Score |\n",
        "|-------------------|------|------------|------------|------------|------------|----------------------|--------------------|------------|------------|-----|----|----|-----|--------------|\n",
        "| Random Forest     | Test | 0.909341   | 0.910202   | 0.909341   | 0.908917   | 0.669212             | 0.090659           | 0.631985   | 0.904289   | 138 | 22 | 11 | 193 | 87.54         |\n",
        "| AdaBoost 2        | Test | 0.923077   | 0.923077   | 0.923077   | 0.922963   | 0.715134             | 0.076923           | 0.687745   | 0.920588   | 144 | 16 | 12 | 192 | 89.57         |\n",
        "| Gradient Boosting | Test | 0.925824   | 0.925787   | 0.925824   | 0.925799   | 0.724505             | 0.074176           | 0.698897   | 0.924387   | 146 | 14 | 13 | 191 | 90.04         |\n",
        "\n",
        "\n",
        "Siendo bastante simples y poco costosos a nivel computacional, vamos a probar a combinarlos con modelos de aprendizaje no supervisado para ver si se pueden mejorar los resultados.\n",
        "\n",
        "Para ellos vamos a preprocesar los datos usando los modelos no supervisados de forma que podamos explotar diferentes aspectos de los datos: \n",
        "\n",
        "    - clustering puede descubrir estructuras ocultas\n",
        "    - la reducción de dimensionalidad puede simplificar la información\n",
        "    - los autoencoders pueden encontrar representaciones más efectivas de los datos\n",
        "\n",
        "De esta manera vamos a crear primero los modelos no supervisados y una vez ajustados, procederemos a combinarlos con los supervisados."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Combinación de modelos mediante Voting classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Un Voting Classifier es un tipo de modelo de ensamble en aprendizaje automático que combina las predicciones de múltiples modelos individuales (también conocidos como estimadores base) para mejorar el rendimiento y la precisión en la clasificación. La idea principal es que al combinar varios modelos, se pueden mitigar los errores individuales y aprovechar las fortalezas de cada uno\n",
        "\n",
        "- Voting Classifier Mayoritario (Hard Voting):\n",
        "\n",
        "    En este tipo, cada modelo individual realiza una predicción para una instancia dada y se toma una votación para determinar la clase final.\n",
        "    La clase final asignada es la que recibe la mayoría de los votos (es decir, la clase más frecuente entre todas las predicciones de los modelos).\n",
        "    Por ejemplo, si tres modelos predicen las clases A, B y B, respectivamente, la predicción final sería B, ya que tiene la mayoría de los votos.\n",
        "\n",
        "- Voting Classifier Ponderado (Soft Voting):\n",
        "\n",
        "    Aquí, cada modelo calcula las probabilidades de cada clase en lugar de hacer una predicción directa.\n",
        "    Las probabilidades se suman (posiblemente ponderadas si se quiere dar más peso a ciertos modelos) y la clase con la mayor probabilidad agregada se selecciona como la predicción final.\n",
        "    Esto permite una combinación más matizada de los modelos, ya que se tienen en cuenta las probabilidades asociadas con cada predicción."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "validation_fraction : \n",
        "Durante el entrenamiento de un modelo de Gradient Boosting, el algoritmo ajusta los árboles secuencialmente. Si se establece el validation_fraction, el modelo se entrena en la fracción restante de los datos (por ejemplo, el 80% si validation_fraction=0.2) y se evalúa en el 20% reservado. Si el rendimiento del modelo en este conjunto de validación interna deja de mejorar durante un número específico de iteraciones (n_iter_no_change), el entrenamiento se detiene temprano para evitar el sobreajuste."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Metrics for Training set :\n",
            " - Accuracy: 0.9770\n",
            " - Precision: 0.9770\n",
            " - Recall: 0.9770\n",
            " - F1-Score: 0.9770\n",
            " - Adjusted Rand Index: 0.9102\n",
            " - Mean Squared Error: 0.0230\n",
            " - R-squared: 0.9071\n",
            " - Área bajo la curva : 0.977\n",
            " - Confusion Matrix: \n",
            "[[475  12]\n",
            " [ 13 589]]\n",
            " - Global Score : 96.9\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9505\n",
            " - Precision: 0.9512\n",
            " - Recall: 0.9505\n",
            " - F1-Score: 0.9506\n",
            " - Adjusted Rand Index: 0.8115\n",
            " - Mean Squared Error: 0.0495\n",
            " - R-squared: 0.8005\n",
            " - Área bajo la curva : 0.952\n",
            " - Confusion Matrix: \n",
            "[[159   6]\n",
            " [ 12 187]]\n",
            " - Global Score : 93.57\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9478\n",
            " - Precision: 0.9482\n",
            " - Recall: 0.9478\n",
            " - F1-Score: 0.9477\n",
            " - Adjusted Rand Index: 0.8015\n",
            " - Mean Squared Error: 0.0522\n",
            " - R-squared: 0.7881\n",
            " - Área bajo la curva : 0.945\n",
            " - Confusion Matrix: \n",
            "[[147  13]\n",
            " [  6 198]]\n",
            " - Global Score : 92.72\n",
            "\n",
            "Metrics for Training set :\n",
            " - Accuracy: 1.0000\n",
            " - Precision: 1.0000\n",
            " - Recall: 1.0000\n",
            " - F1-Score: 1.0000\n",
            " - Adjusted Rand Index: 1.0000\n",
            " - Mean Squared Error: 0.0000\n",
            " - R-squared: 1.0000\n",
            " - Área bajo la curva : 1.000\n",
            " - Confusion Matrix: \n",
            "[[487   0]\n",
            " [  0 602]]\n",
            " - Global Score : 100.0\n",
            "\n",
            "Metrics for Validation set :\n",
            " - Accuracy: 0.9670\n",
            " - Precision: 0.9676\n",
            " - Recall: 0.9670\n",
            " - F1-Score: 0.9671\n",
            " - Adjusted Rand Index: 0.8721\n",
            " - Mean Squared Error: 0.0330\n",
            " - R-squared: 0.8670\n",
            " - Área bajo la curva : 0.968\n",
            " - Confusion Matrix: \n",
            "[[162   3]\n",
            " [  9 190]]\n",
            " - Global Score : 95.76\n",
            "\n",
            "Metrics for Test set :\n",
            " - Accuracy: 0.9725\n",
            " - Precision: 0.9726\n",
            " - Recall: 0.9725\n",
            " - F1-Score: 0.9725\n",
            " - Adjusted Rand Index: 0.8928\n",
            " - Mean Squared Error: 0.0275\n",
            " - R-squared: 0.8885\n",
            " - Área bajo la curva : 0.973\n",
            " - Confusion Matrix: \n",
            "[[156   4]\n",
            " [  6 198]]\n",
            " - Global Score : 96.34\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['Modelos supervisados entrenados/voting_clf_e2_1.joblib']"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "model_name = 'Voting Classifier (KNN + Gradient Boosting + AdaBoost)'\n",
        "# Best parameters for KNN (enfoque 1)\n",
        "''' INICIAL\n",
        "# Mejores parámetros para Gradient Boosting (enfoque 1)\n",
        "best_params_gb_1 = {\n",
        "    'learning_rate': 0.5,\n",
        "    'max_depth': 5,\n",
        "    'min_samples_leaf': 2,\n",
        "    'min_samples_split': 10,\n",
        "    'n_estimators': 200\n",
        "}\n",
        "\n",
        "# Mejores parámetros para Gradient Boosting (enfoque 2)\n",
        "best_params_gb_2 = {\n",
        "    'learning_rate': 0.5,\n",
        "    'max_depth': 4,\n",
        "    'min_samples_leaf': 1,\n",
        "    'min_samples_split': 5,\n",
        "    'n_estimators': 200\n",
        "}\n",
        "\n",
        "# Mejores parámetros para AdaBoost\n",
        "best_params_ada = {\n",
        "    'learning_rate': 0.5,\n",
        "    'n_estimators': 300\n",
        "}\n",
        "''' \n",
        "'''\n",
        "best_params_knn = {\n",
        "    'algorithm': 'brute',\n",
        "    'leaf_size': 10,\n",
        "    'metric': 'hamming',\n",
        "    'n_neighbors': 3,\n",
        "    'weights': 'uniform'\n",
        "}\n",
        "\n",
        "# Best parameters for KNN (enfoque 2)\n",
        "best_params_knn_2 = {\n",
        "    'algorithm': 'brute',\n",
        "    'leaf_size': 10,\n",
        "    'metric': 'hamming',\n",
        "    'n_neighbors': 11,\n",
        "    'weights': 'distance'\n",
        "}\n",
        "\n",
        "# Mejores parámetros para Gradient Boosting (enfoque 1)\n",
        "best_params_gb_1 = {\n",
        "    'learning_rate': 0.015,  # Reducir la tasa de aprendizaje\n",
        "    'max_depth': 3,  # Reducir la profundidad máxima\n",
        "    'min_samples_leaf': 4,\n",
        "    'min_samples_split': 10,\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,  # Añadir subsampling\n",
        "    'validation_fraction': 0.2,  # Usar 20% de los datos para validación\n",
        "    'n_iter_no_change': 10,  # Detener si no hay mejora en 10 iteraciones\n",
        "    'tol': 1e-4  # Tolerancia para el cambio\n",
        "}\n",
        "\n",
        "# Mejores parámetros para Gradient Boosting (enfoque 2)\n",
        "best_params_gb_2 = {\n",
        "    'learning_rate': 0.05,  # Reducir la tasa de aprendizaje\n",
        "    'max_depth': 3,  # Reducir la profundidad máxima\n",
        "    'min_samples_leaf': 4,\n",
        "    'min_samples_split': 10,\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,  # Añadir subsampling\n",
        "    'validation_fraction': 0.2,  # Usar 20% de los datos para validación\n",
        "    'n_iter_no_change': 10,  # Detener si no hay mejora en 10 iteraciones\n",
        "    'tol': 1e-4  # Tolerancia para el cambio\n",
        "}\n",
        "\n",
        "# Mejores parámetros para AdaBoost\n",
        "best_params_ada = {\n",
        "    'learning_rate': 0.015,\n",
        "    'n_estimators': 100  # Reducir el número de estimadores\n",
        "}\n",
        "'''\n",
        "best_params_knn = {\n",
        "    'algorithm': 'brute',\n",
        "    'leaf_size': 10,\n",
        "    'metric': 'hamming',\n",
        "    'n_neighbors': 3,\n",
        "    'weights': 'uniform'\n",
        "}\n",
        "\n",
        "# Best parameters for KNN (enfoque 2)\n",
        "best_params_knn_2 = {\n",
        "    'algorithm': 'brute',\n",
        "    'leaf_size': 10,\n",
        "    'metric': 'hamming',\n",
        "    'n_neighbors': 2,\n",
        "    'weights': 'distance'\n",
        "}\n",
        "\n",
        "# Mejores parámetros para Gradient Boosting (enfoque 1)\n",
        "best_params_gb_1 = {\n",
        "    'learning_rate': 0.015,  # Reducir la tasa de aprendizaje\n",
        "    'max_depth': 3,  # Reducir la profundidad máxima\n",
        "    'min_samples_leaf': 4,\n",
        "    'min_samples_split': 10,\n",
        "    'n_estimators': 200,\n",
        "    'subsample': 0.8,  # Añadir subsampling\n",
        "    'validation_fraction': 0.2,  # Usar 20% de los datos para validación\n",
        "    'n_iter_no_change': 10,  # Detener si no hay mejora en 10 iteraciones\n",
        "    'tol': 1e-4  # Tolerancia para el cambio\n",
        "}\n",
        "\n",
        "# Mejores parámetros para Gradient Boosting (enfoque 2)\n",
        "best_params_gb_2 = {\n",
        "    'learning_rate': 0.05,  # Reducir la tasa de aprendizaje\n",
        "    'max_depth': 2,  # Reducir la profundidad máxima\n",
        "    'min_samples_leaf': 1,\n",
        "    'min_samples_split': 5,\n",
        "    'n_estimators': 100,\n",
        "}\n",
        "\n",
        "# Mejores parámetros para AdaBoost\n",
        "best_params_ada = {\n",
        "    'learning_rate': 0.07,\n",
        "    'n_estimators': 100  # Reducir el número de estimadores\n",
        "}\n",
        "# Crear los modelos KNN con los mejores parámetros\n",
        "knn_1 = KNeighborsClassifier(**best_params_knn)\n",
        "knn_2 = KNeighborsClassifier(**best_params_knn_2)\n",
        "\n",
        "# Crear los modelos GradientBoosting con los mejores parámetros\n",
        "grad_boost_1 = GradientBoostingClassifier(**best_params_gb_1)\n",
        "grad_boost_2 = GradientBoostingClassifier(**best_params_gb_2)\n",
        "\n",
        "# Crear los modelos AdaBoost con los mejores parámetros\n",
        "base_estimator = DecisionTreeClassifier(max_depth=2)\n",
        "ada_boost = AdaBoostClassifier(estimator=base_estimator, **best_params_ada)\n",
        "\n",
        "# Crear el Voting Classifier\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('knn', knn_1), ('grad_boost', grad_boost_1), ('ada_boost', ada_boost)],\n",
        "    voting='soft'  # Puede ser 'hard' para votación mayoritaria\n",
        ")\n",
        "\n",
        "# Entrenar el Voting Classifier\n",
        "voting_clf.fit(X_train_prep, y_train)\n",
        "\n",
        "# Hacer predicciones\n",
        "y_train_pred = voting_clf.predict(X_train_prep)\n",
        "y_val_pred = voting_clf.predict(X_val_prep)\n",
        "y_test_pred = voting_clf.predict(X_test_prep)\n",
        "\n",
        "# Mostrar estadísticas y guardar resultados\n",
        "mostrar_estadisticas_guardar_tabla(y_train, y_train_pred, \"Training\", model_name, enfoque=\"1\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val, y_val_pred, \"Validation\", model_name, enfoque=\"1\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test, y_test_pred, \"Test\", model_name, enfoque=\"1\", print_roc=\"NO\")\n",
        "\n",
        "dump(voting_clf, 'Modelos supervisados entrenados/voting_clf_e1_1.joblib')\n",
        "\n",
        "# Enfoque 2\n",
        "# Crear el Voting Classifier\n",
        "voting_clf_2 = VotingClassifier(\n",
        "    estimators=[('knn', knn_2), ('grad_boost', grad_boost_2), ('ada_boost', ada_boost)],\n",
        "    voting='soft'  # Puede ser 'hard' para votación mayoritaria\n",
        ")\n",
        "\n",
        "voting_clf_2.fit(X_train_prep_2, y_train_2)\n",
        "y_train_pred_2 = voting_clf_2.predict(X_train_prep_2)\n",
        "y_val_pred_2 = voting_clf_2.predict(X_val_prep_2)\n",
        "y_test_pred_2 = voting_clf_2.predict(X_test_prep_2)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla(y_train_2, y_train_pred_2, \"Training\", model_name, enfoque=\"2\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_val_2, y_val_pred_2, \"Validation\", model_name, enfoque=\"2\", print_roc=\"NO\")\n",
        "mostrar_estadisticas_guardar_tabla(y_test_2, y_test_pred_2, \"Test\", model_name, enfoque=\"2\", print_roc=\"NO\")\n",
        "\n",
        "\n",
        "dump(voting_clf_2, 'Modelos supervisados entrenados/voting_clf_e2_1.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Tabla Final con todos los modelos supervisados "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Model                                        | Approach | Set   | Accuracy    | Precision    | Recall       | F1-Score     | Adjusted Rand Index | Mean Squared Error | R-squared   | AUC-ROC      | TN  | FP | FN | TP  | Global Score |\n",
        "|----------------------------------------------|----------|-------|-------------|--------------|--------------|--------------|---------------------|--------------------|-------------|--------------|-----|----|----|-----|--------------|\n",
        "| Regresión Logística                          | 1        | Test  | 0,802197802 | 0,817276538  | 0,802197802  | 0,802705769  | 0,363533396         | 0,197802198        | 0,197058824 | 0,810723039  | 141 | 19 | 53 | 151 | 75,89        |\n",
        "| Regresión Logística                          | 2        | Test  | 0,843406593 | 0,851283246  | 0,843406593  | 0,843979344  | 0,470274186         | 0,156593407        | 0,364338235 | 0,848835784  | 143 | 17 | 40 | 164 | 80,47        |\n",
        "| Árbol de Decisión                            | 1        | Test  | 0,851648352 | 0,851536497  | 0,851648352  | 0,851172866  | 0,493086339         | 0,148351648        | 0,397794118 | 0,846752451  | 129 | 31 | 23 | 181 | 80,3         |\n",
        "| Árbol de Decisión                            | 2        | Test  | 0,843406593 | 0,851283246  | 0,843406593  | 0,843979344  | 0,470274186         | 0,156593407        | 0,364338235 | 0,848835784  | 143 | 17 | 40 | 164 | 80,47        |\n",
        "| Random Forest                                | 1        | Test  | 0,901098901 | 0,901355348  | 0,901098901  | 0,90078191   | 0,642435095         | 0,098901099        | 0,598529412 | 0,896936275  | 138 | 22 | 14 | 190 | 86,58        |\n",
        "| Random Forest                                | 2        | Test  | 0,950549451 | 0,950849151  | 0,950549451  | 0,95043533   | 0,811414197         | 0,049450549        | 0,799264706 | 0,947794118  | 148 | 12 | 6  | 198 | 93,12        |\n",
        "| KNN                                          | 1        | Test  | 0,928571429 | 0,9293277    | 0,928571429  | 0,928695698  | 0,733947196         | 0,071428571        | 0,71004902  | 0,929534314  | 150 | 10 | 16 | 188 | 90,65        |\n",
        "| KNN                                          | 2        | Test  | 0,953296703 | 0,9553011    | 0,953296703  | 0,953425717  | 0,821419074         | 0,046703297        | 0,810416667 | 0,956311275  | 157 | 3  | 14 | 190 | 94,11        |\n",
        "| SVM                                          | 1        | Test  | 0,804945055 | 0,811725726  | 0,804945055  | 0,805658957  | 0,370256238         | 0,195054945        | 0,208210784 | 0,809129902  | 135 | 25 | 46 | 158 | 75,67        |\n",
        "| SVM                                          | 2        | Test  | 0,840659341 | 0,846890428  | 0,840659341  | 0,841239113  | 0,462735934         | 0,159340659        | 0,353186275 | 0,845036765  | 141 | 19 | 39 | 165 | 80,01        |\n",
        "| Naive Bayes - Bernoulli                      | 1        | Test  | 0,804945055 | 0,817839758  | 0,804945055  | 0,805539092  | 0,370238093         | 0,195054945        | 0,208210784 | 0,8125       | 140 | 20 | 51 | 153 | 76,1         |\n",
        "| Naive Bayes - Bernoulli                      | 2        | Test  | 0,793956044 | 0,801798359  | 0,793956044  | 0,794709664  | 0,343859249         | 0,206043956        | 0,163602941 | 0,798651961  | 134 | 26 | 49 | 155 | 74,43        |\n",
        "| AdaBoost 2                                   | 1        | Test  | 0,923076923 | 0,923076923  | 0,923076923  | 0,922963481  | 0,715133877         | 0,076923077        | 0,687745098 | 0,920588235  | 144 | 16 | 12 | 192 | 89,57        |\n",
        "| AdaBoost 2                                   | 2        | Test  | 0,961538462 | 0,961538462  | 0,961538462  | 0,961538462  | 0,851642489         | 0,038461538        | 0,843872549 | 0,960968137  | 153 | 7  | 7  | 197 | 94,8         |\n",
        "| Gradient Boosting                            | 1        | Test  | 0,925824176 | 0,925787175  | 0,925824176  | 0,925798592  | 0,72450472          | 0,074175824        | 0,698897059 | 0,924387255  | 146 | 14 | 13 | 191 | 90,04        |\n",
        "| Gradient Boosting                            | 2        | Test  | 0,96978022  | 0,969774236  | 0,96978022   | 0,969769797  | 0,882432336         | 0,03021978         | 0,877328431 | 0,968995098  | 154 | 6  | 5  | 199 | 95,87        |\n",
        "| SVR                                          | 1        | Test  | 0,782967033 | 0,798556473  | 0,782967033  | 0,783483329  | 0,318389716         | 0,217032967        | 0,118995098 | 0,791544118  | 138 | 22 | 57 | 147 | 73,63        |\n",
        "| SVR                                          | 2        | Test  | 0,832417582 | 0,841474847  | 0,832417582  | 0,833020009  | 0,440485147         | 0,167582418        | 0,319730392 | 0,838357843  | 142 | 18 | 43 | 161 | 79,2         |\n",
        "| Regresión polinomial                         | 1        | Test  | 0,785714286 | 0,79107337   | 0,785714286  | 0,786475063  | 0,324693566         | 0,214285714        | 0,130147059 | 0,788602941  | 130 | 30 | 48 | 156 | 73,23        |\n",
        "| Regresión polinomial                         | 2        | Test  | 0,835164835 | 0,841402559  | 0,835164835  | 0,835764599  | 0,447842204         | 0,164835165        | 0,330882353 | 0,839460784  | 140 | 20 | 40 | 164 | 79,33        |\n",
        "| Perceptrón Multicapa (MLP)                   | 1        | Test  | 0,799450549 | 0,799126195  | 0,799450549  | 0,79923351   | 0,356792106         | 0,200549451        | 0,185906863 | 0,795465686  | 122 | 38 | 35 | 169 | 74,06        |\n",
        "| Perceptrón Multicapa (MLP)                   | 2        | Test  | 0,848901099 | 0,85199339   | 0,848901099  | 0,849364039  | 0,485516338         | 0,151098901        | 0,386642157 | 0,851041667  | 139 | 21 | 34 | 170 | 80,75        |\n",
        "| Voting Classifier (KNN + Gradient Boosting + AdaBoost) | 1        | Test  | 0,942307692 | 0,942313018  | 0,942307692  | 0,942245256  | 0,781903382         | 0,057692308        | 0,765808824 | 0,940441176  | 148 | 12 | 9  | 195 | 92,13        |\n",
        "| Voting Classifier (KNN + Gradient Boosting + AdaBoost) | 2        | Test  | 0,975274725 | 0,975272618  | 0,975274725  | 0,975266197  | 0,903263279         | 0,024725275        | 0,899632353 | 0,974571078  | 155 | 5  | 4  | 200 | 96,61        |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Aprendizaje no supervisado"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering K-means"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "KMeans\n",
            "Número de clusteres optimo : 7\n",
            "Metrics for Training set (KMeans):\n",
            " - Silhouette Score: 0.4276\n",
            " - Davies-Bouldin Index: 1.0653\n",
            " - Calinski-Harabasz Index: 215.2480\n",
            " - Global Score: 94.4536\n",
            "\n",
            "Metrics for Validation set (KMeans):\n",
            " - Silhouette Score: 0.5128\n",
            " - Davies-Bouldin Index: 1.1492\n",
            " - Calinski-Harabasz Index: 72.7247\n",
            " - Global Score: 46.9693\n",
            "\n",
            "Metrics for Test set (KMeans):\n",
            " - Silhouette Score: 0.4927\n",
            " - Davies-Bouldin Index: 0.9756\n",
            " - Calinski-Harabasz Index: 140.3259\n",
            " - Global Score: 72.0595\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Reducción de Dimensionalidad SVD</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.644443</td>\n",
              "      <td>1.650717</td>\n",
              "      <td>216.015457</td>\n",
              "      <td>88.567253</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Reducción de Dimensionalidad SVD</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.672445</td>\n",
              "      <td>1.776153</td>\n",
              "      <td>66.466865</td>\n",
              "      <td>37.093816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Reducción de Dimensionalidad SVD</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.679417</td>\n",
              "      <td>1.382803</td>\n",
              "      <td>116.388579</td>\n",
              "      <td>60.406426</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>94.453611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>46.969295</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.059460</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                       Model         Set  Silhouette Score  \\\n",
              "0           Reducción de Dimensionalidad SVD    Training          0.644443   \n",
              "1           Reducción de Dimensionalidad SVD  Validation          0.672445   \n",
              "2           Reducción de Dimensionalidad SVD        Test          0.679417   \n",
              "3  Detección de Anomalías (Isolation Forest)    Training          0.657663   \n",
              "4  Detección de Anomalías (Isolation Forest)  Validation          0.633634   \n",
              "5  Detección de Anomalías (Isolation Forest)        Test          0.665247   \n",
              "6                                     KMeans    Training          0.427582   \n",
              "7                                     KMeans  Validation          0.512824   \n",
              "8                                     KMeans        Test          0.492667   \n",
              "\n",
              "   Davies-Bouldin Index  Calinski-Harabasz Index  Global Score  \n",
              "0              1.650717               216.015457     88.567253  \n",
              "1              1.776153                66.466865     37.093816  \n",
              "2              1.382803               116.388579     60.406426  \n",
              "3              1.963624               177.373949     70.691969  \n",
              "4              2.064200                56.060025     28.177234  \n",
              "5              1.692517                88.044333     45.560276  \n",
              "6              1.065326               215.248011     94.453611  \n",
              "7              1.149159                72.724670     46.969295  \n",
              "8              0.975618               140.325935     72.059460  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_name = 'KMeans'\n",
        "print(model_name)\n",
        "model_KM = KMeans(random_state=42)\n",
        "\n",
        "# Encontrar el número óptimo de clústeres\n",
        "num_clusters_optimo = encontrar_numero_optimo_clusters(X_train_prep, X_val_prep, model_KM, plot_grafica = 'NO')\n",
        "print(\"Número de clusteres optimo : \" + str(num_clusters_optimo))\n",
        "\n",
        "# Entrenar el modelo final con el número óptimo de clústeres usando los datos de entrenamiento\n",
        "kmeans_final = KMeans(n_clusters=num_clusters_optimo, random_state=0)\n",
        "kmeans_final.fit(X_train_prep) # Entrenar con np.vstack((X_train_prep, X_val_prep))? se recomienda llevar a cabo esta práctica\n",
        "\n",
        "labels_train = model_KM.predict(X_train_prep)\n",
        "labels_val = kmeans_final.predict(X_val_prep)\n",
        "labels_test = kmeans_final.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, 'Training',  model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val,'Validation', model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test,'Test', model_name)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mean Shift"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mean Shift\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danie\\.virtualenvs\\Repositorio_compartido-Xxik1eBC\\Lib\\site-packages\\sklearn\\cluster\\_mean_shift.py:293: UserWarning: Binning data failed with provided bin_size=0.100000, using data points as seeds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bandwidth óptimo: 2.0\n",
            "Mejor Silhouette Score en Validación: 0.49431989853400954\n",
            "Número de clústeres estimados en Validación: 24\n",
            "Metrics for Training set (Mean Shift):\n",
            " - Silhouette Score: 0.4582\n",
            " - Davies-Bouldin Index: 0.5735\n",
            " - Calinski-Harabasz Index: 114.9748\n",
            " - Global Score: 69.7375\n",
            "\n",
            "Metrics for Validation set (Mean Shift):\n",
            " - Silhouette Score: 0.4821\n",
            " - Davies-Bouldin Index: 0.6987\n",
            " - Calinski-Harabasz Index: 74.6950\n",
            " - Global Score: 54.6217\n",
            "\n",
            "Metrics for Test set (Mean Shift):\n",
            " - Silhouette Score: 0.4413\n",
            " - Davies-Bouldin Index: 0.8014\n",
            " - Calinski-Harabasz Index: 68.3151\n",
            " - Global Score: 50.1032\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>76.846230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Model            Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0       KMeans       Training          0.589238              1.635342   \n",
              "1       KMeans     Validation          0.617695              1.765477   \n",
              "2       KMeans           Test          0.653144              1.411056   \n",
              "3       KMeans  Entrenamiento          0.427582              1.065326   \n",
              "4       KMeans     Evaluación          0.512824              1.149159   \n",
              "5       KMeans           Test          0.492667              0.975618   \n",
              "6   Mean Shift       Training          0.458234              0.573478   \n",
              "7       KMeans       Training          0.589238              1.635342   \n",
              "8       KMeans     Validation          0.617695              1.765477   \n",
              "9       KMeans           Test          0.653144              1.411056   \n",
              "10  Mean Shift       Training          0.458234              0.573478   \n",
              "11  Mean Shift     Validation          0.482059              0.698659   \n",
              "12  Mean Shift           Test          0.441301              0.801412   \n",
              "\n",
              "    Calinski-Harabasz Index  Global Score  \n",
              "0                215.772165     87.822329  \n",
              "1                 65.453656     36.021524  \n",
              "2                112.564337     58.222912  \n",
              "3                215.248011     93.364851  \n",
              "4                 72.724670     44.483306  \n",
              "5                140.325935     72.465829  \n",
              "6                114.974797     76.846230  \n",
              "7                215.772165     87.822329  \n",
              "8                 65.453656     36.021524  \n",
              "9                112.564337     58.222912  \n",
              "10               114.974797     69.737530  \n",
              "11                74.694969     54.621658  \n",
              "12                68.315078     50.103171  "
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.cluster import MeanShift\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "model_name = 'Mean Shift'\n",
        "print(model_name)\n",
        "\n",
        "# Función para realizar la búsqueda de bandwidth óptimo\n",
        "def find_optimal_bandwidth(X, bandwidths):\n",
        "    best_bandwidth = None\n",
        "    best_score = -1\n",
        "    best_labels = None\n",
        "    best_cluster_centers = None\n",
        "    \n",
        "    for bandwidth in bandwidths:\n",
        "        ms = MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
        "        ms.fit(X)\n",
        "        labels = ms.labels_\n",
        "        if len(np.unique(labels)) > 1:\n",
        "            score = silhouette_score(X, labels)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_bandwidth = bandwidth\n",
        "                best_labels = labels\n",
        "                best_cluster_centers = ms.cluster_centers_\n",
        "    \n",
        "    return best_bandwidth, best_labels, best_cluster_centers, best_score\n",
        "\n",
        "# Definir el rango de valores de bandwidth para la búsqueda\n",
        "bandwidths = np.linspace(0.1, 2.0, 20)\n",
        "\n",
        "# Buscar el bandwidth óptimo usando el conjunto de validación\n",
        "optimal_bandwidth, labels_val, cluster_centers, best_score = find_optimal_bandwidth(X_val_prep, bandwidths)\n",
        "\n",
        "print(\"Bandwidth óptimo:\", optimal_bandwidth)\n",
        "print(\"Mejor Silhouette Score en Validación:\", best_score)\n",
        "print(\"Número de clústeres estimados en Validación:\", len(np.unique(labels_val)))\n",
        "\n",
        "def map_clusters_to_labels(labels, true_labels):\n",
        "    from scipy.stats import mode\n",
        "    # Asegurarse de que labels y true_labels sean arreglos\n",
        "    labels = np.asarray(labels)\n",
        "    true_labels = np.asarray(true_labels)\n",
        "    \n",
        "    # Crear una matriz de confusión\n",
        "    conf_matrix = confusion_matrix(true_labels, labels)\n",
        "    \n",
        "    # Crear un arreglo para mapear los clusters a las etiquetas reales\n",
        "    cluster_label_map = np.zeros_like(labels)\n",
        "    \n",
        "    # Asignar la etiqueta de clase más frecuente a cada cluster\n",
        "    for i in range(conf_matrix.shape[1]):\n",
        "        mask = labels == i\n",
        "        if np.any(mask):  # Asegurarse de que el cluster tenga etiquetas\n",
        "            most_common_label = mode(true_labels[mask])[0][0]\n",
        "            cluster_label_map[mask] = most_common_label\n",
        "    \n",
        "    return cluster_label_map\n",
        "\n",
        "# Entrenar el modelo final en el conjunto de entrenamiento usando el bandwidth óptimo\n",
        "ms = MeanShift(bandwidth=optimal_bandwidth, bin_seeding=True)\n",
        "ms.fit(X_train_prep)\n",
        "labels_train = ms.labels_\n",
        "\n",
        "labels_train = ms.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "\n",
        "labels_val = ms.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "\n",
        "labels_test = ms.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering DBSCAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DBSCAN\n",
            "Epsilon óptimo: 4.8\n",
            "Mejor Silhouette Score en Entrenamiento: 0.8349455307521673\n",
            "Número de clústeres estimados en Entrenamiento: 2\n",
            "Metrics for Training set (DBSCAN):\n",
            " - Silhouette Score: 0.8349\n",
            " - Davies-Bouldin Index: 1.7217\n",
            " - Calinski-Harabasz Index: 170.5236\n",
            " - Global Score: 75.3953\n",
            "\n",
            "Metrics for Validation set (DBSCAN):\n",
            " - Silhouette Score: 0.8204\n",
            " - Davies-Bouldin Index: 1.6787\n",
            " - Calinski-Harabasz Index: 52.0399\n",
            " - Global Score: 36.3747\n",
            "\n",
            "Metrics for Test set (DBSCAN):\n",
            " - Silhouette Score: 0.8233\n",
            " - Davies-Bouldin Index: 1.1945\n",
            " - Calinski-Harabasz Index: 88.5002\n",
            " - Global Score: 56.6458\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.229602</td>\n",
              "      <td>1.740380</td>\n",
              "      <td>115.132563</td>\n",
              "      <td>46.531218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.210897</td>\n",
              "      <td>1.816310</td>\n",
              "      <td>36.730034</td>\n",
              "      <td>18.819791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.263973</td>\n",
              "      <td>1.729851</td>\n",
              "      <td>47.569378</td>\n",
              "      <td>24.758483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.229602</td>\n",
              "      <td>1.740380</td>\n",
              "      <td>115.132563</td>\n",
              "      <td>46.531218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.210897</td>\n",
              "      <td>1.816310</td>\n",
              "      <td>36.730034</td>\n",
              "      <td>18.819791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.263973</td>\n",
              "      <td>1.729851</td>\n",
              "      <td>47.569378</td>\n",
              "      <td>24.758483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.229602</td>\n",
              "      <td>1.740380</td>\n",
              "      <td>115.132563</td>\n",
              "      <td>46.531218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.210897</td>\n",
              "      <td>1.816310</td>\n",
              "      <td>36.730034</td>\n",
              "      <td>18.819791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.263973</td>\n",
              "      <td>1.729851</td>\n",
              "      <td>47.569378</td>\n",
              "      <td>24.758483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.229602</td>\n",
              "      <td>1.740380</td>\n",
              "      <td>115.132563</td>\n",
              "      <td>46.531218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.210897</td>\n",
              "      <td>1.816310</td>\n",
              "      <td>36.730034</td>\n",
              "      <td>18.819791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.263973</td>\n",
              "      <td>1.729851</td>\n",
              "      <td>47.569378</td>\n",
              "      <td>24.758483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Model         Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0   Random Forest    Training          0.229602              1.740380   \n",
              "1   Random Forest  Validation          0.210897              1.816310   \n",
              "2   Random Forest        Test          0.263973              1.729851   \n",
              "3          DBSCAN    Training          0.834946              1.721699   \n",
              "4          DBSCAN  Validation          0.820430              1.678743   \n",
              "5          DBSCAN        Test          0.823251              1.194508   \n",
              "6             PCA    Training          0.229602              1.740380   \n",
              "7             PCA  Validation          0.210897              1.816310   \n",
              "8             PCA        Test          0.263973              1.729851   \n",
              "9             PCA    Training          0.229602              1.740380   \n",
              "10            PCA  Validation          0.210897              1.816310   \n",
              "11            PCA        Test          0.263973              1.729851   \n",
              "12            PCA    Training          0.229602              1.740380   \n",
              "13            PCA  Validation          0.210897              1.816310   \n",
              "14            PCA        Test          0.263973              1.729851   \n",
              "15         DBSCAN    Training          0.834946              1.721699   \n",
              "16         DBSCAN  Validation          0.820430              1.678743   \n",
              "17         DBSCAN        Test          0.823251              1.194508   \n",
              "\n",
              "    Calinski-Harabasz Index  Global Score  \n",
              "0                115.132563     46.531218  \n",
              "1                 36.730034     18.819791  \n",
              "2                 47.569378     24.758483  \n",
              "3                170.523605     75.395314  \n",
              "4                 52.039884     36.374742  \n",
              "5                 88.500184     56.645778  \n",
              "6                115.132563     46.531218  \n",
              "7                 36.730034     18.819791  \n",
              "8                 47.569378     24.758483  \n",
              "9                115.132563     46.531218  \n",
              "10                36.730034     18.819791  \n",
              "11                47.569378     24.758483  \n",
              "12               115.132563     46.531218  \n",
              "13                36.730034     18.819791  \n",
              "14                47.569378     24.758483  \n",
              "15               170.523605     75.395314  \n",
              "16                52.039884     36.374742  \n",
              "17                88.500184     56.645778  "
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_name = 'DBSCAN'\n",
        "print(model_name)\n",
        "\n",
        "# Función para encontrar el mejor valor de eps\n",
        "def find_optimal_eps(X, eps_values, min_samples):\n",
        "    best_eps = None\n",
        "    best_score = -1\n",
        "    best_labels = None\n",
        "    \n",
        "    for eps in eps_values:\n",
        "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
        "        labels = dbscan.fit_predict(X)\n",
        "        \n",
        "        # Solo evaluar si hay más de un cluster (ignorar ruido)\n",
        "        if len(np.unique(labels)) > 1 and np.sum(labels != -1) > 1:\n",
        "            score = silhouette_score(X, labels)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_eps = eps\n",
        "                best_labels = labels\n",
        "                \n",
        "    return best_eps, best_labels, best_score\n",
        "\n",
        "# Definir el rango de valores de eps para la búsqueda\n",
        "eps_values = np.linspace(0.1, 5.0, 50)\n",
        "min_samples = 5  # Puedes ajustar este valor según sea necesario\n",
        "\n",
        "# Buscar el eps óptimo usando el conjunto de entrenamiento\n",
        "optimal_eps, labels_train, best_score = find_optimal_eps(X_train_prep, eps_values, min_samples)\n",
        "\n",
        "print(\"Epsilon óptimo:\", optimal_eps)\n",
        "print(\"Mejor Silhouette Score en Entrenamiento:\", best_score)\n",
        "print(\"Número de clústeres estimados en Entrenamiento:\", len(np.unique(labels_train)))\n",
        "\n",
        "# Entrenar el modelo final en el conjunto de entrenamiento usando el eps óptimo\n",
        "dbscan_final = DBSCAN(eps=optimal_eps, min_samples=min_samples)\n",
        "\n",
        "labels_train = dbscan_final.fit_predict(X_train_prep)\n",
        "labels_val = dbscan_final.fit_predict(X_val_prep)\n",
        "labels_test = dbscan_final.fit_predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering GMM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mejores parámetros:  {'gmm__covariance_type': 'spherical', 'gmm__n_components': 2}\n",
            "Metrics for Training set (Mean Shift):\n",
            " - Silhouette Score: 0.4229\n",
            " - Davies-Bouldin Index: 1.7908\n",
            " - Calinski-Harabasz Index: 157.8364\n",
            " - Global Score: 63.1478\n",
            "\n",
            "Metrics for Validation set (Mean Shift):\n",
            " - Silhouette Score: 0.4492\n",
            " - Davies-Bouldin Index: 1.8741\n",
            " - Calinski-Harabasz Index: 54.3106\n",
            " - Global Score: 27.6885\n",
            "\n",
            "Metrics for Test set (Mean Shift):\n",
            " - Silhouette Score: 0.4781\n",
            " - Davies-Bouldin Index: 1.7677\n",
            " - Calinski-Harabasz Index: 66.4847\n",
            " - Global Score: 34.0007\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Entrenamiento</td>\n",
              "      <td>0.427582</td>\n",
              "      <td>1.065326</td>\n",
              "      <td>215.248011</td>\n",
              "      <td>93.364851</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Evaluación</td>\n",
              "      <td>0.512824</td>\n",
              "      <td>1.149159</td>\n",
              "      <td>72.724670</td>\n",
              "      <td>44.483306</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.492667</td>\n",
              "      <td>0.975618</td>\n",
              "      <td>140.325935</td>\n",
              "      <td>72.465829</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>76.846230</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.589238</td>\n",
              "      <td>1.635342</td>\n",
              "      <td>215.772165</td>\n",
              "      <td>87.822329</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.617695</td>\n",
              "      <td>1.765477</td>\n",
              "      <td>65.453656</td>\n",
              "      <td>36.021524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>KMeans</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.653144</td>\n",
              "      <td>1.411056</td>\n",
              "      <td>112.564337</td>\n",
              "      <td>58.222912</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.458234</td>\n",
              "      <td>0.573478</td>\n",
              "      <td>114.974797</td>\n",
              "      <td>69.737530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.482059</td>\n",
              "      <td>0.698659</td>\n",
              "      <td>74.694969</td>\n",
              "      <td>54.621658</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.441301</td>\n",
              "      <td>0.801412</td>\n",
              "      <td>68.315078</td>\n",
              "      <td>50.103171</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.422905</td>\n",
              "      <td>1.790767</td>\n",
              "      <td>157.836376</td>\n",
              "      <td>63.147762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.449208</td>\n",
              "      <td>1.874110</td>\n",
              "      <td>54.310631</td>\n",
              "      <td>27.688509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.478050</td>\n",
              "      <td>1.767702</td>\n",
              "      <td>66.484695</td>\n",
              "      <td>34.000696</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.422905</td>\n",
              "      <td>1.790767</td>\n",
              "      <td>157.836376</td>\n",
              "      <td>63.147762</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.449208</td>\n",
              "      <td>1.874110</td>\n",
              "      <td>54.310631</td>\n",
              "      <td>27.688509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Mean Shift</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.478050</td>\n",
              "      <td>1.767702</td>\n",
              "      <td>66.484695</td>\n",
              "      <td>34.000696</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         Model            Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0       KMeans       Training          0.589238              1.635342   \n",
              "1       KMeans     Validation          0.617695              1.765477   \n",
              "2       KMeans           Test          0.653144              1.411056   \n",
              "3       KMeans  Entrenamiento          0.427582              1.065326   \n",
              "4       KMeans     Evaluación          0.512824              1.149159   \n",
              "5       KMeans           Test          0.492667              0.975618   \n",
              "6   Mean Shift       Training          0.458234              0.573478   \n",
              "7       KMeans       Training          0.589238              1.635342   \n",
              "8       KMeans     Validation          0.617695              1.765477   \n",
              "9       KMeans           Test          0.653144              1.411056   \n",
              "10  Mean Shift       Training          0.458234              0.573478   \n",
              "11  Mean Shift     Validation          0.482059              0.698659   \n",
              "12  Mean Shift           Test          0.441301              0.801412   \n",
              "13  Mean Shift       Training          0.422905              1.790767   \n",
              "14  Mean Shift     Validation          0.449208              1.874110   \n",
              "15  Mean Shift           Test          0.478050              1.767702   \n",
              "16  Mean Shift       Training          0.422905              1.790767   \n",
              "17  Mean Shift     Validation          0.449208              1.874110   \n",
              "18  Mean Shift           Test          0.478050              1.767702   \n",
              "\n",
              "    Calinski-Harabasz Index  Global Score  \n",
              "0                215.772165     87.822329  \n",
              "1                 65.453656     36.021524  \n",
              "2                112.564337     58.222912  \n",
              "3                215.248011     93.364851  \n",
              "4                 72.724670     44.483306  \n",
              "5                140.325935     72.465829  \n",
              "6                114.974797     76.846230  \n",
              "7                215.772165     87.822329  \n",
              "8                 65.453656     36.021524  \n",
              "9                112.564337     58.222912  \n",
              "10               114.974797     69.737530  \n",
              "11                74.694969     54.621658  \n",
              "12                68.315078     50.103171  \n",
              "13               157.836376     63.147762  \n",
              "14                54.310631     27.688509  \n",
              "15                66.484695     34.000696  \n",
              "16               157.836376     63.147762  \n",
              "17                54.310631     27.688509  \n",
              "18                66.484695     34.000696  "
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.mixture import GaussianMixture\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Define un pipeline que combina un modelo de mezcla gaussiana\n",
        "pipeline = Pipeline([\n",
        "    ('gmm', GaussianMixture(random_state=42))\n",
        "])\n",
        "\n",
        "# Define la cuadrícula de parámetros para buscar\n",
        "param_grid = {\n",
        "    'gmm__n_components': [1, 2, 3, 4, 5],\n",
        "    'gmm__covariance_type': ['full', 'tied', 'diag', 'spherical']\n",
        "}\n",
        "\n",
        "# Crea un objeto GridSearchCV\n",
        "model_GMM = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1_macro')\n",
        "\n",
        "# Ajusta el objeto GridSearchCV a los datos\n",
        "model_GMM.fit(X_train_prep, y_train)\n",
        "\n",
        "# Imprime los mejores parámetros\n",
        "print(\"Mejores parámetros: \", model_GMM.best_params_)\n",
        "\n",
        "\n",
        "labels_train = model_GMM.predict(X_train_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "\n",
        "labels_val = model_GMM.predict(X_val_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "\n",
        "labels_test = model_GMM.predict(X_test_prep)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis de componentes principales (PCA) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Número óptimo de componentes para explicar al menos el 95% de la variancia es de 9 que coincide con el número actual de variables es de 9. \n",
            "Por tanto no se aplica el algoritmo PCA\n"
          ]
        }
      ],
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Crear y ajustar el modelo PCA para calcular la variancia explicada\n",
        "pca_temp = PCA()\n",
        "pca_temp.fit(X_train_prep)\n",
        "\n",
        "# Calcular la variancia explicada acumulada\n",
        "explained_variance = np.cumsum(pca_temp.explained_variance_ratio_)\n",
        "\n",
        "# Determinar el número de componentes que explican al menos el 95% de la variancia\n",
        "n_components = np.argmax(explained_variance >= 0.95) + 1\n",
        "\n",
        "if n_components == X_train_prep.shape[1] :\n",
        "    print(f\"Número óptimo de componentes para explicar al menos el 95% de la variancia es de {n_components}\" \n",
        "      f\" que coincide con el número actual de variables es de {X_train_prep.shape[1]}. \\n\"\n",
        "       \"Por tanto no se aplica el algoritmo PCA\")\n",
        "    \n",
        "else :\n",
        "    print(f\"Número óptimo de componentes para explicar al menos el 95% de la variancia: {n_components} \")\n",
        "\n",
        "    # Crear y ajustar el modelo PCA con el número óptimo de componentes\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train_prep)\n",
        "    X_val_pca = pca.transform(X_val_prep)\n",
        "    X_test_pca = pca.transform(X_test_prep)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis de componentes independientes (ICA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Análisis de Componentes Independientes (ICA)\n",
            "Metrics for Training set (Análisis de Componentes Independientes (ICA)):\n",
            " - Silhouette Score: 0.6568\n",
            " - Davies-Bouldin Index: 2.0301\n",
            " - Calinski-Harabasz Index: 168.6725\n",
            " - Global Score: 66.6690\n",
            "\n",
            "Metrics for Validation set (Análisis de Componentes Independientes (ICA)):\n",
            " - Silhouette Score: 0.6507\n",
            " - Davies-Bouldin Index: 2.0672\n",
            " - Calinski-Harabasz Index: 56.1600\n",
            " - Global Score: 28.4458\n",
            "\n",
            "Metrics for Test set (Análisis de Componentes Independientes (ICA)):\n",
            " - Silhouette Score: 0.6903\n",
            " - Davies-Bouldin Index: 1.6556\n",
            " - Calinski-Harabasz Index: 93.0848\n",
            " - Global Score: 48.2728\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Clustering Jerárquico</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.836252</td>\n",
              "      <td>1.348159</td>\n",
              "      <td>197.325390</td>\n",
              "      <td>90.576674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clustering Jerárquico</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.888725</td>\n",
              "      <td>0.074304</td>\n",
              "      <td>82.044252</td>\n",
              "      <td>74.255095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Clustering Jerárquico</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.620584</td>\n",
              "      <td>1.498707</td>\n",
              "      <td>100.826994</td>\n",
              "      <td>52.306950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.743961</td>\n",
              "      <td>2.077912</td>\n",
              "      <td>157.092062</td>\n",
              "      <td>63.464839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.682671</td>\n",
              "      <td>2.087814</td>\n",
              "      <td>53.477549</td>\n",
              "      <td>27.740134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.736900</td>\n",
              "      <td>1.630349</td>\n",
              "      <td>91.849113</td>\n",
              "      <td>49.058886</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.657663</td>\n",
              "      <td>1.963624</td>\n",
              "      <td>177.373949</td>\n",
              "      <td>70.691969</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.633634</td>\n",
              "      <td>2.064200</td>\n",
              "      <td>56.060025</td>\n",
              "      <td>28.177234</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>Detección de Anomalías (Isolation Forest)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.665247</td>\n",
              "      <td>1.692517</td>\n",
              "      <td>88.044333</td>\n",
              "      <td>45.560276</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.656759</td>\n",
              "      <td>2.030070</td>\n",
              "      <td>168.672498</td>\n",
              "      <td>66.668980</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.650697</td>\n",
              "      <td>2.067150</td>\n",
              "      <td>56.159985</td>\n",
              "      <td>28.445776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>Análisis de Componentes Independientes (ICA)</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.690275</td>\n",
              "      <td>1.655601</td>\n",
              "      <td>93.084814</td>\n",
              "      <td>48.272849</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                           Model         Set  \\\n",
              "0                          Clustering Jerárquico    Training   \n",
              "1                          Clustering Jerárquico  Validation   \n",
              "2                          Clustering Jerárquico        Test   \n",
              "3                                 Algoritmo AnDE    Training   \n",
              "4                                 Algoritmo AnDE  Validation   \n",
              "5                                 Algoritmo AnDE        Test   \n",
              "6      Detección de Anomalías (Isolation Forest)    Training   \n",
              "7      Detección de Anomalías (Isolation Forest)  Validation   \n",
              "8      Detección de Anomalías (Isolation Forest)        Test   \n",
              "9      Detección de Anomalías (Isolation Forest)    Training   \n",
              "10     Detección de Anomalías (Isolation Forest)  Validation   \n",
              "11     Detección de Anomalías (Isolation Forest)        Test   \n",
              "12     Detección de Anomalías (Isolation Forest)    Training   \n",
              "13     Detección de Anomalías (Isolation Forest)  Validation   \n",
              "14     Detección de Anomalías (Isolation Forest)        Test   \n",
              "15     Detección de Anomalías (Isolation Forest)    Training   \n",
              "16     Detección de Anomalías (Isolation Forest)  Validation   \n",
              "17     Detección de Anomalías (Isolation Forest)        Test   \n",
              "18     Detección de Anomalías (Isolation Forest)    Training   \n",
              "19     Detección de Anomalías (Isolation Forest)  Validation   \n",
              "20     Detección de Anomalías (Isolation Forest)        Test   \n",
              "21     Detección de Anomalías (Isolation Forest)    Training   \n",
              "22     Detección de Anomalías (Isolation Forest)  Validation   \n",
              "23     Detección de Anomalías (Isolation Forest)        Test   \n",
              "24     Detección de Anomalías (Isolation Forest)    Training   \n",
              "25     Detección de Anomalías (Isolation Forest)  Validation   \n",
              "26     Detección de Anomalías (Isolation Forest)        Test   \n",
              "27     Detección de Anomalías (Isolation Forest)    Training   \n",
              "28     Detección de Anomalías (Isolation Forest)  Validation   \n",
              "29     Detección de Anomalías (Isolation Forest)        Test   \n",
              "30     Detección de Anomalías (Isolation Forest)    Training   \n",
              "31     Detección de Anomalías (Isolation Forest)  Validation   \n",
              "32     Detección de Anomalías (Isolation Forest)        Test   \n",
              "33     Detección de Anomalías (Isolation Forest)    Training   \n",
              "34     Detección de Anomalías (Isolation Forest)  Validation   \n",
              "35     Detección de Anomalías (Isolation Forest)        Test   \n",
              "36  Análisis de Componentes Independientes (ICA)    Training   \n",
              "37  Análisis de Componentes Independientes (ICA)  Validation   \n",
              "38  Análisis de Componentes Independientes (ICA)        Test   \n",
              "\n",
              "    Silhouette Score  Davies-Bouldin Index  Calinski-Harabasz Index  \\\n",
              "0           0.836252              1.348159               197.325390   \n",
              "1           0.888725              0.074304                82.044252   \n",
              "2           0.620584              1.498707               100.826994   \n",
              "3           0.743961              2.077912               157.092062   \n",
              "4           0.682671              2.087814                53.477549   \n",
              "5           0.736900              1.630349                91.849113   \n",
              "6           0.657663              1.963624               177.373949   \n",
              "7           0.633634              2.064200                56.060025   \n",
              "8           0.665247              1.692517                88.044333   \n",
              "9           0.657663              1.963624               177.373949   \n",
              "10          0.633634              2.064200                56.060025   \n",
              "11          0.665247              1.692517                88.044333   \n",
              "12          0.657663              1.963624               177.373949   \n",
              "13          0.633634              2.064200                56.060025   \n",
              "14          0.665247              1.692517                88.044333   \n",
              "15          0.657663              1.963624               177.373949   \n",
              "16          0.633634              2.064200                56.060025   \n",
              "17          0.665247              1.692517                88.044333   \n",
              "18          0.657663              1.963624               177.373949   \n",
              "19          0.633634              2.064200                56.060025   \n",
              "20          0.665247              1.692517                88.044333   \n",
              "21          0.657663              1.963624               177.373949   \n",
              "22          0.633634              2.064200                56.060025   \n",
              "23          0.665247              1.692517                88.044333   \n",
              "24          0.657663              1.963624               177.373949   \n",
              "25          0.633634              2.064200                56.060025   \n",
              "26          0.665247              1.692517                88.044333   \n",
              "27          0.657663              1.963624               177.373949   \n",
              "28          0.633634              2.064200                56.060025   \n",
              "29          0.665247              1.692517                88.044333   \n",
              "30          0.657663              1.963624               177.373949   \n",
              "31          0.633634              2.064200                56.060025   \n",
              "32          0.665247              1.692517                88.044333   \n",
              "33          0.657663              1.963624               177.373949   \n",
              "34          0.633634              2.064200                56.060025   \n",
              "35          0.665247              1.692517                88.044333   \n",
              "36          0.656759              2.030070               168.672498   \n",
              "37          0.650697              2.067150                56.159985   \n",
              "38          0.690275              1.655601                93.084814   \n",
              "\n",
              "    Global Score  \n",
              "0      90.576674  \n",
              "1      74.255095  \n",
              "2      52.306950  \n",
              "3      63.464839  \n",
              "4      27.740134  \n",
              "5      49.058886  \n",
              "6      70.691969  \n",
              "7      28.177234  \n",
              "8      45.560276  \n",
              "9      70.691969  \n",
              "10     28.177234  \n",
              "11     45.560276  \n",
              "12     70.691969  \n",
              "13     28.177234  \n",
              "14     45.560276  \n",
              "15     70.691969  \n",
              "16     28.177234  \n",
              "17     45.560276  \n",
              "18     70.691969  \n",
              "19     28.177234  \n",
              "20     45.560276  \n",
              "21     70.691969  \n",
              "22     28.177234  \n",
              "23     45.560276  \n",
              "24     70.691969  \n",
              "25     28.177234  \n",
              "26     45.560276  \n",
              "27     70.691969  \n",
              "28     28.177234  \n",
              "29     45.560276  \n",
              "30     70.691969  \n",
              "31     28.177234  \n",
              "32     45.560276  \n",
              "33     70.691969  \n",
              "34     28.177234  \n",
              "35     45.560276  \n",
              "36     66.668980  \n",
              "37     28.445776  \n",
              "38     48.272849  "
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.decomposition import FastICA\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Crear y ajustar el modelo ICA\n",
        "model_name = 'Análisis de Componentes Independientes (ICA)'\n",
        "print(model_name)\n",
        "model_ICA = FastICA()\n",
        "model_ICA.fit(X_train_prep)\n",
        "\n",
        "# No se aplica el modelo ICA a los datos de entrenamiento, validación y prueba\n",
        "X_train_ica = model_ICA.transform(X_train_prep)\n",
        "X_val_ica = model_ICA.transform(X_val_prep)\n",
        "X_test_ica = model_ICA.transform(X_test_prep)\n",
        "\n",
        "# Crear y ajustar el modelo Isolation Forest\n",
        "isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "isolation_forest.fit(X_train_ica)\n",
        "\n",
        "# Predicción de anomalías en el conjunto de entrenamiento, validación y prueba\n",
        "anomalies_train = isolation_forest.predict(X_train_ica)\n",
        "anomalies_val = isolation_forest.predict(X_val_ica)\n",
        "anomalies_test = isolation_forest.predict(X_test_ica)\n",
        "\n",
        "# Convertir las predicciones a etiquetas binarias (0 para normal, 1 para anomalía)\n",
        "anomalies_train_labels = np.where(anomalies_train == -1, 1, 0)\n",
        "anomalies_val_labels = np.where(anomalies_val == -1, 1, 0)\n",
        "anomalies_test_labels = np.where(anomalies_test == -1, 1, 0)\n",
        "\n",
        "# Mostrar estadísticas de anomalías\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, anomalies_train_labels, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, anomalies_val_labels, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, anomalies_test_labels, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Análisis discriminante lineal (LDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "LDA\n",
            "Best parameters : {'n_components': None, 'solver': 'svd'}\n",
            "Metrics for Training set (LDA):\n",
            " - Silhouette Score: 0.3091\n",
            " - Davies-Bouldin Index: 1.7594\n",
            " - Calinski-Harabasz Index: 133.1994\n",
            " - Global Score: 53.5623\n",
            "\n",
            "Metrics for Validation set (LDA):\n",
            " - Silhouette Score: 0.2775\n",
            " - Davies-Bouldin Index: 1.8459\n",
            " - Calinski-Harabasz Index: 41.1149\n",
            " - Global Score: 20.8976\n",
            "\n",
            "Metrics for Test set (LDA):\n",
            " - Silhouette Score: 0.3448\n",
            " - Davies-Bouldin Index: 1.7564\n",
            " - Calinski-Harabasz Index: 53.9366\n",
            " - Global Score: 27.7848\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.229602</td>\n",
              "      <td>1.740380</td>\n",
              "      <td>115.132563</td>\n",
              "      <td>46.531218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.210897</td>\n",
              "      <td>1.816310</td>\n",
              "      <td>36.730034</td>\n",
              "      <td>18.819791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.263973</td>\n",
              "      <td>1.729851</td>\n",
              "      <td>47.569378</td>\n",
              "      <td>24.758483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.229602</td>\n",
              "      <td>1.740380</td>\n",
              "      <td>115.132563</td>\n",
              "      <td>46.531218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.210897</td>\n",
              "      <td>1.816310</td>\n",
              "      <td>36.730034</td>\n",
              "      <td>18.819791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.263973</td>\n",
              "      <td>1.729851</td>\n",
              "      <td>47.569378</td>\n",
              "      <td>24.758483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.229602</td>\n",
              "      <td>1.740380</td>\n",
              "      <td>115.132563</td>\n",
              "      <td>46.531218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.210897</td>\n",
              "      <td>1.816310</td>\n",
              "      <td>36.730034</td>\n",
              "      <td>18.819791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.263973</td>\n",
              "      <td>1.729851</td>\n",
              "      <td>47.569378</td>\n",
              "      <td>24.758483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.229602</td>\n",
              "      <td>1.740380</td>\n",
              "      <td>115.132563</td>\n",
              "      <td>46.531218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.210897</td>\n",
              "      <td>1.816310</td>\n",
              "      <td>36.730034</td>\n",
              "      <td>18.819791</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>PCA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.263973</td>\n",
              "      <td>1.729851</td>\n",
              "      <td>47.569378</td>\n",
              "      <td>24.758483</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.834946</td>\n",
              "      <td>1.721699</td>\n",
              "      <td>170.523605</td>\n",
              "      <td>75.395314</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.820430</td>\n",
              "      <td>1.678743</td>\n",
              "      <td>52.039884</td>\n",
              "      <td>36.374742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.823251</td>\n",
              "      <td>1.194508</td>\n",
              "      <td>88.500184</td>\n",
              "      <td>56.645778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.231252</td>\n",
              "      <td>1.753035</td>\n",
              "      <td>114.951883</td>\n",
              "      <td>46.287582</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.193475</td>\n",
              "      <td>1.844831</td>\n",
              "      <td>34.742839</td>\n",
              "      <td>17.391673</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>DBSCAN</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.229651</td>\n",
              "      <td>1.767221</td>\n",
              "      <td>43.197879</td>\n",
              "      <td>22.106459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.229651</td>\n",
              "      <td>1.767221</td>\n",
              "      <td>43.197879</td>\n",
              "      <td>22.106459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.309113</td>\n",
              "      <td>1.759360</td>\n",
              "      <td>133.199381</td>\n",
              "      <td>53.562338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.277450</td>\n",
              "      <td>1.845891</td>\n",
              "      <td>41.114890</td>\n",
              "      <td>20.897624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.344773</td>\n",
              "      <td>1.756416</td>\n",
              "      <td>53.936624</td>\n",
              "      <td>27.784813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.309113</td>\n",
              "      <td>1.759360</td>\n",
              "      <td>133.199381</td>\n",
              "      <td>53.562338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.277450</td>\n",
              "      <td>1.845891</td>\n",
              "      <td>41.114890</td>\n",
              "      <td>20.897624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.344773</td>\n",
              "      <td>1.756416</td>\n",
              "      <td>53.936624</td>\n",
              "      <td>27.784813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.309113</td>\n",
              "      <td>1.759360</td>\n",
              "      <td>133.199381</td>\n",
              "      <td>53.562338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.277450</td>\n",
              "      <td>1.845891</td>\n",
              "      <td>41.114890</td>\n",
              "      <td>20.897624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.344773</td>\n",
              "      <td>1.756416</td>\n",
              "      <td>53.936624</td>\n",
              "      <td>27.784813</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.309113</td>\n",
              "      <td>1.759360</td>\n",
              "      <td>133.199381</td>\n",
              "      <td>53.562338</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.277450</td>\n",
              "      <td>1.845891</td>\n",
              "      <td>41.114890</td>\n",
              "      <td>20.897624</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>LDA</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.344773</td>\n",
              "      <td>1.756416</td>\n",
              "      <td>53.936624</td>\n",
              "      <td>27.784813</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            Model         Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0   Random Forest    Training          0.229602              1.740380   \n",
              "1   Random Forest  Validation          0.210897              1.816310   \n",
              "2   Random Forest        Test          0.263973              1.729851   \n",
              "3          DBSCAN    Training          0.834946              1.721699   \n",
              "4          DBSCAN  Validation          0.820430              1.678743   \n",
              "5          DBSCAN        Test          0.823251              1.194508   \n",
              "6             PCA    Training          0.229602              1.740380   \n",
              "7             PCA  Validation          0.210897              1.816310   \n",
              "8             PCA        Test          0.263973              1.729851   \n",
              "9             PCA    Training          0.229602              1.740380   \n",
              "10            PCA  Validation          0.210897              1.816310   \n",
              "11            PCA        Test          0.263973              1.729851   \n",
              "12            PCA    Training          0.229602              1.740380   \n",
              "13            PCA  Validation          0.210897              1.816310   \n",
              "14            PCA        Test          0.263973              1.729851   \n",
              "15         DBSCAN    Training          0.834946              1.721699   \n",
              "16         DBSCAN  Validation          0.820430              1.678743   \n",
              "17         DBSCAN        Test          0.823251              1.194508   \n",
              "18         DBSCAN    Training          0.231252              1.753035   \n",
              "19         DBSCAN  Validation          0.193475              1.844831   \n",
              "20         DBSCAN        Test          0.229651              1.767221   \n",
              "21            LDA        Test          0.229651              1.767221   \n",
              "22            LDA    Training          0.309113              1.759360   \n",
              "23            LDA  Validation          0.277450              1.845891   \n",
              "24            LDA        Test          0.344773              1.756416   \n",
              "25            LDA    Training          0.309113              1.759360   \n",
              "26            LDA  Validation          0.277450              1.845891   \n",
              "27            LDA        Test          0.344773              1.756416   \n",
              "28            LDA    Training          0.309113              1.759360   \n",
              "29            LDA  Validation          0.277450              1.845891   \n",
              "30            LDA        Test          0.344773              1.756416   \n",
              "31            LDA    Training          0.309113              1.759360   \n",
              "32            LDA  Validation          0.277450              1.845891   \n",
              "33            LDA        Test          0.344773              1.756416   \n",
              "\n",
              "    Calinski-Harabasz Index  Global Score  \n",
              "0                115.132563     46.531218  \n",
              "1                 36.730034     18.819791  \n",
              "2                 47.569378     24.758483  \n",
              "3                170.523605     75.395314  \n",
              "4                 52.039884     36.374742  \n",
              "5                 88.500184     56.645778  \n",
              "6                115.132563     46.531218  \n",
              "7                 36.730034     18.819791  \n",
              "8                 47.569378     24.758483  \n",
              "9                115.132563     46.531218  \n",
              "10                36.730034     18.819791  \n",
              "11                47.569378     24.758483  \n",
              "12               115.132563     46.531218  \n",
              "13                36.730034     18.819791  \n",
              "14                47.569378     24.758483  \n",
              "15               170.523605     75.395314  \n",
              "16                52.039884     36.374742  \n",
              "17                88.500184     56.645778  \n",
              "18               114.951883     46.287582  \n",
              "19                34.742839     17.391673  \n",
              "20                43.197879     22.106459  \n",
              "21                43.197879     22.106459  \n",
              "22               133.199381     53.562338  \n",
              "23                41.114890     20.897624  \n",
              "24                53.936624     27.784813  \n",
              "25               133.199381     53.562338  \n",
              "26                41.114890     20.897624  \n",
              "27                53.936624     27.784813  \n",
              "28               133.199381     53.562338  \n",
              "29                41.114890     20.897624  \n",
              "30                53.936624     27.784813  \n",
              "31               133.199381     53.562338  \n",
              "32                41.114890     20.897624  \n",
              "33                53.936624     27.784813  "
            ]
          },
          "execution_count": 47,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "model_name = 'LDA'\n",
        "print(model_name)\n",
        "\n",
        "lda = LinearDiscriminantAnalysis()\n",
        "\n",
        "n_features = X_train_prep.shape[1]\n",
        "n_classes = len(np.unique(y_train))\n",
        "max_components = min(n_features, n_classes - 1)\n",
        "\n",
        "# Definir los hiperparámetros a buscar\n",
        "param_grid = {\n",
        "    'solver': ['svd', 'lsqr', 'eigen'],\n",
        "    'n_components': [None] + list(range(1, max_components + 1))\n",
        "}\n",
        "\n",
        "# Definir GridSearchCV\n",
        "grid_search = GridSearchCV(lda, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Entrenar el modelo con los datos de entrenamiento y encontrar los mejores hiperparámetros\n",
        "grid_search.fit(X_train_prep, y_train)\n",
        "\n",
        "# Obtener el mejor modelo y sus hiperparámetros\n",
        "model_LDA = grid_search.best_estimator_\n",
        "best_params = grid_search.best_params_\n",
        "print(f\"Best parameters : {best_params}\")\n",
        "\n",
        "model_LDA.fit(X_train_prep, y_train)\n",
        "\n",
        "labels_train = model_LDA.predict(X_train_prep)\n",
        "labels_val = model_LDA.predict(X_val_prep)\n",
        "labels_test = model_LDA.predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Clustering Jerárquico"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model_name = 'Clustering Jerárquico'\n",
        "print(model_name)\n",
        "\n",
        "# Función para encontrar el mejor número de clústeres\n",
        "def find_optimal_clusters(X, max_clusters):\n",
        "    best_num_clusters = None\n",
        "    best_score = -1\n",
        "    best_labels = None\n",
        "\n",
        "    for num_clusters in range(2, max_clusters + 1):\n",
        "        agg_clustering = AgglomerativeClustering(n_clusters=num_clusters)\n",
        "        labels = agg_clustering.fit_predict(X)\n",
        "\n",
        "        score = silhouette_score(X, labels)\n",
        "        if score > best_score:\n",
        "            best_score = score\n",
        "            best_num_clusters = num_clusters\n",
        "            best_labels = labels\n",
        "\n",
        "    return best_num_clusters, best_labels, best_score\n",
        "\n",
        "# Definir el número máximo de clústeres para la búsqueda\n",
        "max_clusters = 10\n",
        "\n",
        "# Buscar el número óptimo de clústeres usando el conjunto de entrenamiento\n",
        "optimal_num_clusters, labels_train, best_score = find_optimal_clusters(X_train_prep, max_clusters)\n",
        "\n",
        "print(\"Número óptimo de clústeres:\", optimal_num_clusters)\n",
        "print(\"Mejor Silhouette Score en Entrenamiento:\", best_score)\n",
        "\n",
        "# Entrenar el modelo final en el conjunto de entrenamiento usando el número óptimo de clústeres\n",
        "agg_clustering_final = AgglomerativeClustering(n_clusters=optimal_num_clusters)\n",
        "\n",
        "labels_train = agg_clustering_final.fit_predict(X_train_prep)\n",
        "labels_val = agg_clustering_final.fit_predict(X_val_prep)\n",
        "labels_test = agg_clustering_final.fit_predict(X_test_prep)\n",
        "\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, labels_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, labels_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, labels_test, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Algoritmo AnDE (Adaptive Nearest Neighbors Density Estimation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Algoritmo AnDE\n",
            "Umbral óptimo basado en la densidad: 0.34953294503378424\n",
            "Silhouette Score en el conjunto de entrenamiento: 0.7439612751877717\n",
            "Metrics for Training set (Algoritmo AnDE):\n",
            " - Silhouette Score: 0.7440\n",
            " - Davies-Bouldin Index: 2.0779\n",
            " - Calinski-Harabasz Index: 157.0921\n",
            " - Global Score: 63.4648\n",
            "\n",
            "Metrics for Validation set (Algoritmo AnDE):\n",
            " - Silhouette Score: 0.6827\n",
            " - Davies-Bouldin Index: 2.0878\n",
            " - Calinski-Harabasz Index: 53.4775\n",
            " - Global Score: 27.7401\n",
            "\n",
            "Metrics for Test set (Algoritmo AnDE):\n",
            " - Silhouette Score: 0.7369\n",
            " - Davies-Bouldin Index: 1.6303\n",
            " - Calinski-Harabasz Index: 91.8491\n",
            " - Global Score: 49.0589\n",
            "\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Model</th>\n",
              "      <th>Set</th>\n",
              "      <th>Silhouette Score</th>\n",
              "      <th>Davies-Bouldin Index</th>\n",
              "      <th>Calinski-Harabasz Index</th>\n",
              "      <th>Global Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Clustering Jerárquico</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.836252</td>\n",
              "      <td>1.348159</td>\n",
              "      <td>197.325390</td>\n",
              "      <td>90.576674</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Clustering Jerárquico</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.888725</td>\n",
              "      <td>0.074304</td>\n",
              "      <td>82.044252</td>\n",
              "      <td>74.255095</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Clustering Jerárquico</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.620584</td>\n",
              "      <td>1.498707</td>\n",
              "      <td>100.826994</td>\n",
              "      <td>52.306950</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Training</td>\n",
              "      <td>0.743961</td>\n",
              "      <td>2.077912</td>\n",
              "      <td>157.092062</td>\n",
              "      <td>63.464839</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Validation</td>\n",
              "      <td>0.682671</td>\n",
              "      <td>2.087814</td>\n",
              "      <td>53.477549</td>\n",
              "      <td>27.740134</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Algoritmo AnDE</td>\n",
              "      <td>Test</td>\n",
              "      <td>0.736900</td>\n",
              "      <td>1.630349</td>\n",
              "      <td>91.849113</td>\n",
              "      <td>49.058886</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                   Model         Set  Silhouette Score  Davies-Bouldin Index  \\\n",
              "0  Clustering Jerárquico    Training          0.836252              1.348159   \n",
              "1  Clustering Jerárquico  Validation          0.888725              0.074304   \n",
              "2  Clustering Jerárquico        Test          0.620584              1.498707   \n",
              "3         Algoritmo AnDE    Training          0.743961              2.077912   \n",
              "4         Algoritmo AnDE  Validation          0.682671              2.087814   \n",
              "5         Algoritmo AnDE        Test          0.736900              1.630349   \n",
              "\n",
              "   Calinski-Harabasz Index  Global Score  \n",
              "0               197.325390     90.576674  \n",
              "1                82.044252     74.255095  \n",
              "2               100.826994     52.306950  \n",
              "3               157.092062     63.464839  \n",
              "4                53.477549     27.740134  \n",
              "5                91.849113     49.058886  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import silhouette_score\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "# TODO Verificar este modelo\n",
        "# Algoritmo AnDE : calcular la densidad de los puntos y luego determinar anomalías basadas en un umbral.\n",
        "\n",
        "class AnDE:\n",
        "    def __init__(self, k=5, threshold=0.5):\n",
        "        self.k = k\n",
        "        self.threshold = threshold\n",
        "\n",
        "    def fit(self, X):\n",
        "        self.nbrs = NearestNeighbors(n_neighbors=self.k).fit(X)\n",
        "        self.densities = self.nbrs.kneighbors_graph(X).sum(axis=1)\n",
        "\n",
        "    def score_samples(self, X):\n",
        "        distances, _ = self.nbrs.kneighbors(X)\n",
        "        density_of_neighbors = self.densities.flatten()\n",
        "        density_of_neighbors[density_of_neighbors == 0] = np.finfo(float).eps\n",
        "        return 1 / (density_of_neighbors.mean() / distances.mean(axis=1))\n",
        "\n",
        "model_name = 'Algoritmo AnDE'\n",
        "print(model_name)\n",
        "\n",
        "# Entrenar el modelo AnDE\n",
        "ande_model = AnDE()\n",
        "ande_model.fit(X_train_prep)\n",
        "\n",
        "# Calcular la densidad para cada punto en el conjunto de entrenamiento\n",
        "densities_train = ande_model.score_samples(X_train_prep)\n",
        "\n",
        "# Calcular el umbral óptimo basado en la densidad\n",
        "threshold = np.percentile(densities_train, 95)\n",
        "\n",
        "# Predicción de anomalías en el conjunto de entrenamiento\n",
        "anomalies_train = densities_train > threshold\n",
        "\n",
        "# Calcular el Silhouette Score para evaluar la calidad del modelo\n",
        "silhouette_score_train = silhouette_score(X_train_prep, anomalies_train)\n",
        "\n",
        "print(\"Umbral óptimo basado en la densidad:\", threshold)\n",
        "print(\"Silhouette Score en el conjunto de entrenamiento:\", silhouette_score_train)\n",
        "\n",
        "# Predicción de anomalías en los conjuntos de validación y prueba\n",
        "anomalies_val = ande_model.score_samples(X_val_prep) > threshold\n",
        "anomalies_test = ande_model.score_samples(X_test_prep) > threshold\n",
        "\n",
        "# Mostrar estadísticas de anomalías\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, anomalies_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, anomalies_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, anomalies_test, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Detección de Anomalías (Isolation Forest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Detección de Anomalías (Isolation Forest)\n",
            "Metrics for Training set (Detección de Anomalías (Isolation Forest)):\n",
            " - Silhouette Score: 0.6577\n",
            " - Davies-Bouldin Index: 1.9636\n",
            " - Calinski-Harabasz Index: 177.3739\n",
            " - Global Score: 70.6920\n",
            "\n",
            "Metrics for Validation set (Detección de Anomalías (Isolation Forest)):\n",
            " - Silhouette Score: 0.6336\n",
            " - Davies-Bouldin Index: 2.0642\n",
            " - Calinski-Harabasz Index: 56.0600\n",
            " - Global Score: 28.1772\n",
            "\n",
            "Metrics for Test set (Detección de Anomalías (Isolation Forest)):\n",
            " - Silhouette Score: 0.6652\n",
            " - Davies-Bouldin Index: 1.6925\n",
            " - Calinski-Harabasz Index: 88.0443\n",
            " - Global Score: 45.5603\n",
            "\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 1500x500 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "## TODO hasta que punto son anómalos estos registros y que hacer con ellos si de verdad lo son (cambiar a mano, borrar, entrenar por separado datos anómalos y normales) \n",
        "# Nos interesa borrar registros ? tenemos un número no muy elevado de ellos \n",
        "model_name = 'Detección de Anomalías (Isolation Forest)'\n",
        "print(model_name)\n",
        "\n",
        "# Convertir los conjuntos de datos a numpy arrays si no lo son ya\n",
        "X_train_prep = np.array(X_train_prep)\n",
        "X_val_prep = np.array(X_val_prep)\n",
        "X_test_prep = np.array(X_test_prep)\n",
        "\n",
        "# Entrenar el modelo Isolation Forest\n",
        "model_isolation_forest = IsolationForest(contamination=0.1, random_state=42)\n",
        "model_isolation_forest.fit(X_train_prep)\n",
        "\n",
        "# Predicción de anomalías en el conjunto de entrenamiento, validación y prueba\n",
        "anomalies_train = model_isolation_forest.predict(X_train_prep)\n",
        "anomalies_val = model_isolation_forest.predict(X_val_prep)\n",
        "anomalies_test = model_isolation_forest.predict(X_test_prep)\n",
        "\n",
        "# Convertir las predicciones a etiquetas binarias (0 para normal, 1 para anomalía)\n",
        "anomalies_train_labels = np.where(anomalies_train == -1, 1, 0)\n",
        "anomalies_val_labels = np.where(anomalies_val == -1, 1, 0)\n",
        "anomalies_test_labels = np.where(anomalies_test == -1, 1, 0)\n",
        "\n",
        "# Mostrar estadísticas de anomalías\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_prep, anomalies_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_prep, anomalies_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_prep, anomalies_test, \"Test\", model_name)\n",
        "\n",
        "# Eliminar registros anómalos\n",
        "X_train_prep_sin_anom = X_train_prep[anomalies_train_labels == 0]\n",
        "X_val_prep_sin_anom = X_val_prep[anomalies_val_labels == 0]\n",
        "X_test_prep_sin_anom = X_test_prep[anomalies_test_labels == 0]\n",
        "\n",
        "# Label por color\n",
        "def label_to_color(y_pred):\n",
        "    return ['blue' if y == 1 else 'red' for y in y_pred]\n",
        "\n",
        "# Mostrar los resultados\n",
        "plt.figure(figsize=(15, 5))\n",
        "\n",
        "# Conjunto de Entrenamiento\n",
        "plt.subplot(1, 3, 1)\n",
        "plt.title(\"Entrenamiento - Predicciones\")\n",
        "plt.scatter(X_train_prep[:, 0], X_train_prep[:, 1], color=label_to_color(anomalies_train_labels))\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# Conjunto de Validación\n",
        "plt.subplot(1, 3, 2)\n",
        "plt.title(\"Validación - Predicciones\")\n",
        "plt.scatter(X_val_prep[:, 0], X_val_prep[:, 1], color=label_to_color(anomalies_val_labels))\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "# Conjunto de Prueba\n",
        "plt.subplot(1, 3, 3)\n",
        "plt.title(\"Prueba - Predicciones\")\n",
        "plt.scatter(X_test_prep[:, 0], X_test_prep[:, 1], color=label_to_color(anomalies_test_labels))\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Reducción de dimensionalidad SVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "## TODO ver si interesa reducir componentes. De primeras para la detección del cancer diría que no ya que hemos reducido a 9 variables previamente.\n",
        "model_name = 'Reducción de Dimensionalidad SVD'\n",
        "print(model_name)\n",
        "\n",
        "max_components = X_train_prep.shape[1]\n",
        "\n",
        "# Crear y ajustar el modelo SVD\n",
        "model_SVD = TruncatedSVD(n_components=max_components)\n",
        "model_SVD.fit(X_train_prep)\n",
        "\n",
        "# Calcular la varianza explicada acumulada para establecer el peso de cada variable \n",
        "explained_variance_ratio = model_SVD.explained_variance_ratio_\n",
        "cumulative_explained_variance = np.sum(explained_variance_ratio)\n",
        "print(\"Varianza explicada acumulada:\", cumulative_explained_variance)\n",
        "\n",
        "# Aplicar el modelo SVD a los datos de entrenamiento, validación y prueba\n",
        "X_train_reduced = model_SVD.transform(X_train_prep)\n",
        "X_val_reduced = model_SVD.transform(X_val_prep)\n",
        "X_test_reduced = model_SVD.transform(X_test_prep)\n",
        "\n",
        "# Usamos Kmeans para poder dar valores a los los datos labels_train, labels_val y labels_test\n",
        "model_KM = KMeans(random_state=42)\n",
        "\n",
        "# Obtención del número de clusters optimo, escoger entre elbow o silhouette\n",
        "optimal_k = optimal_cluster_number(X_train_prep, X_val_prep, model_KM, method='silhouette')\n",
        "print(optimal_k)\n",
        "# Ajustar el modelo con el número óptimo de clusters\n",
        "kmeans = KMeans(n_clusters = optimal_k)\n",
        "labels_train = kmeans.fit_predict(X_train_reduced)\n",
        "labels_val = kmeans.predict(X_val_reduced)\n",
        "labels_test = kmeans.predict(X_test_reduced)\n",
        "\n",
        "# Mostrar estadísticas de anomalías\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_train_reduced, labels_train, \"Training\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_val_reduced, labels_val, \"Validation\", model_name)\n",
        "mostrar_estadisticas_guardar_tabla_NS(X_test_reduced, labels_test, \"Test\", model_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformar tabla_results_NS_df a formato Excel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tabla_results_NS_df.to_excel('model_results_NS.xlsx', index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Borrar informacion de las tablas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Borrar todos los registros\n",
        "tabla_results_df = tabla_results_df.iloc[0:0]\n",
        "tabla_results_NS_df = tabla_results_NS_df.iloc[0:0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Cruces entre modelos  no supervisados y supervisados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test 1 - K-means + Random Forest "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PARTE 2 - Detección del tipo de cáncer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Preprocesar Datos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocesador\n",
        "import joblib\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "important_features = [\n",
        "        'sFas (pg/ml)',\n",
        "        'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
        "        'CA 15-3 (U/ml)',\n",
        "        'CA19-9 (U/ml)',\n",
        "        'CA-125 (U/ml)',\n",
        "        'TIMP-2 (pg/ml)',\n",
        "        'TGFa (pg/ml)',\n",
        "        'Leptin (pg/ml)',\n",
        "        'IL-8 (pg/ml)',\n",
        "        'IL-6 (pg/ml)',\n",
        "        'AFP (pg/ml)',\n",
        "        'GDF15 (ng/ml)',\n",
        "        'Prolactin (pg/ml)',\n",
        "        'HGF (pg/ml)',\n",
        "        'CD44 (ng/ml)',\n",
        "        'Midkine (pg/ml)',\n",
        "        'Thrombospondin-2 (pg/ml)',\n",
        "        'TIMP-1 (pg/ml)',\n",
        "        'HE4 (pg/ml)'\n",
        "    ]\n",
        "\n",
        "def get_preprocessor():\n",
        "    numeric_features = important_features\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "    \n",
        "    return preprocessor\n",
        "\n",
        "important_features = [\n",
        "        'sFas (pg/ml)',\n",
        "        'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
        "        'CA 15-3 (U/ml)',\n",
        "        'CA19-9 (U/ml)',\n",
        "        'CA-125 (U/ml)',\n",
        "        'TIMP-2 (pg/ml)',\n",
        "        'TGFa (pg/ml)',\n",
        "        'Leptin (pg/ml)',\n",
        "        'IL-8 (pg/ml)',\n",
        "        'IL-6 (pg/ml)',\n",
        "        'AFP (pg/ml)',\n",
        "        'GDF15 (ng/ml)',\n",
        "        'Prolactin (pg/ml)',\n",
        "        'HGF (pg/ml)',\n",
        "        'CD44 (ng/ml)',\n",
        "        'Midkine (pg/ml)',\n",
        "        'Thrombospondin-2 (pg/ml)',\n",
        "        'TIMP-1 (pg/ml)',\n",
        "        'HE4 (pg/ml)'\n",
        "    ]\n",
        "\n",
        "df = pd.read_excel('C:/Users/danie/OneDrive/Documentos/Master/Lusku/TFM/Repositorio compartido/kschool_master/CancerPredictionMLflow/research/validated_data.xlsx')\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "        \n",
        "y_train = train_df.pop('Tumor type')\n",
        "y_test = test_df.pop('Tumor type')\n",
        "\n",
        "preprocessor = get_preprocessor()\n",
        "\n",
        "train_df_transformed = preprocessor.fit_transform(train_df[important_features])\n",
        "test_df_transformed = preprocessor.transform(test_df[important_features])\n",
        "\n",
        "feature_names = important_features\n",
        "\n",
        "train_df_transformed = pd.DataFrame(train_df_transformed, columns=feature_names)\n",
        "test_df_transformed = pd.DataFrame(test_df_transformed, columns=feature_names)\n",
        "\n",
        "train_df_transformed['Tumor type'] = y_train.values\n",
        "test_df_transformed['Tumor type'] = y_test.values\n",
        "\n",
        "#train_df_transformed.to_excel(self.config.transformed_train_data_path, index=False)\n",
        "#test_df_transformed.to_excel(self.config.transformed_test_data_path, index=False)\n",
        "\n",
        "joblib.dump(preprocessor, 'C:/Users/danie/OneDrive/Documentos/Master/Lusku/TFM/Repositorio compartido/kschool_master/Modelos supervisados entrenados/preprocessor.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crear y entrenar modelo Vosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gen. (-2.37) | Discrim. (0.14): 100%|██████████| 200/200 [00:12<00:00, 16.07it/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000481 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 4845\n",
            "[LightGBM] [Info] Number of data points in the train set: 2483, number of used features: 19\n",
            "[LightGBM] [Info] Start training from score -2.077430\n",
            "[LightGBM] [Info] Start training from score -2.077430\n",
            "[LightGBM] [Info] Start training from score -2.080650\n",
            "[LightGBM] [Info] Start training from score -2.080650\n",
            "[LightGBM] [Info] Start training from score -2.080650\n",
            "[LightGBM] [Info] Start training from score -2.077430\n",
            "[LightGBM] [Info] Start training from score -2.080650\n",
            "[LightGBM] [Info] Start training from score -2.080650\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
          ]
        }
      ],
      "source": [
        "from ctgan import CTGAN\n",
        "from imblearn.over_sampling import SMOTE\n",
        "import joblib\n",
        "import lightgbm as lgb\n",
        "import xgboost as xgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "important_features = [\n",
        "        'sFas (pg/ml)',\n",
        "        'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
        "        'CA 15-3 (U/ml)',\n",
        "        'CA19-9 (U/ml)',\n",
        "        'CA-125 (U/ml)',\n",
        "        'TIMP-2 (pg/ml)',\n",
        "        'TGFa (pg/ml)',\n",
        "        'Leptin (pg/ml)',\n",
        "        'IL-8 (pg/ml)',\n",
        "        'IL-6 (pg/ml)',\n",
        "        'AFP (pg/ml)',\n",
        "        'GDF15 (ng/ml)',\n",
        "        'Prolactin (pg/ml)',\n",
        "        'HGF (pg/ml)',\n",
        "        'CD44 (ng/ml)',\n",
        "        'Midkine (pg/ml)',\n",
        "        'Thrombospondin-2 (pg/ml)',\n",
        "        'TIMP-1 (pg/ml)',\n",
        "        'HE4 (pg/ml)'\n",
        "    ]\n",
        "rf_params = {\n",
        "    'n_estimators': 100,\n",
        "    'max_depth': 5\n",
        "}\n",
        "gb_params = {\n",
        "    'n_estimators': 100,\n",
        "    'learning_rate': 0.06,\n",
        "    'max_depth': 3\n",
        "}\n",
        "\n",
        "lgb_params = {\n",
        "    'n_estimators': 100,\n",
        "    'learning_rate': 0.06,\n",
        "    'num_leaves': 31\n",
        "}\n",
        "\n",
        "xgb_params = {\n",
        "    'n_estimators': 100,\n",
        "    'learning_rate': 0.06,\n",
        "    'max_depth': 3\n",
        "}\n",
        "\n",
        "def train():\n",
        "    # Cargar datos de entrenamiento y prueba\n",
        "    train_data = pd.read_excel('C:/Users/danie/OneDrive/Documentos/Master/Lusku/TFM/Repositorio compartido/kschool_master/CancerPredictionMLflow/research/train_df.xlsx')\n",
        "    test_data = pd.read_excel('C:/Users/danie/OneDrive/Documentos/Master/Lusku/TFM/Repositorio compartido/kschool_master/CancerPredictionMLflow/research/test_df.xlsx')\n",
        "\n",
        "    # Combinar datos de entrenamiento y prueba\n",
        "    df_combined = pd.concat([train_data, test_data], axis=0)\n",
        "\n",
        "    # Seleccionar características importantes\n",
        "    X_real = df_combined[important_features]\n",
        "    y_real = df_combined['Tumor type']\n",
        "\n",
        "    # Identificar clases minoritarias\n",
        "    class_counts = y_real.value_counts()\n",
        "    minority_classes = class_counts[class_counts < class_counts.median()].index\n",
        "\n",
        "    # Separar datos de clases minoritarias\n",
        "    X_minority = X_real[y_real.isin(minority_classes)]\n",
        "    y_minority = y_real[y_real.isin(minority_classes)]\n",
        "\n",
        "    # Entrenar el modelo CTGAN solo con las clases minoritarias\n",
        "    ctgan_params = {\n",
        "    'epochs': 200,\n",
        "    'batch_size': 500,\n",
        "    'discriminator_steps': 1,\n",
        "    'verbose': [False]\n",
        "}\n",
        "    model = CTGAN(**ctgan_params)\n",
        "    model.fit(X_minority)\n",
        "\n",
        "    # Generar datos sintéticos para las clases minoritarias\n",
        "    synthetic_data_minority = model.sample(len(X_minority))\n",
        "\n",
        "    # Asignar etiquetas correctas a los datos sintéticos generados\n",
        "    synthetic_data_minority['Tumor type'] = np.random.choice(minority_classes, len(synthetic_data_minority))\n",
        "\n",
        "    # Separar características y etiquetas de los datos sintéticos generados\n",
        "    X_synthetic = synthetic_data_minority[important_features]\n",
        "    y_synthetic = synthetic_data_minority['Tumor type']\n",
        "\n",
        "    # Combinar datos reales y datos sintéticos generados\n",
        "    X_combined = pd.concat([X_real, X_synthetic], axis=0)\n",
        "    y_combined = pd.concat([y_real, y_synthetic], axis=0)\n",
        "\n",
        "    # Aplicar SMOTE para sobremuestrear las clases minoritarias en el conjunto combinado\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_combined, y_combined)\n",
        "\n",
        "    # Dividir en conjuntos de entrenamiento y prueba\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42, stratify=y_resampled)\n",
        "\n",
        "    # Definir los modelos individuales con regularización\n",
        "    rf_clf = RandomForestClassifier(random_state=42, **rf_params)\n",
        "    gb_clf = GradientBoostingClassifier(random_state=42, **gb_params)\n",
        "    lgbm_clf = lgb.LGBMClassifier(random_state=42, **lgb_params)\n",
        "    xgb_clf = xgb.XGBClassifier(random_state=42, **xgb_params)\n",
        "\n",
        "    # Definir el Voting Classifier\n",
        "    voting_clf = VotingClassifier(\n",
        "        estimators=[\n",
        "            ('rf', rf_clf),\n",
        "            ('gb', gb_clf),\n",
        "            ('lgbm', lgbm_clf),\n",
        "            ('xgb', xgb_clf)\n",
        "        ],\n",
        "        voting='soft'  # 'soft' uses predicted probabilities\n",
        "    )\n",
        "\n",
        "    # Entrenar el Voting Classifier con todos los datos resampleados\n",
        "    voting_clf.fit(X_train, y_train)\n",
        "\n",
        "    # Guardar el modelo entrenado\n",
        "    joblib.dump(voting_clf, 'C:/Users/danie/OneDrive/Documentos/Master/Lusku/TFM/Repositorio compartido/kschool_master/Modelos supervisados entrenados/voting_clf.joblib')\n",
        "\n",
        "#Ejecutar función para entrenar\n",
        "train()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creación de datos sintéticos mediante CTGAN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este apartado se lleva a cabo la creación de los modelos de generación de datos sintéticos. Ambos con objetivos distintos:\n",
        "\n",
        "- Parte 1. La creación de datos sintéticos en esta parte tiene unicamente el fin de simular datos que podrían encontrarse en producción para la app creada. \n",
        "        Para cada variable se genera un sample con una distribución acorde a la original, de forma que se intenta respetar la legitimidad del dato en la mayor medida de lo posible.\n",
        "\n",
        "- Parte 2.  La creación de datos sintéticos en esta parte tiene dos fines. El primero abordar el problema de poseer un dump de datos tan pobre, numericamente hablando, \n",
        "y compensar los desbalances entre los registros para cada tipo de cáncer. Al aumentar el número de registros, la etapa de training es más rica y el modelo alcanza a ajustarse mejor ofreciendo unos resultados mucho mejores en la etapa de testing.\n",
        "La segunda razón es utilizar samples creados con este modelo de CTGAN con una distribución acorde a la original, de forma que se intenta respetar de igual manera, la legitimidad del dato en la mayor medida de lo posible. \n",
        "\n",
        "Contraparte: \n",
        "Existe la posibilidad de condicionar el resultado ya que se está alimentando la etapa de entrenamiento con la generación de datos en base a un modelo, y más tarde en la etapa de producción, alimentamos con la creación de un sample de este modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['C:/Users/danie/OneDrive/Documentos/Master/Lusku/TFM/Repositorio compartido/kschool_master/Modelos supervisados entrenados/preprocessor.joblib']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Preprocesador\n",
        "import joblib\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "important_features = [\n",
        "        'sFas (pg/ml)',\n",
        "        'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
        "        'CA 15-3 (U/ml)',\n",
        "        'CA19-9 (U/ml)',\n",
        "        'CA-125 (U/ml)',\n",
        "        'TIMP-2 (pg/ml)',\n",
        "        'TGFa (pg/ml)',\n",
        "        'Leptin (pg/ml)',\n",
        "        'IL-8 (pg/ml)',\n",
        "        'IL-6 (pg/ml)',\n",
        "        'AFP (pg/ml)',\n",
        "        'GDF15 (ng/ml)',\n",
        "        'Prolactin (pg/ml)',\n",
        "        'HGF (pg/ml)',\n",
        "        'CD44 (ng/ml)',\n",
        "        'Midkine (pg/ml)',\n",
        "        'Thrombospondin-2 (pg/ml)',\n",
        "        'TIMP-1 (pg/ml)',\n",
        "        'HE4 (pg/ml)'\n",
        "    ]\n",
        "\n",
        "def get_preprocessor():\n",
        "    numeric_features = important_features\n",
        "\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features)\n",
        "        ],\n",
        "        remainder='passthrough'\n",
        "    )\n",
        "    \n",
        "    return preprocessor\n",
        "\n",
        "important_features = [\n",
        "        'sFas (pg/ml)',\n",
        "        'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
        "        'CA 15-3 (U/ml)',\n",
        "        'CA19-9 (U/ml)',\n",
        "        'CA-125 (U/ml)',\n",
        "        'TIMP-2 (pg/ml)',\n",
        "        'TGFa (pg/ml)',\n",
        "        'Leptin (pg/ml)',\n",
        "        'IL-8 (pg/ml)',\n",
        "        'IL-6 (pg/ml)',\n",
        "        'AFP (pg/ml)',\n",
        "        'GDF15 (ng/ml)',\n",
        "        'Prolactin (pg/ml)',\n",
        "        'HGF (pg/ml)',\n",
        "        'CD44 (ng/ml)',\n",
        "        'Midkine (pg/ml)',\n",
        "        'Thrombospondin-2 (pg/ml)',\n",
        "        'TIMP-1 (pg/ml)',\n",
        "        'HE4 (pg/ml)'\n",
        "    ]\n",
        "\n",
        "df = pd.read_excel('C:/Users/danie/OneDrive/Documentos/Master/Lusku/TFM/Repositorio compartido/kschool_master/CancerPredictionMLflow/research/validated_data.xlsx')\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)\n",
        "        \n",
        "y_train = train_df.pop('Tumor type')\n",
        "y_test = test_df.pop('Tumor type')\n",
        "\n",
        "preprocessor = get_preprocessor()\n",
        "\n",
        "train_df_transformed = preprocessor.fit_transform(train_df[important_features])\n",
        "test_df_transformed = preprocessor.transform(test_df[important_features])\n",
        "\n",
        "feature_names = important_features\n",
        "\n",
        "train_df_transformed = pd.DataFrame(train_df_transformed, columns=feature_names)\n",
        "test_df_transformed = pd.DataFrame(test_df_transformed, columns=feature_names)\n",
        "\n",
        "train_df_transformed['Tumor type'] = y_train.values\n",
        "test_df_transformed['Tumor type'] = y_test.values\n",
        "\n",
        "#train_df_transformed.to_excel(self.config.transformed_train_data_path, index=False)\n",
        "#test_df_transformed.to_excel(self.config.transformed_test_data_path, index=False)\n",
        "\n",
        "joblib.dump(preprocessor, 'C:/Users/danie/OneDrive/Documentos/Master/Lusku/TFM/Repositorio compartido/kschool_master/Modelos supervisados entrenados/preprocessor.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 1 - Detección de cancer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este apartado se entrena y se guarda el modelo de generación de datos CTGAN para la primera parte del estudio, es decir, la predicción del tipo de cáncer. \n",
        "\n",
        "Se ha realizado la búsqueda de los mejores parámetros en un proceso externo, obteniendo los presentes en la variable 'param_grid'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import joblib\n",
        "from ctgan import CTGAN\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "import numpy as np\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "class CTGANTrainer:\n",
        "    def __init__(self, param_grid, model_path, pac=1):\n",
        "        self.param_grid = param_grid\n",
        "        self.model_path = model_path\n",
        "        self.pac = pac\n",
        "\n",
        "    def evaluate_ctgan(self, params, X):\n",
        "        # Ajustar los parámetros de CTGAN\n",
        "        params['pac'] = self.pac\n",
        "        model = CTGAN(**params)\n",
        "        model.fit(X)\n",
        "\n",
        "        # Generar una muestra sintética\n",
        "        synthetic_data = model.sample(len(X))\n",
        "        \n",
        "        return synthetic_data\n",
        "\n",
        "    def grid_search(self, X):\n",
        "        # Realizar la búsqueda de hiperparámetros\n",
        "        best_score = -np.inf\n",
        "        best_params = None\n",
        "\n",
        "        for params in ParameterGrid(self.param_grid):\n",
        "            synthetic_data = self.evaluate_ctgan(params, X)\n",
        "            score = self.calculate_score(X, synthetic_data)\n",
        "            if score > best_score:\n",
        "                best_score = score\n",
        "                best_params = params\n",
        "\n",
        "        return best_params\n",
        "\n",
        "    def calculate_score(self, real_data, synthetic_data):\n",
        "        # Aquí puedes implementar la métrica de evaluación que desees\n",
        "        # En este caso, se puede comparar alguna métrica entre los datos reales y sintéticos\n",
        "        return 0  # Ejemplo simplificado, implementa tu métrica adecuada aquí\n",
        "\n",
        "    def train_and_save(self, df):\n",
        "        # Convertir numpy array de vuelta a DataFrame si es necesario\n",
        "        if isinstance(df, np.ndarray):\n",
        "            df = pd.DataFrame(df, columns=self.feature_names)\n",
        "\n",
        "        # Ajustar el preprocesador si es necesario\n",
        "        # Asumiendo que ya tienes preprocesado tu dataframe\n",
        "\n",
        "        # Separar características (X)\n",
        "        X = df\n",
        "        \n",
        "        model = CTGAN(**param_grid)\n",
        "        model.fit(X)\n",
        "\n",
        "        # Guardar el modelo entrenado\n",
        "        joblib.dump(model, self.model_path)\n",
        "\n",
        "    def load_model(self):\n",
        "        # Cargar el modelo entrenado\n",
        "        return joblib.load(self.model_path)\n",
        "\n",
        "    def generate_sample(self, model, num_samples=1):\n",
        "        # Generar datos sintéticos\n",
        "        return model.sample(num_samples)\n",
        "\n",
        "# Ejemplo de uso:\n",
        "# Definir el grid de hiperparámetros para CTGAN\n",
        "param_grid = {\n",
        "    'epochs': 200,\n",
        "    'batch_size': 500,\n",
        "    'discriminator_steps': 1,\n",
        "    'verbose': [False]\n",
        "}\n",
        "\n",
        "# Ruta donde se guardará el modelo entrenado\n",
        "model_path = 'Modelo CTGAN/ctgan_model.pkl'\n",
        "\n",
        "# Crear instancia del entrenador CTGAN\n",
        "trainer = CTGANTrainer(param_grid, model_path, pac=1)\n",
        "\n",
        "# Suponiendo que 'df' es el dataframe completo proporcionado\n",
        "# Asumiendo que ya tienes df preprocesado y df_reduced sin la variable objetivo\n",
        "# target_column = 'Tumor type'  # Columna objetivo\n",
        "\n",
        "# Entrenar y guardar el modelo con el df del segundo enfoque\n",
        "trainer.train_and_save(df_reduced_segundo_enfoque)\n",
        "\n",
        "# Cargar el modelo y generar un único registro sintético\n",
        "model = trainer.load_model()\n",
        "\n",
        "synthetic_sample = trainer.generate_sample(model, num_samples=1)\n",
        "\n",
        "print(synthetic_sample)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parte 2 - Detección del tipo de cancer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este apartado se entrena y se guarda el modelo de generación de datos CTGAN para la segunda parte del estudio, es decir, la predicción del tipo de cáncer. \n",
        "\n",
        "Se ha realizado la búsqueda de los mejores parámetros en un proceso externo, obteniendo los presentes en la variable 'param_grid'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Gen. (-3.27) | Discrim. (-0.02): 100%|██████████| 200/200 [00:34<00:00,  5.87it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   sFas (pg/ml)  sHER2/sEGFR2/sErbB2 (pg/ml)  CA 15-3 (U/ml)  CA19-9 (U/ml)  \\\n",
            "0   1925.347605                  2976.467821        7.500011      42.419739   \n",
            "\n",
            "   CA-125 (U/ml)  TIMP-2 (pg/ml)  TGFa (pg/ml)  Leptin (pg/ml)  IL-8 (pg/ml)  \\\n",
            "0      16.449311    52857.351976     30.994827   339670.571172     18.283827   \n",
            "\n",
            "   IL-6 (pg/ml)  AFP (pg/ml)  GDF15 (ng/ml)  Prolactin (pg/ml)  HGF (pg/ml)  \\\n",
            "0     22.181022  4866.537938       0.290144       17639.133102   204.169717   \n",
            "\n",
            "   CD44 (ng/ml)  Midkine (pg/ml)  Thrombospondin-2 (pg/ml)  TIMP-1 (pg/ml)  \\\n",
            "0      5.577523       492.933961                 1089.5091    35962.406416   \n",
            "\n",
            "   HE4 (pg/ml)  \n",
            "0  3668.606196  \n"
          ]
        }
      ],
      "source": [
        "class CTGAN2:\n",
        "    def __init__(self, param_grid, model_path, pac=1):\n",
        "        self.param_grid = param_grid\n",
        "        self.model_path = model_path\n",
        "        self.pac = pac\n",
        "        \n",
        "    def train_and_save(self, df):\n",
        "        # Convertir numpy array de vuelta a DataFrame si es necesario\n",
        "        if isinstance(df, np.ndarray):\n",
        "            df = pd.DataFrame(df, columns=self.feature_names)\n",
        "\n",
        "        # Ajustar el preprocesador si es necesario\n",
        "        # Asumiendo que ya tienes preprocesado tu dataframe\n",
        "\n",
        "        # Separar características (X)\n",
        "        X = df\n",
        "        \n",
        "        model = CTGAN(**param_grid)\n",
        "        model.fit(X)\n",
        "\n",
        "        # Guardar el modelo entrenado\n",
        "        joblib.dump(model, self.model_path)\n",
        "\n",
        "    def load_model(self):\n",
        "        # Cargar el modelo entrenado\n",
        "        return joblib.load(self.model_path)\n",
        "\n",
        "    def generate_sample(self, model, num_samples=1):\n",
        "        # Generar datos sintéticos\n",
        "        return model.sample(num_samples)\n",
        "\n",
        "model_path_2 = 'Modelo CTGAN/ctgan_model_2.pkl'\n",
        "param_grid = {\n",
        "    'epochs': 200,\n",
        "    'batch_size': 500,\n",
        "    'discriminator_steps': 1,\n",
        "    'verbose': [False]\n",
        "}\n",
        "columnas = [\n",
        "    'sFas (pg/ml)',\n",
        "    'sHER2/sEGFR2/sErbB2 (pg/ml)',\n",
        "    'CA 15-3 (U/ml)',\n",
        "    'CA19-9 (U/ml)',\n",
        "    'CA-125 (U/ml)',\n",
        "    'TIMP-2 (pg/ml)',\n",
        "    'TGFa (pg/ml)',\n",
        "    'Leptin (pg/ml)',\n",
        "    'IL-8 (pg/ml)',\n",
        "    'IL-6 (pg/ml)',\n",
        "    'AFP (pg/ml)',\n",
        "    'GDF15 (ng/ml)',\n",
        "    'Prolactin (pg/ml)',\n",
        "    'HGF (pg/ml)',\n",
        "    'CD44 (ng/ml)',\n",
        "    'Midkine (pg/ml)',\n",
        "    'Thrombospondin-2 (pg/ml)',\n",
        "    'TIMP-1 (pg/ml)',\n",
        "    'HE4 (pg/ml)'\n",
        "]\n",
        "# Crear instancia del entrenador CTGAN\n",
        "trainer = CTGAN2(param_grid, model_path, pac=1)\n",
        "\n",
        "# Suponiendo que 'df' es el dataframe completo proporcionado\n",
        "# Asumiendo que ya tienes df preprocesado y df_reduced sin la variable objetivo\n",
        "# target_column = 'Tumor type'  # Columna objetivo\n",
        "\n",
        "# Entrenar y guardar el modelo con el df del segundo enfoque\n",
        "trainer.train_and_save(df[columnas])\n",
        "\n",
        "# Cargar el modelo y generar un único registro sintético\n",
        "model = trainer.load_model()\n",
        "\n",
        "synthetic_sample = trainer.generate_sample(model, num_samples=1)    \n",
        "\n",
        "print(synthetic_sample)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting python-box\n",
            "  Using cached python_box-7.2.0-cp312-cp312-win_amd64.whl.metadata (8.0 kB)\n",
            "Using cached python_box-7.2.0-cp312-cp312-win_amd64.whl (1.2 MB)\n",
            "Installing collected packages: python-box\n",
            "Successfully installed python-box-7.2.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.0 -> 24.1.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "!pip install python-box"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SimpleImputer from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator StandardScaler from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator Pipeline from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n",
            "c:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:348: InconsistentVersionWarning: Trying to unpickle estimator ColumnTransformer from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
            "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Ejemplo para cargar el modelo directamente fuera de la aplicación\n",
        "try:\n",
        "    model_path = 'C:/Users/danie/OneDrive/Documentos/Master/Lusku/TFM/Repositorio compartido/kschool_master/Modelos supervisados entrenados/preprocessor.joblib'\n",
        "    preprocessor = joblib.load(model_path)\n",
        "    if preprocessor is None:\n",
        "        raise ValueError(\"El preprocesador no se pudo cargar correctamente.\")\n",
        "    # Continuar con el uso del preprocesador\n",
        "except Exception as e:\n",
        "    print(f\"Error al cargar o aplicar el preprocesador: {e}\")\n",
        "\n",
        "    preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Name: scikit-learn\n",
            "Version: 1.3.2\n",
            "Summary: A set of python modules for machine learning and data mining\n",
            "Home-page: http://scikit-learn.org\n",
            "Author: \n",
            "Author-email: \n",
            "License: new BSD\n",
            "Location: C:\\Users\\danie\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\n",
            "Requires: joblib, numpy, scipy, threadpoolctl\n",
            "Required-by: rdt\n"
          ]
        }
      ],
      "source": [
        "!pip show scikit-learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Conclusiones"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
